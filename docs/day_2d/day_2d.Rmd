---
title: '<span style="font-size:48pt;">Cross-Validation: Advanced</span>'
subtitle: '📈  💻  🤖️' 
author: 'Pittsburgh Summer Methodology Series'
date: 'Day 2D &emsp; &emsp; August 9, 2022'
output:
  xaringan::moon_reader:
    css: [xaringan-themer.css, style.css]
    nature:
      beforeInit: "macros.js"
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
    self_contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  comment = "#>",
  collapse = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  footnote_font_size = "20px",
  footnote_color = "gray",
  text_slide_number_font_size = "18px"
)
```

```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_tachyons()
xaringanExtra::use_clipboard()
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
```

class: inverse, center, middle
# Overview

<style type="text/css">
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
.remark-code {
  font-size: 24px;
  border: 1px solid grey;
}
a {
  background-color: lightblue;
}
.remark-inline-code {
  background-color: white;
}
</style>

---
class: inverse, center, middle
# A Full {parsnip} Walkthrough

---
class: onecol
# Applied Example

Let's put what we just learned into practice in R! 

Let's use {rsample} and {parsnip} to train a regression model on the `titanic` data. 

We will: 

- Load in and split the data.

- Train a regression model to predict each passenger's fare (how much they paid).

- Make predictions on the test set.

---
class: onecol
# Load and Split Data

```{r}
# load titanic data
titanic <- read_csv("https://bit.ly/amlr-titanic")

# create a training and testing set (stratified by fare)
set.seed(1234)
fare_index <- initial_split(data = titanic, prop = 0.8, 
                                     strata = 'fare')
fare_train <- training(titanic_split_strat)
fare_test <- testing(titanic_split_strat)
```

--

```{r}
# Check sizes
dim(fare_train)
dim(fare_test)
```

---
class: onecol
# Specify and Fit Model

```{r}
# Specify Model
ols_reg <- linear_reg() %>% 
  set_engine("lm")

# Fit Model
fare_fit <- ols_reg %>% 
  fit(fare ~ age + sibsp + parch, data = fare_train)
```

---
class: onecol
# Inspect Results

```{r, eval = FALSE}
# Inspect model results
tidy(fare_fit)
```

```{r, echo = FALSE}
tidy(fare_fit) %>% kable()
```

---
class: onecol
# Inspect Results

```{r, eval = FALSE}
# Performance metrics on training data 
glance(fare_fit)
```

```{r, echo = FALSE}
glance(fare_fit) %>% kable() %>% scroll_box(width = "1000px")
```

---
class: onecol
# Test Set Predictions

To evaluate our model's performance on the test set, we need two things: 

1. Predictions from the model on the test set (e.g., values, classes, or probabilities)

2. Trusted labels on the test set 

--

<p style="padding-top:30px;"> We can compare these predicted and trusted labels using {yardstick}

Some common options for regression include *RMSE*, $R^2$, and *MAE*.

We will learn about performance evaluation and {yardstick} in more detail tomorrow!

---
class: onecol
# Test Set Predictions

To get predictions from the final model on new data<sup>1</sup>, we can use `predict()`

Argument&emsp; | Description
:------- | :----------
object | A trained model object created by `fit()`
new_data | A data frame with the same features as training
type | Data type (numeric, classes, probabilities, etc.)

.footnote[
[1] The same process is used for both evaluating performance and deploying the model in the real world.
]

---
class: onecol
# Test Set Predictions

```{r}
fare_pred <- predict(fare_fit, new_data = fare_test)
fare_pred
```

---
class: onecol
# Visualizing Predicted and Trusted Labels

```{r, eval = FALSE}
qplot(x = fare_test$fare, y = fare_pred$.pred) + abline()
```

```{r quick, echo=FALSE, fig.height=3.25}
qplot(x = fare_test$fare, y = fare_pred$.pred) + geom_abline() +
  theme_xaringan(text_font_size = 20) +
  theme(
    axis.title = element_text(size = 20),
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
# Estimating Test Set Performance 

`predict()` returns predictions in the **same order** as the original data.

This makes it easy to merge predictions with test data to estimate model performance.

--

.scroll50[
```{r}
fare_pred <- fare_test %>% 
  select(fare) %>% 
  bind_cols(predict(fare_fit, new_data = fare_test))

fare_pred
```
]

---
class: onecol
# Estimating Test Set Performance

To estimate performance in {tidymodels}, use {yardstick}<sup>1</sup>.

```{r, eval = FALSE}
# create a performance metric set 
fare_metrics <- metric_set(rmse, rsq, mae)

fare_metrics(fare_pred, truth = fare, estimate = .pred)
```

```{r, echo = FALSE}
# create a performance metric set 
fare_metrics <- metric_set(rmse, rsq, mae)

fare_metrics(fare_pred, truth = fare, estimate = .pred) %>% kable(digits = 5)
```

.footnote[
[1] More on {yardstick} coming tomorrow! 
]

---
class: onecol
# Model Interpretation 

Predictive **accuracy** is emphasized in ML over interpretability and inference

- The main goal of most applied ML studies is to **quantify performance**

--

However, some algorithms can provide insight into their decision-making

- As a model usually used for inference, linear regression has strong interpretability

- We can examine the model coefficients (intercept and slopes)

--

```{r, eval=FALSE}
fare_fit$fit %>% coef()
```

```{r, echo=FALSE}
fare_fit$fit %>% coef() %>% t() %>% kable(digits = 2)
```

---
class: onecol
# Variable Importance

We can also plot variable importance using the {vip} package.

```{r, out.width = "50%"}
library(vip)
vip(fare_fit)
```

.footnote[
Other algorithms have different ways to estimate variable importance, but `vip()` will take care of it.
]

---
class: inverse, center, middle
# End of Day 2 

