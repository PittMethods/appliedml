---
title: '<span style="font-size:48pt;">Building a Model: Start to Finish</span>'
subtitle: '📈  💻  🤖️' 
author: 'Pittsburgh Summer Methodology Series'
date: 'Day 2D &emsp; &emsp; August 9, 2022'
output:
  xaringan::moon_reader:
    css: [../css/xaringan-themer.css, ../css/styles.css]
    nature:
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
      navigation:
        scroll: false
    self_contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  comment = "#>",
  collapse = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
```

```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_tachyons()
xaringanExtra::use_clipboard()
xaringanExtra::use_tile_view()
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
```

class: inverse, center, middle
# Overview

---
class: onecol
## Plan for Today

Thus far, we have learned {rsample}, {workflows}, {recipes}, {yardstick}, and {parsnip}.

This lecture aims to tie it all together and .imp[build a model from start to finish].

--

<p style="padding-top:30px;">We will adapt **familiar classification algorithms** to a predictive modeling framework.

This will **ease the transition to ML** and highlight its similarities with classical statistics.

Finally, we have a **hands-on coding activity** to build your own predictive model.

---
class: onecol
## Applied Example

Let's put what we learned into practice in R! 

Let's train a classification model on the `titanic` data to predict survival.

--

<p style="padding-top:30px;">We will: 

- Load the data

- Create a recipe for feature engineering

- Train a classification model to predict if each passanger survived the titanic

- Evaluate the model using 10-fold cross-validation 

- Interpret the model 

---
class: onecol
## Load Data and Create Recipe

```{r}
library(tidymodels)
titanic <- read.csv("https://tinyurl.com/titanic-pm")

survived_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_mutate(survived = factor(survived),
              pclass = factor(pclass, levels = c(1, 2, 3)),
              sex = factor(sex, levels = c("female", "male"))) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(pclass, sex) %>%
  step_impute_linear(age, fare) %>%
  step_nzv(all_predictors()) %>%
  step_corr(all_predictors()) %>%
  step_lincomb(all_predictors())
```

---
class: onecol
## Specify Model with {parsnip}

```{r}
# Specify Model
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")

log_reg
```

---
class: onecol
## Build Workflow with {workflows}

.scroll70[
```{r}
survived_workflow <- 
  workflow() %>%
  add_model(log_reg) %>%
  add_recipe(survived_recipe)

survived_workflow
```
]

---
class: onecol
## Configure Resampling

```{r}
set.seed(2022)

# configure resampling
survived_folds <- vfold_cv(data = titanic, 
                           v = 10, 
                           repeats = 3,
                           strata = 'survived')
```

--

The goal of `fit_resamples()` is to .imp[measure model importance]. 

Therefore, by default, the models trained are not saved or used later.

---
class: onecol
## Configure Resampling

To save model coefficients, we need to .imp[extract] the underlying model object (engine fit). 

We can do this with a custom function, and specify it in `control_resamples()`: 

--

```{r}
get_resample_coefs <- function(x) {
  x %>% 
    # get the lm model object
    extract_fit_engine() %>% 
    # transform its format
    tidy()
}

# save predictions from resampling
keep_pred <- control_resamples(save_pred = TRUE, extract = get_resample_coefs)
```

---
class: onecol
## Fit Model with Resampling

Now that we've set the workflow & configured resampling, we're ready to fit the model! 

--

.scroll65[
```{r}
# train the model using the recipe, data, and method 
survived_results <- survived_workflow %>%
  fit_resamples(resamples = survived_folds, control = keep_pred)

survived_results
```
]

---
class: twocol
## Fit Model with Resampling

.pull-left[
```{r, echo = FALSE}
include_graphics("../figs/kfold5.png")
```
]

.pull-right[
Note: we are resampling the **entire data set**.

This means that we only have training and test sets (N = 30). 

We have no separate validation sets. 

We will obtain metrics for each test set. 

We will average performance metrics across all resamples.

These averages serve as model evaluation. 
] 

---
class: inverse, center, middle 
# Model Evaluation 

---
class: onecol
## Cross-Validated Test Performance

The object created by `fit_resamples()` will contain lots of information.

We can view the predictions made in each test set with `collect_predictions()`.

--

```{r, eval = FALSE}
collect_predictions(survived_results)
```

```{r, echo = FALSE}
collect_predictions(survived_results) %>% kable() %>% scroll_box(height = "300px")
```

---
class: onecol
## Cross-Validated Test Performance

We can view a summary of the test set performance with `collect_metrics()`<sup>1</sup>.

.footnote[
[1] To see metrics for each fold, include `summarize = FALSE` in `collect_metrics()`.
]

--

```{r, eval = FALSE}
collect_metrics(survived_results)
```


```{r, echo = FALSE}
collect_metrics(survived_results) %>% kable()
```

--

These accuracy and AUC values look pretty good! 

---
class: onecol
## Cross-Validated Test Performance 

Let's look at a .imp[confusion matrix] for a better understanding of model performance.

```{r, eval = FALSE}
survived_cm <- collect_predictions(survived_results) %>%
  conf_mat(truth = survived, estimate = .pred_class)

autoplot(survived_cm, type = "mosaic")
autoplot(survived_cm, type = "heatmap")
```

--

.pull-left[
```{r, echo = FALSE}
survived_cm <- 
  collect_predictions(survived_results) %>%
  conf_mat(truth = survived, estimate = .pred_class)

autoplot(survived_cm, type = "mosaic")
```
]

.pull-right[
```{r, echo = FALSE}
autoplot(survived_cm, type = "heatmap")
```
]

---
class: onecol
## Cross-Validated Test Performance

Finally, let's calculate all confusion matrix metrics. 

```{r, eval = FALSE}
summary(survived_cm)
```

```{r, echo = FALSE}
summary(survived_cm) %>% kable() %>% scroll_box(height = "300px")
```

---
class: inverse, center, middle
# Model Interpretation

---
class: onecol
## Model Interpretation 

Predictive **accuracy** is emphasized in ML over interpretability and inference

- The main goal of most applied ML studies is to **quantify performance**

--

However, some algorithms can provide insight into their decision-making

- As a model usually used for inference, logistic regression has strong interpretability

- We can examine the model coefficients (intercept and slopes)

The `get_resample_coefs()` function we wrote earlier will allow us to do this!

---
class: onecol
## Model Coefficients

Extracting the coefficients we need is a bit messy, but let's walk through it.

Let's take a look at the output from our resampled model `survived_results`:

--
.scroll40[
```{r}
survived_results
```
]

--

The output of `get_resample_coefs()` is contained in the column `.extracts`.


---
class: twocol
## Model Coefficients

Inside `survived_results$.extracts` is *another* `.extracts` column.

```{r}
survived_results$.extracts[[1]]
```

--

This nested column has the coefficients from each k-fold!

.scroll40[
```{r}
survived_results$.extracts[[1]]$.extracts
```
]

---
class: twocol
## Model Coefficients

We can tidy up these results with the {tidyr} `unnest()` function.

.scroll65[
```{r}
survived_coefs <- 
  survived_results %>% 
  select(id, id2, .extracts) %>%
  unnest(.extracts) %>% 
  unnest(.extracts) %>% 
  mutate(resample = paste0(id, "_", id2))

survived_coefs
```
]

---
class: twocol
## Model Coefficients

Now we can plot the model coefficients across each k-fold resample: 
```{r, eval = FALSE}
# plot coefficients across each resample
survived_coefs %>% filter(term != "(Intercept)") %>%
  ggplot(aes(x = term, y = estimate, group = resample, col = resample)) + 
  geom_hline(yintercept = 0, lty = 3) + 
  geom_line(alpha = 0.3, lwd = 0.5)
```

```{r, echo = FALSE, out.width = "60%"}
config <-   
  theme_xaringan(text_font_size = 14, title_font_size = 18,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "white")
  )

# make a column indicating each resample 
survived_coefs$resample <-  
  paste0(survived_coefs$id, "_", survived_coefs$id2)

# plot coefficients across each resample
survived_coefs %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(x = term, y = estimate, group = resample, col = resample)) + 
  geom_hline(yintercept = 0, lty = 3) + 
  geom_line(alpha = 0.3, lwd = 0.5) + 
  labs(y = "Coefficient", x = "Feature") + 
  theme(legend.position = "none") + 
  config
```

---
class: onecol
## Variable Importance

Another way of interpreting a model is with .imp[variable importance] in the {vip} package.

This is particularly helpful for **complex ML models** that are less interpretable<sup>1</sup>. 

{vip} requires a **final model**, so we can re-fit a logistic regression on the full dataset.

.footnote[
Unlike logistic regression, which has coefficients with straightforward interpretation. Other models have different methods to estimate variable importance, but `vip()` will take care of it.
]

--

```{r}
library(vip)

# fit final model using the workflow specified previously
survived_final <- 
  survived_workflow %>% 
  fit(titanic)
```

---
class: onecol
## Variable Importance

From this final model, we can **extract the fit** and visualize the most important features. 

--

```{r, out.width = "90%"}
survived_final %>% extract_fit_parsnip() %>% vip()
```

---
class: onecol 
## Hands-On Coding Activity

Today we've gone through a **full walkthrough** of building a predictive model.

Now it's time to practice the methods we've learned! 

In this hands-on activity, you will build a regression model from start to finish.

<p style="padding-top:30px;">You can find the activity at https://pittmethods.github.io/ under `Day 2D - Activity`.

If you have questions, please post them in the chat or Slack channel. 


---
class: inverse, center, middle
# End of Day 2 

