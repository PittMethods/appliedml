---
title: '<span style="font-size:48pt;">Workflows and Metrics</span>'
subtitle: 'üö£  üíØ  üìè '
author: 'Applied Machine Learning in R<br />Pittsburgh Summer Methodology Series'
date: 'Day 2A &emsp; &emsp; August 9, 2022'
output:
  xaringan::moon_reader:
    css: [../css/xaringan-themer.css, ../css/styles.css]
    nature:
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
    self_contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  collapse = TRUE
)
```

```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_tachyons()
xaringanExtra::use_clipboard()
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(xaringanthemer)
```

class: inverse, center, middle
# Workflows

---

# Setup

.scroll[
```{r, eval=FALSE}
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
titanic <- 
  read_csv("https://tinyurl.com/titanic-pm") %>% 
  mutate(
    survived = factor(survived),
    pclass = factor(pclass),
    sex = factor(sex)
  )

# Create train/test split, stratified by fare
set.seed(2022)
fare_split <- initial_split(data = titanic, prop = 0.8, strata = 'fare')

fare_train <- training(fare_split)
fare_test <- testing(fare_split)

# Set up model (linear regression using lm)
lm_model <- 
  linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")
```
]

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
titanic <- 
  read_csv("../data/titanic.csv") %>% 
  mutate(
    survived = factor(survived),
    pclass = factor(pclass),
    sex = factor(sex)
  )

# Create initial split, stratified by fare
set.seed(2022)
fare_split <- initial_split(data = titanic, prop = 0.8, strata = 'fare')
fare_train <- training(fare_split)
fare_test <- testing(fare_split)

# Set up model
lm_model <- 
  linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")
```

---

## Create a simple workflow

.onecol[
-   A .imp[workflow] collects various specifications for your ML experiment

  -   The **model** specifies the algorithm, mode, engine, and tuning steps
  
  -   The **preprocessor** specifies the formula and feature engineering steps
]

```{r}
fare_wflow <-
  workflow() %>% 
  add_model(lm_model) %>% 
  print()
```

---

## Add a formula as a simple preprocessor

.onecol[
-   We can use a .imp[formula] as a simple preprocessor (without feature engineering)
]

```{r}
fare_wflow <- 
  fare_wflow %>% 
  add_formula(fare ~ pclass + sex + age + sibsp + parch) %>% 
  print()
```

.footnote[*Note.* We will soon learn to use "recipes" as more powerful preprocessors]

---

## Fit a model using a workflow

.onecol[
-   We can explicitly fit the model to the training data using `fit()`
]

.scroll.h-1l[
```{r}
fare_fit <- 
  fit(fare_wflow, fare_train) %>% 
  print()
```
]

---

## Make predictions using the fit model

.onecol[
-   We can explicitly make predictions in the testing set using `predict()`
]

.scroll.h-1l[
```{r, message=FALSE}
fare_pred <- 
  predict(fare_fit, fare_test) %>% 
  print()
```
]

---

## A helpful shortcut

.onecol[
-   Or we can do both automatically using `last_fit()` and the "split" object

  -   This will fit to the (entire) training set and predict the testing set
]

.scroll.h-2l[
```{r}
fare_fit <- last_fit(fare_wflow, split = fare_split)
fare_fit
```
]

---

## Collecting the predictions

.onecol[
-   We can gather the testing set predictions with `collect_predictions()`
]

.scroll.h-1l[
```{r}
fare_pred <- collect_predictions(fare_fit)
fare_pred
```

]

---

## Plotting the predictions (basic)

.pull-left[
```{r, eval=FALSE}
ggplot(
  fare_pred, 
  aes(x = fare, y = .pred)
) + 
  geom_point()
```

```{r, echo=FALSE, fig.show="hide"}
ggplot(fare_pred, aes(x = fare, y = .pred)) + 
  geom_point() +
  theme_xaringan(css_file = "../css/xaringan-themer.css") +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
ggsave("../figs/fare_pred1.png", width = 3.5, height = 3.5, units = "in")
```

]

.pull-right[
![](../figs/fare_pred1.png)
]

---

## Plotting the predictions (advanced)

.pull-left[
```{r, eval=FALSE}
ggplot(
  fare_pred, 
  aes(x = fare, y = .pred)
) + 
  geom_point(alpha = .2) +
  geom_abline(color = "darkred") +
  coord_obs_pred() +
  labs(
    x = "Observed Fare", 
    y = "Predicted Fare"
  )
```

```{r, echo=FALSE, fig.show="hide"}
ggplot(
  fare_pred, 
  aes(x = fare, y = .pred)
) + 
  geom_point(alpha = .2) +
  geom_abline(color = "darkred") +
  coord_obs_pred() +
  labs(
    x = "Observed Fare", 
    y = "Predicted Fare"
  ) +
  theme_xaringan(css_file = "../css/xaringan-themer.css") +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
ggsave("../figs/fare_pred2.png", width = 3.5, height = 3.5, units = "in")
```

]

.pull-right[
![](../figs/fare_pred2.png)
]

---

## Collecting the performance metrics

.onecol[
-   We can gather the testing set predictions with `collect_metrics()`
]

```{r}
fare_perf <- collect_metrics(fare_fit)
fare_perf
```

.footnote[*Note.* We will soon learn how to interpret these metrics and calculate alternatives.]

---

## Putting it all together

.scroll.h-0l[
```{r, eval=FALSE}
# Load packages and data
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
titanic <- 
  read_csv("https://tinyurl.com/titanic-pm") %>% 
  mutate(
    survived = factor(survived),
    pclass = factor(pclass),
    sex = factor(sex)
  )

# Create data splits, stratified by fare
set.seed(2022)
fare_split <- initial_split(data = titanic, prop = 0.8, strata = 'fare')

# Set up model (linear regression using lm)
lm_model <- 
  linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")

# Set up workflow with simple formula preprocessor
fare_wflow <-
  workflow() %>% 
  add_model(lm_model) %>% 
  add_formula(fare ~ pclass + sex + age + sibsp + parch)

# Fit workflow and make predictions using data splits
fare_fit <- last_fit(fare_wflow, split = fare_split)

# Collect predictions and performance metrics
fare_pred <- collect_predictions(fare_fit)
fare_perf <- collect_metrics(fare_fit)
```
]

---

## Modifying it for classification

.scroll.h-0l[
```{r, eval=FALSE}
# Load packages and data
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
titanic <- 
  read_csv("https://tinyurl.com/titanic-pm") %>% 
  mutate(
    survived = factor(survived),
    pclass = factor(pclass),
    sex = factor(sex)
  )

# Create data splits, stratified by survived
set.seed(2022)
surv_split <- initial_split(titanic, prop = 0.8, strata = 'survived') #<<

# Set up model (logistic regression using glm)
glm_model <- 
  logistic_reg() %>% #<<
  set_mode("classification") %>% #<<
  set_engine("glm") #<<

# Set up workflow with simple formula preprocessor
surv_wflow <-
  workflow() %>% 
  add_model(glm_model) %>% #<<
  add_formula(survived ~ pclass + sex + age + sibsp + parch + fare) #<<

# Fit workflow and make predictions using data splits
surv_fit <- last_fit(surv_wflow, split = surv_split)

# Collect predictions and performance metrics
surv_pred <- collect_predictions(surv_fit)
surv_perf <- collect_metrics(surv_fit)
```

```{r, echo=FALSE}
# Create data splits, stratified by survived
set.seed(2022)
surv_split <- initial_split(titanic, prop = 0.8, strata = 'survived') #<<

# Set up model (logistic regression using glm)
glm_model <- #<<
  logistic_reg() %>% #<<
  set_mode("classification") %>% #<<
  set_engine("glm") #<<

# Set up workflow with simple formula preprocessor
surv_wflow <-
  workflow() %>% 
  add_model(glm_model) %>% #<<
  add_formula(survived ~ pclass + sex + age + sibsp + parch + fare) #<<

# Fit workflow and make predictions using data splits
surv_fit <- last_fit(surv_wflow, split = surv_split)

# Collect predictions and performance metrics
surv_pred <- collect_predictions(surv_fit)
surv_perf <- collect_metrics(surv_fit)
```
]

---

## Inspecting Classification Results

.scroll.h-0l[
```{r}
surv_pred
```
]

---

## Inspecting Classification Results

```{r}
surv_perf
```
---
class: inverse, center, middle
# Performance Metrics
---
class: onecol
## Performance Metrics

.left-column.pt3[
```{r target, echo=FALSE}
include_graphics("../figs/target.jpg")
```
]

.right-column[

**Metrics for Supervised Regression**

- .imp[Distance] between predicted and trusted values

- .imp[Correlation] between predicted and trusted values

.pt1[
**Metrics for Supervised Classification**
]
- .imp[Confusion matrix] between predicted and trusted classes

- Compare predicted .imp[class probabilities] to trusted classes
]

---
class: inverse, center, middle
# Metrics for Regression
---
class: onecol
## Classic Distance Metrics for Regression

.pull-left[
**Root Mean Squared Error (RMSE)**
- Based on squared loss
- Penalizes severe errors harsher, Sensitive to outliers
- Ranges from $0$ to $+\infty$, lower is better
]

.pull-right.pt4[
$$RMSE=\sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2}$$
]

.footnote[[1] Note that, here, we will refer to the trusted labels as `\\(y\\)` and the predicted labels as `\\(p\\)`.]

--

.pull-left[
**Mean Absolute Error (MAE)**
- Based on absolute loss
- Penalizes error consistently, Robust to outliers
- Ranges from $0$ to $+\infty$, lower is better
]

.pull-right.pt4[
$$MAE=\frac{1}{n} \sum_{i=1}^n \left\lvert y_i - p_i \right\rvert$$
]

---
exclude: true
class: onecol
## Advanced Distance Metrics for Regression
**Huber loss (HL)**

- Combines benefits of RMSE and MAE
 + Easy to computationally optimize AND more robust to outliers
- Requires setting or tuning $\delta$ (controls what to consider an outlier)
- Ranges from $0$ (best) to $+\infty$ (worst), lower is better

$$\begin{split}
HL &= \frac{1}{n} \sum_{i=1}^n L_\delta(y_i, p_i) \\
L_\delta(y_i, p_i) &= \begin{cases}\frac{1}{2}(y_i - p_i)^2 & \text{for } \lvert y_i - p_i \rvert \le \delta \\ \delta (\lvert y_i - p_i \rvert - \frac{1}{2}\delta) & \text{otherwise} \end{cases}
\end{split}$$

---
## Visualizing Regression Loss Functions

```{r losses, echo=FALSE}
tibble(
  error = seq(-2, 2, length.out = 100), 
  sq_err = error^2,
  abs_err = abs(error)
) %>% 
  pivot_longer(-error, names_to = "type", values_to = "loss") %>% 
  mutate(type = factor(
    type, 
    levels = c("sq_err", "abs_err"), 
    labels = c("Squared Loss (RMSE)", "Absolute Loss (MAE)")
  )) %>% 
  ggplot(aes(x = error, y = loss, color = type)) + 
  facet_wrap(~type, nrow = 1) + 
  geom_line(size = 1.5) + 
  labs(x = latex2exp::TeX("Error $(y_i - p_i)$"), y = "Loss") +
  theme_xaringan(text_font_size = 18, title_font_size = 18,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    axis.title = element_text(size = 20),
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
exclude: true
## Comparing Loss Functions

```{r losses3, echo=FALSE}
tibble(
  error = seq(-2, 2, length.out = 100), 
  sq_err = error^2,
  abs_err = abs(error),
  huber1 = ifelse(abs(error) <= 1, 0.5 * error^2, 1 * (abs(error) - 0.5 * 1))
) %>% 
  pivot_longer(-error, names_to = "type", values_to = "loss") %>% 
  mutate(type = factor(
    type, 
    levels = c("sq_err", "abs_err", "huber1"), 
    labels = c("Squared Loss", "Absolute Loss", "Huber Loss (delta=1)")
  )) %>% 
  ggplot(aes(x = error, y = loss, color = type)) + 
  facet_wrap(~type, nrow = 1) + 
  geom_line(size = 1.5) + 
  labs(x = "Error (y - p)", y = "Loss") +
  theme_xaringan(text_font_size = 18, title_font_size = 18,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Correlation Metrics for Regression

**R-Squared $(R^2$ or RSQ)**
- Calculated in ML as the **squared correlation** between the predictions and labels
- Ranges from $0$ to $1$, higher is better

$$R^2 = \left(\frac{\text{cov}(y, p)}{\sigma_y\sigma_p}\right)^2 = \left(\frac{\sum(y_i - \bar{y})(p_i - \bar{p})}{\sqrt{\sum (y_i-\bar{y})^2}\sqrt{\sum(p_i-\bar{p})^2}}\right)^2$$

--

.bg-light-yellow.b--light-red.ba.bw1.br3.pl4[
**Caution!**
+ RSQ is a measure of *consistency* (i.e., linear association) and not distance
+ RSQ can become unstable or undefined when data variability is low
+ RSQ can become unstable when applied in small samples (e.g., test sets)

]

---
class: onecol
## Advanced Correlation Metrics for Regression

**Concordance Correlation Coefficient (CCC)**

- Combines both accuracy (distance) and consistency (correlation) information

- Very similar to certain formulations of the intraclass correlation coefficient

- Ranges from $-1$ to $+1$, where higher is better

.pt1[
$$CCC = \frac{2\rho_{yp}\sigma_y\sigma_p}{\sigma_y^2 + \sigma_p^2 + (\mu_y - \mu_p)^2}
= \frac{\frac{2}{n}\sum (y_i - \bar{y})(p_i - \bar{p})}{\frac{1}{n}\sum(y_i - \bar{y})^2 + \frac{1}{n}\sum(p_i - \bar{p})^2 + (\bar{y} - \bar{p})^2}$$
]
---
class: onecol
## Comparing Regression Performance Metrics

```{r dvc, echo=FALSE}
set.seed(2021)
dvc <- tibble(
  label = rnorm(100, 0, 10),
  pred1 = label + rnorm(100, 0, 5),
  pred2 = 5 * label + rnorm(100, 0, 28)
)
title1 <- paste0(
  "RMSE = ", round(yardstick::rmse_vec(dvc$label, dvc$pred1), 1),
  ", MAE = ", round(yardstick::mae_vec(dvc$label, dvc$pred1), 1),
  "\nRSQ = ", round(yardstick::rsq_vec(dvc$label, dvc$pred1), 2),
  ", CCC = ", round(yardstick::ccc_vec(dvc$label, dvc$pred1), 2)
)
p1 <- ggplot(dvc, aes(x = label, y = pred1)) + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "grey") + 
  geom_point(color = "#1b9e77", size = 2) +
  labs(x = "Trusted Label", y = "Model 1's Predictions", title = title1) +
  theme_xaringan(text_font_size = 20, title_font_size = 14,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.title.position = "panel",
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
title2 <- paste0(
  "RMSE = ", round(yardstick::rmse_vec(dvc$label, dvc$pred2), 1),
  ", MAE = ", round(yardstick::mae_vec(dvc$label, dvc$pred2), 1),
  "\nRSQ = ", round(yardstick::rsq_vec(dvc$label, dvc$pred2), 2),
  ", CCC = ", round(yardstick::ccc_vec(dvc$label, dvc$pred2), 2)
)
p2 <- ggplot(dvc, aes(x = label, y = pred2)) + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "grey") + 
  geom_point(color = "#d95f02", size = 2) +
  labs(x = "Trusted Label", y = "Model 2's Predictions", title = title2) +
  theme_xaringan(text_font_size = 20, title_font_size = 14,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5),
    plot.title.position = "panel",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p1 | p2
```

---
class: inverse, center, middle
# Metrics for Classification 
## (Based on Predicted Classes)
---
class: twocol
## Confusion Matrix Metrics

 | Trusted = No<br /> $(y=0)$ | Trusted = Yes<br /> $(y=1)$
:--:| :--: | :--:
**Predicted = No**&emsp;<br /> $(p=0)$ | True Negatives (TN) | False Negative (FN)
**Predicted = Yes**&emsp;<br /> $(p=1)$ | False Positive (FP) | True Positive (TP)

--

.mt4[
$$\text{Accuracy} = \frac{TN + TP}{n}$$
]
.tc[
Ranges from $0$ to $1$, higher is better
]

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Accuracy can be misleading when the classes are highly imbalanced (e.g., more $0$s than $1$s)
]

---
class:onecol
## Confusion Matrix Metrics

With imbalanced classes, predicting the larger class will often be right "by chance"

--

To "correct" accuracy for imbalanced classes, **Cohen's Kappa** is often used

$$\kappa = \frac{\text{Accuracy} - \text{Chance}}{1 - \text{Chance}}$$

--

Chance agreement is estimated using the observed class probabilities from $y$ and $p$

$$\text{Chance} = \Pr(y=0)\cdot\Pr(p=0) + \Pr(y=1)\cdot\Pr(p=1)$$

Kappa also ranges from $0$ to $1$ (technically $-1$ to $1$), higher is better

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Paradoxically, Kappa can be overly conservative when classes are very imbalanced.
]

---
## Additional Confusion Matrix Metrics

 | $y=0$ | $y=1$
:--:| :--: | :--:
&emsp; ** $p=0$**&emsp; | &emsp;True Negatives (TN)&emsp; | &emsp;False Negative (FN)&emsp;
** $p=1$** | False Positive (FP) | True Positive (TP)

.pull-left.pt3[
$$\begin{split}
\text{Sensitivity} &= \frac{TP}{TP+FN} \\
\text{Specificity} &= \frac{TN}{TN + FP} \\
\text{Balanced Accuracy} &= \frac{\text{Sensitivity} + \text{Specificity}}{2} \\
\end{split}$$
]

--

.pull-right.pt3[
$$\begin{split}
\text{Precision} &= \frac{TP}{TP+FP} \\
\text{Recall} &= \frac{TP}{TP+FN} \\
F_1 \text{ Score} &= \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
\end{split}$$
]

.tc[
All range from $0$ to $1$, higher is better
]

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
$F_1$ does not consider $TN$, so only use it when detecting negatives is not important for your application.]



---
## Confusion Matrix Metrics Examples
.pull-left.tc[
&emsp;**Balanced**&emsp; | &emsp; $y=0$ &emsp; | &emsp; $y=1$ &emsp;
:--:| :--: | :--:
** $p=0$ ** | 101 | 54
** $p=1$ ** | 33 | 105

.pt3.f5[
$$\begin{split}
\text{Accuracy} &= 0.70 \\
\kappa &= 0.41\\ \\
\text{Sensitivity} &= 0.66 \\
\text{Specificity} &= 0.75 \\
\text{Balanced Accuracy} &= 0.71 \\ \\
\text{Precision} &= 0.76 \\
\text{Recall} &= 0.66 \\
F_1 &= 0.71
\end{split}$$

]
]

--

.pull-right.tc[
&emsp;**Imbalanced**&emsp; | &emsp; $y=0$ &emsp; | &emsp; $y=1$ &emsp;
:--:| :--: | :--:
** $p=0$ ** | 256 | 11
** $p=1$ ** | 6 | 2

.pt3.f5[

$$\begin{split}
\text{Accuracy} &= 0.94 \\ 
\kappa &= 0.16 \\
\\
\text{Sensitivity} &= 0.15 \\
\text{Specificity} &= 0.98 \\
\text{Balanced Accuracy} &= 0.57 \\
\\
\text{Precision} &= 0.25 \\
\text{Recall} &= 0.15 \\
F_1 &= 0.19
\end{split}$$
]
]

---
exclude: true
class: onecol
## Multiclass Performance Strategies

With more than two classes, you can make a larger (e.g., $3\times 3$) confusion matrix)

 | &emsp; $y$ = Healthy&emsp; | &emsp; $y$ = Depression&emsp; | &emsp; $y$ = Mania&emsp;
:--| :--: | :--: | :--:
** $p$ = Healthy**&emsp; | 100 | 3 | 7
** $p$ = Depression**&emsp; | 30 | 25 | 20 
** $p$ = Mania**&emsp; | 10 | 1 | 10

<!-- -- -->

.pt3[
.imp[Macro-averaging]: compute the standard binary metric for each class separately (using a one-vs-rest procedure) and then calculate the average metric score across classes
]

<!-- $$(F_1^H = 0.79, F_1^D = 0.46, F_1^M = 0.34)~\text{Macro }F_1=0.53$$ -->

<!-- -- -->

.imp[Micro-averaging]: compute a confusion matrix for each class separately (one-vs-rest), add these matrices together, and calculate the binary metric from the summed matrix

<!-- $$ (\Sigma TN=345, \Sigma TP=135, \Sigma FP=75, \Sigma FN=75)~\text{Micro }F_1=0.64$$ -->

---
class: inverse, center, middle
# Metrics for Classification 
## (Based on Class Probabilities)
---
class: onecol
## Class Probability Metrics for Classification

Some classifiers estimate **the probability of each class** as their prediction $(p_{ij})$

If we consider a higher estimated probability as higher "confidence"

- We can **reward** the classifier for being more **confident when correct**...

- ...and .imp[penalize] the classifier for being more .imp[confident when wrong]

--

This gives rise to the Logistic or **Log Loss**, which can be summed or averaged

$$L_{log}(Y,P) = -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^q \left(y_{ij}\log( p_{ij})\right)$$

.pl4[
$y_{ij}\in\{0,1\}$ is a binary indicator of whether observation $i$ is truly in class $j$<br />
$p_{ij}\in(0,1)$ is the estimated probability that observation $i$ is in class $j$
]

---

## Visualizing Log Loss in Binary Classification

```{r logloss, echo=FALSE}
tibble(
  truth = factor(rep(0:1, each = 501), levels = c(1, 0)),
  p = c(seq(0, 1, length.out = 501), seq(1, 0, length.out = 501))
) %>% 
  rowwise() %>% 
  mutate(loss = yardstick::mn_log_loss_vec(truth, p, event_level = "first")) %>% 
  ggplot(aes(x = p, y = loss, color = truth, linetype = truth)) + 
  geom_line(size = 2.25) +
  coord_cartesian(ylim = c(0, 8)) +
  scale_color_brewer(palette = "Set2", guide = guide_legend(
    keywidth = unit(2, "cm"), reverse = TRUE)) +
  scale_linetype_discrete(guide = guide_legend(reverse = TRUE)) +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "Predicted Probability of Class 1", y = "Log Loss", 
       color = "True Class", linetype = "True Class") +
  theme_xaringan(text_font_size = 18, title_font_size = 16,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    axis.title = element_text(size = 18),
    legend.position = "top",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Performance Curves

- When a classifier outputs class probabilities, we can choose any **decision threshold**

- We might naturally consider any probability over 50% positive and all others negative

- But we could choose a threshold more conservative (e.g., 75%) or liberal (e.g., 25%)

--

.pt1[
- **Performance curves** plot the characteristics of different decision thresholds

- This gives us an overview of how the classification system performs in general
]

--

.pt1[
- There are many performance curves<sup>1</sup>, so we'll use the original as an example

- Finally, the **area under the curve (AUC)** is often used as a performance metric
]

.footnote[[1] Popular options include ROC curves, precision-recall curves, gain curves, and lift curves.]

---
class: onecol
## Receiver Operating Characteristic (ROC) Curves

.pull-left[
Each point in a ROC curve corresponds to a possible decision threshold

The performance metrics compared for each point are Sensitivity and Specificity

Better curves are closer to the top-left

The area under the ROC curve (AUC-ROC) ranges from $0.5$ to $1.0$, higher is better.

AUCROC is the probability that a random positive example has a higher estimate than a random negative example.

]

.pull-right[
```{r rocex, echo=FALSE, fig.width=7, fig.height=6.8, out.width='95%'}
test_data <- read_rds("preds.rds")
yardstick::roc_curve(test_data, yes, truth = obs, event_level = "second") %>% 
  arrange(sensitivity) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dotted", size = 2, color = "grey") +
  geom_line(size = 2, color = "darkblue") +
  annotate(geom = "text", x = 0.75, y = 0.125, size = 12, label = "AUC = 0.82", 
           color = "darkblue") +
  coord_fixed() +
  labs(x = "1 - Specificity", y = "Sensitivity") +
  theme_xaringan(text_font_size = 20) +
  theme(
    axis.title = element_text(size = 26),
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```
]

---
class: twocol
## Comprehension Check \#1

<span style="font-size:30px;">Bindi trains Model [A] to predict how many kilometers each bird will migrate this year and Model [B] to predict whether or not it will reproduce this year.</span>

.pull-left[
**1. Which combination of performance metrics would be appropriate to use?**

a) Log Loss for [A] and CCC for [B]

b) Precision for [A] and Recall for [B]

c) MAE for [A] and Balanced Accuracy for [B]

d) None of the above

]

.pull-right[
**2. Which combination of performance scores should Bindi hope to see?**

a) RMSE = 531.6 and AUC-ROC = 0.04

b) RMSE = 1129.7 and AUC-ROC = 0.04

c) RMSE = 531.6 and AUC-ROC = 0.88

d) RMSE = 1129.7 and AUC-ROC = 0.88
]

---
class: inverse, center, middle
# Time for a Break!
```{r countdown, echo=FALSE}
countdown(
  minutes = 10, 
  seconds = 0, 
  right = "33%", 
  left = "33%",
  bottom = "15%",
  color_background = "white",
  color_text = "black",
  color_running_background = "white",
  color_running_text = "black",
  warn_when = 120
)
```
