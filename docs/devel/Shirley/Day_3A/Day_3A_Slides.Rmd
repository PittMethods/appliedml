---
title: '<span style="font-size:48pt;">Regularized Regression</span>'
subtitle: '.big[‚õ∞ ü§† üï∏Ô∏è ]'
author: 'Applied Machine Learning in R </br> Pittsburgh Summer Methodology Series'
date: 'Lecture 3-A &emsp; &emsp; July 21, 2021'
output:
  xaringan::moon_reader:
    css: [xaringan-themer.css, extras.css]
    nature:
      beforeInit: ["macros.js", "cols_macro.js"]
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  par(bg = "#E9ECEF")
)
knitr::opts_knit$set(global.par = TRUE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  footnote_font_size = "20px",
  footnote_color = "gray",
  text_slide_number_font_size = "18px"
)
```

```{r, include=FALSE}
options(width = 60)
local({
  hook_output <- knitr::knit_hooks$get('output')
  knitr::knit_hooks$set(output = function(x, options) {
    if (!is.null(options$max.height)) options$attr.output <- c(
      options$attr.output,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(tidymodels)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(summarytools)
library(mice)
library(VIM)
library(caret)
library(gridExtra)
library(psych)
library(psychTools)
library(ggcorrplot)
library(GGally)
library(corrplot)
library(AppliedPredictiveModeling)
library(MASS)
library(gdata)
library(elasticnet)
library(glmnet)
library(pROC)
```

class: inverse, center, middle
# Overview

<style type="text/css">
.remark-code {
  border: 1px solid grey;
}
</style>

---
## Lecture Topics

.pull-left[
**Linear Regression Review**
- Ordinary least squares 
- Minimizing sum-of-squared-errors
- Limitations of OLS regression

**Introduction to Regularization**
- Why regularize? 
- Bias-variance tradeoff 
- Coefficient paths
- Feature selection
- Tuning hyperparameters
]

.pull-right[
**Ridge**
- $L_2$ penalty
- Parameter shrinkage towards zero

**Lasso**
- $L_1$ penalty
- Some parameters actually go to zero

**Elastic Net**
- Combining penalty terms
- $\lambda$ and $\alpha$ tuning parameters
]

---
class: inverse, center, middle
# Linear Regression Review

---
## Linear Regression 

Linear regression and closely related models (ridge, lasso, elastic net, etc.) can all be written in the form: 

$$y_i = b_0 + b_1x_{i1} + b_2x_{i2} + ... + b_Px_{iP} + e_i$$

where

- $y_i$: value of the response for the $i$th observation
- $b_0$: estimated intercept 
- $b_j$: estimated coefficient for the $j$th predictor
- $x_{ij}$: value of the $j$th predictor for the $i$th observation
- $e_i$: random error unexplained by the model

--

Such models are ![:emphasize](linear in the parameters) because each parameter (e.g., $b_1$, $b_2$, etc.) appears only with a power of 1 and is not multipled or divided by any other parameter. 

Nonlinear *variables* (e.g., $x_1^2$) can still be included in linear regression as long as the *parameters* remain linear.

---
## Ordinary Least Squares Regression

In ordinary least squares regression, the parameters (e.g., $b_1$, $b_2$, etc.) are estimated to ![:emphasize](minimize model bias at the expense of increasing model variance).

</br>
```{r complexity, echo=FALSE, out.width="90%"}
set.seed(2021)
signal <- function(x) {sin(2*pi*x)}
x_linspace <- seq(0, 1, by = 0.02)
x_data <- runif(length(x_linspace), 0, 1)
y_true <- signal(x_linspace)
y_data <- signal(x_data) + rnorm(length(x_data), 0, 0.25)
dat <- tibble(
  x_linspace,
  x_data,
  y_true,
  y_data
)

ggplot(dat) + 
  geom_point(aes(x = x_data, y = y_data), shape = 1, size = 3, color = "grey30") + 
  stat_smooth(
    aes(x = x_data, y = y_data), 
    method = "lm",
    se = FALSE,
    formula = y ~ poly(x, 17), 
    color = "red",
    size = 1.5
  ) +
  coord_cartesian(xlim = c(0, 1)) +
  labs(title = "Overfitting: Low Bias, High Variance",x = "feature", y = NULL) +
  theme_xaringan(text_font_size = 14, title_font_size = 16) +
  theme(panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "white"))

```

---
##Ordinary Least Squares Regression

Specifically, parameters in OLS regression are chosen to minimize the sum-of-squared errors (SSE):

$$SSE = \sum\limits_{i = 1}^n(y_i - \hat{y_i})^2$$

where
- $n$: number of observations 
- $y_1$: $i$th observed response
- $\hat{y_i}$: $i$th predicted response

<br/>
--

However, any dataset is influenced by both the underlying data-generating mechanisms *and* ![:emphasize](sampling error), which by definition is not shared with other samples drawn from the same population. 

---
##Ordinary Least Squares Regression

```{r, echo = FALSE, out.width="60%", fig.align='center'}
include_graphics("overfit.jpg")
```

---
##Ordinary Least Squares Regression

While OLS regression is readily interpretable and easy to compute, there are several noteworthy limitations:

- Risk of ![:emphasize](overfitting)
- ![:emphasize](Inflated) parameter estimates
- Poor ![:emphasize](predictive accuracy) in new datasets
- Sensitive to ![:emphasize](outliers) (adjusting parameter estimates to better accommodate outlier observations with large residuals in order to minimize SSE)
- Cannot handle datasets with high ![:emphasize](multicollinearity)
- Cannot handle datasets with ![:emphasize](more predictors than observations) 
- May not adequately capture ![:emphasize](nonlinear) relationships<sup>1<sup>

<br/>
Regularization addresses many (but not all) of these problems.

.footnote[
[1] This remains a problem with regularized regression models, but see methods for adding nonlinearity to linear models in lecture 2-A (Feature Eningeering).
]

---
class: inverse, center, middle
# Regularization

---
##Why regularize?
.left-column[
<br />
```{r, echo=FALSE}
include_graphics("overfit_graph.png")
```
]
.right-column[
**Reduce Overfitting**

OLS regression models overfit the data and inflate parameter estimates.

Regularized regression adds an additional ![:emphasize](penalty term) to the error function, which constrains parameter estimates and penalizes model complexity.

Compared to OLS regression models, regularized regression models have a ![:emphasize](higher bias) but ![:emphasize](lower variance).

In essense, we make our model less sensitive to the data in the training set in order to achieve higher accuracy in the test set. 
]

---
##Why regularize?
.left-column[
<br />
```{r, echo=FALSE}
include_graphics("feature_selection.png")
```
]
.right-column[
**Feature Selection**

We are often interested in finding a subset of 'good' predictors.

However, there are are many problems<sup>1</sup> with the traditional approach of stepwise regression models.

One benefit of regularization is that it shrinks parameter estimates towards zero. In some cases, such as lasso regression, some parameters are ![:emphasize](actually set to zero).

Thus, regularization models can simultaneously reduce overfitting and perform feature selection.
]

.footnote[
[1] see Harrell (2015) for details and explanation: https://link.springer.com/book/10.1007/978-3-319-19425-7
]

---
## Comprehension check
.pull-left[
### Question 1
**Determine if each of the following models are linear or nonlinear.**

a) $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon$

b) $y = \beta_0 + \beta_1^2x_1 + \beta_2x_2 + \epsilon$

c) $y = \beta_0 + \beta_1x_1^2 + \beta_2\sin(x_1x_2)+ \epsilon$

d) $y = \beta_0 + e^{\beta_1x_1} + \beta_2x_2 + \epsilon$
]

.pull-right[
### Question 2
**Which is not a benefit of regularized compared to nonregularized models?**

a) Feature selection

b) Improves out-of-sample prediction

c) Overcomes measurement errors

d) Limits overfitting
]


---
class: inverse, center, middle
# Ridge Regression

---
## Ridge Regression

Recall the SSE error function for OLS regression: 

$$SSE = \sum\limits_{i = 1}^n(y_i - \hat{y_i})^2$$

--

Ridge regression adds a penalty term to this function known as the $L_2$ penalty: 

$$SSE_{L2} = \sum\limits_{i = 1}^n(y_i - \hat{y_i})^2 + \lambda \sum\limits_{j = 1}^P \beta_j^2$$
where
- $n$: number of observations 
- $y_1$: $i$th observed response
- $\hat{y_i}$: $i$th predicted response
- $P$: number of parameters 
- $\beta_j$: the $j$th parameter

---
## Ridge Regression
.left-column[
<br />
```{r, echo = FALSE}
include_graphics("ridge.png")
```
]
.right-column[
Whereas OLS regression minimizes the difference between the observed and predicted data, ridge regression has an additional goal of ![:emphasize](minimizing) $\color{darkred}\lambda$ ![:emphasize](* the squared value of all parameter estimates).

Thus, parameter estimates in ridge regression are only able to become large if there is a proportional reduction in $SSE_{L2}$. 

The $\lambda$ parameter is a ![:emphasize](hyperparameter) controlling the degree of regularization that ranges from [0, inf].

At higher values of $\lambda$, parameters will be shrunk closer to zero.

We can find the 'best' value of $\lambda$ through cross-validation hyperparameter tuning.
]

---
## Hyperparameter Tuning

```{r, echo = FALSE, out.width="90%"}
include_graphics("hyperparameter_tuning1.png")
```

---
## Hyperparameter Tuning

```{r, echo = FALSE, out.width="90%"}
include_graphics("hyperparameter_tuning2.png")
```

---
## Dataset 

Today, we'll work with a simulated dataset examining predictors of eating disorder severity<sup>1</sup>. Simulated variables include  theoretically *related* and theoretically *unrelated* to eating disorders severity:

.pull-left[
- Emotion regulation
- Depression 
- Impulsivity
- Self-criticism
- Anxiety
- Race (categorical)
- Age
- Family psychiatric history
- Prior psychiatric treatment
- Length of time being in treatment
- Perfectionism
]

.pull-right[
- Temperature that day
- Average rainfall over past month
- Number of siblings
- Number of cellphones owned
- Time spent reading the news
- Time spent watching TV
- Number of pets owned
]


.footnote[
[1] Note: Slides will offer a regression example and live coding will offer a classification example in a different dataset.
]


---
## Ridge Regression in R

```{r, echo = FALSE, results = 'hide'}
# simulate some data with: 
# one continuous outcome variable + one binary outcome variable
# five predictors that are correlated with one another
# five predictors that are uncorrelated with one another
# eight variables that are not predictive
# one predictor that has cutoffs - fine before a certain point

set.seed(1)
emptyCorMat <- diag(x = 1, nrow= 20, ncol=20, names = TRUE)
emptyCorMat[1,2] <- .8

correlated_relationship <- rnorm(n = 25, mean = .8, sd = .1)
correlated_relationship[correlated_relationship > 1] <- .67
emptyCorMat[3:7, 3:7] <- correlated_relationship

uncorrelated_relationship <- rnorm(n = 50, mean = 0, sd = .1)
uncorrelated_relationship[uncorrelated_relationship > 1] <- .1
emptyCorMat[3:12, 8:12] <- uncorrelated_relationship

no_relationship <- rnorm(n = 8*20, mean = 0, sd = .01)
emptyCorMat[,13:20] <- no_relationship

firstRowCor <- c(rnorm(n = 5, mean = .9, sd = .2), rnorm(n = 5, mean = .5, sd = .2))
firstRowCor[firstRowCor>1] <- .82
emptyCorMat[1,3:12] <- firstRowCor

secondRowCor <- c(rnorm(n = 5, mean = .9, sd = .2), rnorm(n = 5, mean = .5, sd = .2))
secondRowCor[secondRowCor>1] <- .76
emptyCorMat[2,3:12] <- secondRowCor

lowerTriangle(emptyCorMat) = upperTriangle(emptyCorMat, byrow=TRUE)
diag(emptyCorMat) <- 1

stddev <- abs(rnorm(20, mean = 1, sd = 0.5))
covMat <- stddev %*% t(stddev) * emptyCorMat
mu <- rnorm(20, mean = 4, sd = 1)
nn <- corpcor::make.positive.definite(covMat)
dat1 <- mvrnorm(n = 500, mu = mu, Sigma = nn, empirical = FALSE)

# nonlinearities -- for decision trees later
indices <- dat1[,20] < mean(dat1[,20])
indices_length <- length(dat1[indices, 1])
newNumbersToAdd <- rnorm(n=indices_length, mean =3, sd = .3)
dat1[indices, 1] <- dat1[indices, 1] + newNumbersToAdd
dat1[indices, 2] <- dat1[indices, 2] + (newNumbersToAdd * rnorm(n = indices_length, mean = 1, sd = .3))

dat1[,2] <- dplyr::case_when(
  dat1[,2] <= mean(dat1[,2])~ 0,
  TRUE ~ 1) 

for (i in 1:ncol(dat1)){
  if (min(dat1[, i]) < 0) {
    dat1[, 1] <- dat1[, 1] + min(dat1[, i])
  }
}

cor(as.data.frame(dat1))

EDsim <- as.data.frame(dat1)
varnames <- c("ED_severity", "ED_diagnosis", 
              "emo_reg", "depression", "impulsivity", "self_crit", "anxiety",
              "race", "age", "family_history", "prior_treatment", "perfectionism",
              "day_temperature", "rainfall", "num_siblings", "num_cellphones", "time_news", "time_tv", "num_pets",
              "treatment_length")
names(EDsim) <- varnames
EDsim <- EDsim %>% mutate(ED_diagnosis = factor(ED_diagnosis, labels = c("no_ED", "ED")), race = as.factor(floor(race)), age = age + 14, num_siblings = floor(num_siblings), num_cellphones = floor(num_cellphones), num_pets = floor(num_pets))
```
**Step 0: Split into a training (including cross-validation) and held-out test set.**

```{r, out.width="50%"}
# use caret for simple data split
library(caret)
set.seed(2021)
trainIndex <- createDataPartition(EDsim$ED_severity, p = 0.8, list = FALSE, times = 1)
EDsim_train <- EDsim[trainIndex, ]
EDsim_test <- EDsim[-trainIndex, ]
```

--

```{r}
# check data splitting
dim(EDsim_train)
dim(EDsim_test)
```

---
## Ridge Regression in R 

**Step 1: Exploratory Data Analysis** 

```{r, echo = FALSE}
print(dfSummary(EDsim_train, valid.col = FALSE, headings = FALSE, style = 'grid', 
                plain.ascii = FALSE, graph.magnif = 0.85, tmp.img.dir = "/tmp"),
      max.tbl.height = 300, method = 'render')
```

---
## Ridge Regression in R 

**Step 1: Exploratory Data Analysis**
```{r, echo = FALSE}
cormat <- cor(EDsim_train[, sapply(EDsim_train, is.numeric)])
par(bg = "#E9ECEF")
corrplot(cormat, tl.col = '#23395b', type = 'lower', tl.cex = 0.8, mar = c(0, 0, 0, 10))
```

---
## Ridge Regression in R

**Step 2: Feature Engineering**<sup>1</sup>. 

Normalizing (standardizing) features before regularization is important to ensure that the shrinkage parameter $\lambda$ affects all features equally.


```{r}
EDsim_recipe <- 
  EDsim_train %>% 
  recipe(ED_severity ~ .) %>%
  step_rm(ED_diagnosis) %>%
  step_normalize(all_numeric_predictors()) %>% #<<
  step_dummy(all_nominal_predictors()) 
```

.footnote[
[1] Note than when using `recipes`, dummy variables should be created manually with `step_dummy`. If using `caret` by itself without `recipes`, dummy variables are created automatically. 
]

---
## Ridge Regression in R 

**Step 3: Set model training and tuning methods in `caret::traincontrol()`**

```{r}
EDsim_control <- trainControl(method = 'repeatedcv',
                              number = 10,
                              repeats = 3, 
                              classProbs = FALSE,
                              summaryFunction = defaultSummary)
```

<br/>
This sets our up training procedure with the following specifications: 

- 10-fold cross-validation, repeated 3 times
- Other commonly used options include `method = 'cv'` for a single (non-repeated) cross-validation and `method = 'boot'` for bootstrapping
- For classification, set `classProbs = TRUE` and `summaryFunction = twoClassSummary` or `summaryFunction = prSummary`

---
## Ridge Regression in R

Note that the cross-validation process involves choosing random indices for data splitting. To ensure reproducibility, you can set seeds to use for resampling in the model training process:

```{r}
set.seed(2021)
seeds <- vector(mode = "list", length = 30) # length = n data splits #<<
for(i in 1:30) seeds[[i]] <- sample.int(1000, 21) #<<
seeds[[31]] <- sample.int(1000, 1) # for the last model  #<<
```

--

This `seeds` object can be included in the `trainControl` function to obtain the same resampling seeds each time the code is run for fully reproducible results. 

```{r}
EDsim_control <- trainControl(method = 'repeatedcv',
                              number = 10,
                              repeats = 3,
                              classProbs = FALSE,
                              summaryFunction = defaultSummary,
                              seeds = seeds) #<<
```

---
## Ridge Regression in R 

**Step 4: Set a tuning grid** (optional)

There are several ways to set a tuning grid in `caret`. 

The first option is to do nothing, as `caret` will automatically create a tuning grid if one is not specified. By default, `caret::train()` creates a tuning grid of size $3^P$, where $P$is the number of tuning parameters. 

--

The second option is to specify the length of the tuning grid in the `caret::train()` function: 

```{r, eval = FALSE}
train(Y ~ X, method = 'glmnet', tuneLength = 10)
```

--

The third option is to specify the grid yourself to call in `caret::train()`:

```{r}
ridgegrid <- expand.grid(alpha = 0, lambda = seq(0, 1, 0.01))
```

This option is useful for training ridge regression models within the `glmnet` method in `caret`, which allows for nice data visualizations. However, `caret` will automatically create a grid of both $\alpha$ and $\lambda$ values for `method = 'glmnet'`, but we can avoid that by setting the tuning grid ourselves. 

---
## Ridge Regression in R

**Step 5: Train your model using `caret::train()`**

```{r}
set.seed(2021)
ridgefit <- train(EDsim_recipe, data = EDsim_train,
                  method = 'glmnet',
                  tuneGrid = ridgegrid,
                  trControl = EDsim_control)
```

--

.scroll-output[
```{r}
ridgefit
```
]

---
## Ridge Regression in R 

Examine results of internal cross-validation: 

.scroll-output[
```{r}
ridgefit$results
```
]

---
## Ridge Regression in R

**Step 6: Predict on held-out test set**

```{r}
EDsim_test_baked <- 
  EDsim_recipe %>%
  prep(training = EDsim_train) %>% #<<
  bake(new_data = EDsim_test) #<<

ridgepred_test <- predict(ridgefit, newdata = EDsim_test)
  
postResample(pred = ridgepred_test, obs = EDsim_test_baked$ED_severity)
```

---
## Ridge Regression in R

Examine accuracy as a function of $\lambda$. 

```{r, eval = FALSE}
ggplot(ridgefit)
```

```{r, echo = FALSE}
ggplot(ridgefit) +
  theme_xaringan(text_font_size = 14, title_font_size = 18) +
  theme(panel.grid.minor = element_blank(), panel.background = element_rect(fill = "white"))
```

---
## Ridge Regression in R

Examine coefficients as a function of $\lambda$.

```{r}
library(coefplot)
coefpath(ridgefit$finalModel)
```

---
## Ridge Regression in R

Examine variable importance.

```{r, eval = FALSE}
ggplot(varImp(ridgefit, scale = FALSE))
```

```{r, echo = FALSE}
ggplot(varImp(ridgefit, scale = FALSE)) +
  theme_xaringan(text_font_size = 10, title_font_size = 18) +
  theme(panel.grid.minor = element_blank(), panel.background = element_rect(fill = "white"))
```

---
class: inverse, center, middle
# Lasso Regression

---
## Lasso Regression

Lasso stands for the ![:emphasize](Least Absolute Shrinkage and Selection Operator). 

Similar to ridge regression, lasso regression adds an additional penalty term (in this case, the $L1$ penalty) to the OLS error function:

$$SSE_{L1} = \sum\limits_{i = 1}^n(y_i - \hat{y_i})^2 + \lambda \sum\limits_{j = 1}^P \lvert \beta_j \rvert$$
where
- $n$: number of observations 
- $y_1$: $i$th observed response
- $\hat{y_i}$: $i$th predicted response
- $P$: number of parameters 
- $\beta_j$: the $j$th parameter 

---
## Lasso Regression
.left-column[
<br />
```{r, echo = FALSE}
include_graphics("lasso.png")
```
]
.right-column[
Here, we aim to minimize the **absolute value** of all parameters (rather than the square of parameters as with ridge). 

While this may seem like a small change, it has the effect of ![:emphasize](fully penalizing some parameter values down to zero), whereas ridge only shrinks values **towards** zero. 

If $\lambda$ is set high enough, all parameters will be shrunk to zero. 

Ridge and lasso also differ in their handling of **multicollinearity.**

Whereas ridge tends to shrink coefficients of correlated predictors towards each other, lasso tends to **pick one and ignore the rest**. 
]

---
## Lasso Regression in R

**Set tuning grid and train model** 

```{r}
lassogrid <- expand.grid(alpha = 1, lambda = seq(0, 1, 0.01)) #<<
set.seed(2021)
lassofit <- train(EDsim_recipe, data = EDsim_train,
                  method = 'glmnet',
                  tuneGrid = lassogrid, #<<
                  trControl = EDsim_control)
```

--

.scroll-output[
```{r}
lassofit
```
]

---
## Lasso Regression in R 

Examine results of internal cross-validation: 

.scroll-output[
```{r}
lassofit$results
```
]

--

**Predict on held-out test set**

```{r}
lassopred_test <- predict(lassofit, EDsim_test)
postResample(pred = lassopred_test, obs = EDsim_test_baked$ED_severity)
```

---
## Lasso Regression in R

```{r, eval = FALSE}
ggplot(lassofit) 
```

```{r, echo = FALSE}
ggplot(lassofit) +
  theme_xaringan(text_font_size = 14, title_font_size = 18) +
  theme(panel.grid.minor = element_blank(), panel.background = element_rect(fill = "white"))
```

---
## Lasso Regression in R

```{r}
coefpath(lassofit$finalModel)
```

---
## Lasso Regression in R

```{r, eval = FALSE}
ggplot(varImp(lassofit, scale = FALSE))
```

```{r, echo = FALSE}
ggplot(varImp(lassofit, scale = FALSE)) +
  theme_xaringan(text_font_size = 10, title_font_size = 18) +
  theme(panel.grid.minor = element_blank(), panel.background = element_rect(fill = "white"))
```

---
class: inverse, center, middle
# Elastic Net Regression


---
## Elastic Net Regression

Elastic net regression is a generalization of the lasso model, which combines both $L1$ (lasso) and $L2$ (ridge) penalty terms together in the error function:

$$SSE_{EN} = \sum\limits_{i = 1}^n(y_i - \hat{y_i})^2 + \lambda_1 \sum\limits_{j = 1}^P \beta_j^2 + \lambda_2 \sum\limits_{j = 1}^P \lvert \beta_j \rvert$$

where
- $n$: number of observations 
- $y_1$: $i$th observed response
- $\hat{y_i}$: $i$th predicted response
- $P$: number of parameters 
- $\beta_j$: the $j$th parameter 

---
## Elastic Net
.left-column[
<br />
```{r, echo = FALSE}
include_graphics("elasticnet.png")
```
]
.right-column[
The elastic net model allows for effective ridge-like regularization with lasso-like feature selection.

In `caret`, we now tune two hyperparameters: $\lambda$ and $\alpha$.

As with ridge and lasso, $\lambda$ controls the degree of regularization. 

The new $\alpha$ hyperparameter is called the ![:emphasize](elastic net mixing parameter), and ranges from [0, 1]. 

At $\alpha = 0$ ridge is performed and at $\alpha = 1$ lasso is performed.  

Elastic net is particularly good at handling correlated predictors. 
]

---
## Elastic Net in R 

**Set tuning grid and train model** 

```{r}
enfit <- train(EDsim_recipe, data = EDsim_train,
               method = 'glmnet',
               tuneLength = 20, #<<
               trControl = EDsim_control)
```

--

.scroll-output[
```{r}
enfit
```
]

---
## Elastic Net in R 

Examine results of internal cross-validation: 

.scroll-output[
```{r}
enfit$results
```
]

--

**Predict on held-out test set**

```{r}
enpred_test <- predict(enfit, EDsim_test)
postResample(pred = enpred_test, obs = EDsim_test_baked$ED_severity)
```

---
## Elastic Net in R 

```{r, eval = FALSE}
ggplot(enfit) 
```

```{r, echo = FALSE}
ggplot(enfit) +
  theme_xaringan(text_font_size = 14, title_font_size = 18) +
  theme(panel.grid.minor = element_blank(), panel.background = element_rect(fill = "white"))
```

---
## Elastic Net in R 

```{r}
coefpath(enfit$finalModel)
```

---
## Elastic Net in R 

```{r, eval = FALSE}
ggplot(varImp(enfit, scale = FALSE))
```

```{r, echo = FALSE}
ggplot(varImp(enfit, scale = FALSE)) +
  theme_xaringan(text_font_size = 10, title_font_size = 18) +
  theme(panel.grid.minor = element_blank(), panel.background = element_rect(fill = "white"))
```

---
## Comprehension check
.pull-left[
### Question 1
**Which model uses an $L_2$ squared penalty?**

a) Ridge

b) Lasso

c) Elastic Net

d) None of the above
]

.pull-right[
### Question 2
**What do the $\lambda$ and $\alpha$ hyperparameters refer to?**

a) $\lambda$ = shrinkage, $\alpha$ = validation

b) $\lambda$ = feature selection, $\alpha$ = mixing

c) $\lambda$ = shrinkage, $\alpha$ = mixing

d) $\lambda$ = feature selection, $\alpha$ = shrinkage
]

---
class: inverse, center, middle
# Live Coding

---
## Live Coding Activity

**Live Coding**: I will walk through examples of ridge, lasso, and elastic net for classification in RStudio. 

- You can follow along in your own RStudio (easiest with 1 large or 2+ monitors)
- Or you can download `Day_3A_Activity.Rmd` and follow along
- In either case, please download the `heart.csv` and `hcp_memory.csv` files. 

.footnote[
All files are on the workshop OSF page: https://osf.io/3qhc8/. 
]

--
</br>

**Small Group Activity**: Afterwards, we will split you into small breakout room groups to practice a full machine learning workflow using a new dataset. 

If you have any questions, please post them in the chat or workshop Slack channel. 

We will also float between different breakout rooms to answer questions. 


