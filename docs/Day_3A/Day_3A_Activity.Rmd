---
title: "Day 3_A Activity <br /> (Regularized Regression)"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    df_print: paged
editor_options: 
  chunk_output_type: inline
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  df_print = "paged"
)
```

# Live Coding 
Goal: Classify likelihood of having a heart attack based on demographics (age, sex) and health variables (type of chest pain, resting blood pressure, cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate, exercise induced angina, etc.). Compare performance of ridge, lasso, and elastic net models. 

## Load Packages

```{r, message=FALSE}
if (!require("tidyverse")) {install.packages("tidyverse"); require("tidyverse")}     
if (!require("caret")) {install.packages("caret"); require("caret")}
if (!require("tidymodels")) {install.packages("tidymodels"); require("tidymodels")}
if (!require("summarytools")) {install.packages("summarytools"); require("summarytools")}
if (!require("psych")) {install.packages("psych"); require("psych")}
if (!require("coefplot")) {install.packages("coefplot"); require("coefplot")}
if (!require("pROC")) {install.packages("pROC"); require("pROC")}
```

## Read in data 
```{r}
heart <- read_csv("https://bit.ly/amlr-heart", col_types = "dffddffdfdiiif")
```

## Split into train/test sets 
```{r}
set.seed(2021)
trainIndex <- createDataPartition(heart$output, p = 0.8, list = FALSE)
heart_train <- heart[trainIndex, ]
heart_test <- heart[-trainIndex, ]

# check to see that data splits were correct
dim(heart_train)
dim(heart_test)
```

## Quick EDA: View summary statistics and descriptives
```{r}
print(dfSummary(heart_train), method = 'render')
```

## Feature Engineering: Normalize (Standardize) Training and Test Sets
```{r}
heart_recipe <- 
  heart_train %>% 
  recipe(output ~ .) %>%
  step_mutate(output = factor(output, levels = c(0, 1), labels = c("low_risk", "high_risk"))) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())

# normalize test dataset too 
heart_test_baked <- 
  heart_recipe %>%
  prep(training = heart_train) %>%
  bake(new_data = heart_test)
```

## Set up Resampling Methods
```{r}
# let's use 5-fold cross-validation, repeated 3 times 

# set train control
heart_control <- trainControl(method = 'repeatedcv',
                              number = 5,
                              repeats = 3,
                              classProbs = TRUE)

# set ridge tuning grid
ridgegrid <- expand.grid(alpha = 0, lambda = seq(0, 1, 0.01))

# set lasso tuning grid
lassogrid <- expand.grid(alpha = 1, lambda = seq(0, 1, 0.01))
```

## Fit Models
```{r}
ridgefit <- train(heart_recipe, data = heart_train,
                  method = 'glmnet',
                  family = "binomial",
                  tuneGrid = ridgegrid,
                  trControl = heart_control)

lassofit <- train(heart_recipe, data = heart_train,
                  method = 'glmnet',
                  family = "binomial",
                  tuneGrid = lassogrid,
                  trControl = heart_control)

enfit <- train(heart_recipe, data = heart_train,
               method = 'glmnet',
               family = "binomial",
               tuneLength = 20,
               trControl = heart_control)
```

## Examine Models 

```{r}
describe(ridgefit$results)
describe(lassofit$results)
describe(enfit$results)
```

## Plot Accuracy by Hyperparameters
```{r}
ggplot(ridgefit)
ggplot(lassofit)
ggplot(enfit)
```

## Plot Coefficients by Hyperparameters
```{r}
coefpath(ridgefit$finalModel)
coefpath(lassofit$finalModel)
coefpath(enfit$finalModel)
```


## Evaluate Classification Accuracy
```{r}
## Ridge
ridge_preds <- predict(ridgefit, newdata = heart_test)
ridge_prob <- predict(ridgefit, newdata = heart_test, type = "prob")

# Collect all predicted and trusted labels in a dataframe (for {yardstick})
collected_labels_ridge <- bind_cols(
  ridge_prob,
  true = heart_test_baked$output, 
  pred = ridge_preds)

cm_log <- conf_mat(collected_labels_ridge, truth = true, estimate = pred)
summary(cm_log)
roc_auc(collected_labels_ridge, truth = true, high_risk, event_level = "second")

## Lasso
lasso_preds <- predict(lassofit, newdata = heart_test)
lasso_prob <- predict(lassofit, newdata = heart_test, type = "prob")

# Collect all predicted and trusted labels in a dataframe (for {yardstick})
collected_labels_lasso <- bind_cols(
  lasso_prob,
  true = heart_test_baked$output, 
  pred = lasso_preds)

cm_lasso <- conf_mat(collected_labels_lasso, truth = true, estimate = pred)
summary(cm_lasso)
roc_auc(collected_labels_lasso, truth = true, high_risk, event_level = "second")

## Elastic Net
en_preds <- predict(enfit, newdata = heart_test)
en_prob <- predict(enfit, newdata = heart_test, type = "prob")

# Collect all predicted and trusted labels in a dataframe (for {yardstick})
collected_labels_en <- bind_cols(
  en_prob,
  true = heart_test_baked$output, 
  pred = en_preds)

cm_en <- conf_mat(collected_labels_en, truth = true, estimate = pred)
summary(cm_en)
roc_auc(collected_labels_en, truth = true, high_risk, event_level = "second")
```


# Hands-On Activity

The goal of this activity is to practice a full workflow of using machine learning models (ridge, lasso, and elastic net). 

In this activity, your goal is to predict scores on a memory test based on neuroimaging data from 18 brain regions involved in memory. Specifically, we will be using resting state fMRI connectivity from the Human Connectome Project (http://www.humanconnectomeproject.org/), which provides measures of the extent to which any given 2 brain regions are interacting. Please note that these data have been artificially modified to increase predictive power and make the activity more engaging.

These fMRI data were first reduced to a simple matrix for each participant, with each participant having an 18 x 18 matrix of brain connections data (e.g., the entry (1, 2) represents the strength of the connection between brain region 1 and brain region 2). We then flattened these matrices for our final data setup, where each row is a participant and each column is the connection strength between 2 brain regions. 

Overview of this activity: 

1. Split data into training and testing subsets
2. Feature engineering 
3. Fit a linear regression on the training data
4. Fit an elastic net model on the training data (use internal cross-validation to determine the best hyperparameters)
5. Evaluate how well the standard regression predicts scores on the test set
6. Evaluate how well elastic net predicts scores on the test set. 
7. Compare standard regression vs. elastic net

## Load data
Read in the dataset from "https://bit.ly/amlr-memory". 

```{r}

```

## Split into training and testing datasets
Your training set should contain 80% of your data; your testing set should contain 20%.
(remember to set a seed so your results are reproducible!)

```{r}

```

## Feature Engineering
Normalize (standardize) features in your training and test sets. 

```{r}

```

## Training Methods
Set your model training and tuning methods using trainControl(). Use 10-fold internal cross-validation (no repeats).

```{r}

```

## Fit Linear Regression 
Use train() to fit a linear regression and examine the results of internal cross-validation. Examine variable importance. 

```{r}

```

## Fit Elastic Net
Use train() to fit an elastic net model with a tune length of 20, and examine the results of internal cross-validation. Examine the effect of tuning parameters on accuracy and coefficient values, as well as variable importance. 

```{r}

```

## Evaluate Models
Use the standard linear regression and the elastic net model to predict your held-out test set. 
Which model does better? Why? 

```{r}

```




# Bonus: Fit Ridge and Lasso
If you finish the above exercises with time remaining, use your training data to also fit ridge and lasso regression models. Predict the held-out test data with ridge and lasso, and compare to your standard regression and elastic net. What similarities and differences do you notice between all the models? Which one performs the best?

```{r}


```


# Answer key

<details><summary>Click here to view the answer key to the hands-on activity</summary>

## Load data
Read in the dataset from "https://bit.ly/amlr-memory". 

```{r}
memory <- read.csv("https://bit.ly/amlr-memory")
```

## Split into training and testing datasets
Your training set should contain 80% of your data; your testing set should contain 20%.
(remember to set a seed so your results are reproducible!)

```{r}
set.seed(2021)
index <- createDataPartition(memory$memory_score, p = 0.8, list = FALSE)
memory_train <- memory[index, ]
memory_test <- memory[-index, ]
dim(memory_train)
dim(memory_test)
```

## Feature Engineering
Normalize (standardize) features in your training and test sets. 

```{r}
memory_recipe <- 
  memory_train %>%
  recipe(memory_score ~ .) %>% 
  step_normalize(all_numeric_predictors())

# normalize test dataset too 
memory_test_baked <-
  memory_recipe %>%
  prep(training = memory_train) %>%
  bake(new_data = memory_test)
```

## Training Methods
Set your model training and tuning methods using trainControl(). Use 10-fold internal cross-validation (no repeats).

```{r}
# set train control
memory_control <- trainControl(method = 'cv',
                               number = 10)
```

## Fit Linear Regression 
Use train() to fit a linear regression and examine the results of internal cross-validation. Examine variable importance. 

```{r}
linfit_memory <- train(memory_recipe, data = memory_train,
                       method = 'lm', 
                       trControl = memory_control)
```

## Fit Elastic Net
Use train() to fit an elastic net model with a tune length of 20, and examine the results of internal cross-validation. Examine the effect of tuning parameters on accuracy and coefficient values, as well as variable importance. 

```{r}
enfit_memory <- train(memory_recipe, data = memory_train,
                      method = 'glmnet',
                      tuneLength = 20,
                      trControl = memory_control)
```

## Evaluate Models
Use the standard linear regression and the elastic net model to predict your held-out test set. 
Which model does better? Why? 

```{r}
linpred_test_memory <- predict(linfit_memory, newdata = memory_test)
enpred_test_memory <- predict(enfit_memory, newdata = memory_test)

postResample(pred = linpred_test_memory, obs = memory_test_baked$memory_score)
postResample(pred = enpred_test_memory, obs = memory_test_baked$memory_score)
```

```{r}
ggplot(enfit_memory)
```

</details>

