---
title: "Day XX Activity (Decision Trees and Random Forests)"
output:
  html_notebook:
    df_print: paged
editor_options: 
  chunk_output_type: inline
---



# Live Coding 
Goal: Classify likelihood of having a heart attack based on demographics (age, sex) and health variables (type of chest pain, resting blood pressure, cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate, exercise induced angina, etc.). Compare performance of fully grown trees, pruned trees, trees grown with a stopping parameter, and random forests.


## Load Packages

```{r, message=FALSE}
if (!require("tidyverse")) {install.packages("tidyverse"); require("tidyverse")}     
if (!require("caret")) {install.packages("caret"); require("caret")}
if (!require("tidymodels")) {install.packages("tidymodels"); require("tidymodels")}
if (!require("summarytools")) {install.packages("summarytools"); require("summarytools")}
if (!require("psych")) {install.packages("psych"); require("psych")}
if (!require("coefplot")) {install.packages("coefplot"); require("coefplot")}
if (!require("pROC")) {install.packages("pROC"); require("pROC")}
if (!require("rpart")) {install.packages("rpart"); require("rpart")}

```


## Read in data 
```{r}
heart <- read.csv("http://people.fas.harvard.edu/~mair/datasets/Heart.csv", row.names = 1) %>% tibble()

#heart <- read.csv("heart.csv")

# Let's convert our outcome variable AHD, to numeric. This makes our accuracy/ROC calculations easier later.
# if we just run as.numeric(), we get 1's and 2's. Let's subtract 1 from each response, so that
# 0 corresponds to no, 1 to yes.

heart$AHD<- as.numeric(heart$AHD) - 1 
heart$AHD 

# Let's also remove data with missing values
heart <- na.omit(heart) # removes any rows with any missing data

```

## Split into train/test sets 
```{r}
set.seed(2021)
trainIndex <- createDataPartition(heart$AHD, p = 0.8, list = FALSE, times = 1)
heart_train <- heart[trainIndex, ]
heart_test <- heart[-trainIndex, ]

# check to see that data splits were correct
dim(heart_train)
dim(heart_test)
```

## Fit full decision tree on training set

We'll step outside of the caret world for a second to fit this tree. It's something you wouldn't typically do (you'll see why in a minute), which is why it's harder to implement in caret this way

```{r}
fullTreeFit <- rpart(AHD ~ ., data = heart_train,
                     control = rpart.control(minsplit = 2, # minimum number of items in a node, in order to try to split
                                             minbucket = 1, # minimum number of items in the terminal leaf
                                             cp = 0.00 #complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. set to 0 so that it splits all the way down
                                             ))

plot(fullTreeFit)
text(fullTreeFit, use.n = TRUE, all = TRUE)

```

## evaluate classification accuracy

Let's calculate how well it does on the training set, relative to the test set. This gives us a sense of how over-fit the data is on our training set

```{r}

full_tree_train_preds <- predict(fullTreeFit, newdata = heart_train)
full_tree_test_preds <- predict(fullTreeFit, newdata = heart_test)

head(full_tree_train_preds) 

roc(predictor = full_tree_train_preds, response = heart_train$AHD)
roc(predictor = full_tree_test_preds, response = heart_test$AHD)

```

So we are fully overfitting - we have perfect prediction on our training set. Our AUC drops on our test set ~ .7. Let's see if we can improve on this by having modifying our approach.

We'll be using caret, so let's make sure our outcome variable is now a factor.

```{r}
heart_train$AHD <- as.factor(heart_train$AHD)
```

## Preventing the tree from overfititng to training data: Pruning 
```{r}
pruned_tree_model <- train(
  AHD ~., data = heart_train, method = "rpart",
  trControl = trainControl("cv", number = 10)
  )

plot(pruned_tree_model)

```

If we wanted to make sure we got the best possible model, what could we do now?

We could create our own array of values that the complexitiy parameter could take on. Let's say .001 to .05, in steps of .001.
We could then run the above again.

## Preventing the tree from overfititng to training data: Stopping criteria (e.g. max depth)

We'll be using the 'rpart2' method in caret. This grow a decision tree, but will stop growing after a certain number of splits (governed by a parameter called 'maxdepth'). We'll be using crossvalidation to pick the best maxdepth. We could give caret a series of maxdepth values it should try out, or we can let it try to find them automatically. Here, we'll let caret do it automatically.


```{r}

early_stopped_tree_model <- train(
  AHD ~., data = heart_train, method = "rpart2",
  trControl = trainControl("cv", number = 10)
  )

plot(early_stopped_tree_model)

```



## evaluate classification accuracy on test set

Let's calculate how well it does on the training set, relative to the test set. This gives us a sense of how over-fit the data is on our training set

```{r}

full_tree_test_preds <- predict(fullTreeFit, newdata = heart_test)
pruned_tree_test_preds <- predict(pruned_tree_model, newdata = heart_test)
early_stopped_tree_test_preds <- predict(early_stopped_tree_model, newdata = heart_test)


roc(predictor = full_tree_test_preds, response = heart_test$AHD)
roc(predictor = as.numeric(pruned_tree_test_preds), response = heart_test$AHD)
roc(predictor = as.numeric(early_stopped_tree_test_preds), response = heart_test$AHD)


```

A bit better, but not by too much. In other datasets you might see a bigger effect. So how do we improve more significantly on this? 


## Preventing the tree from overfitting to training data: random forests
These trees are very interpretable: for any data point you just follow the leaves down, and you see exactly how you arrive at your prediction. 

What's a solution to unstable trees (and less generalization to new data)?

  - Find a robust prediction by fitting many trees on subsets of data and averaging their predictions
  
     What happens under the hood:
         - sample from your rows to create ‘new’ datasets, fit a tree on each ‘new’ dataset.
          - this allows us to have some bad models, but their errors get averaged away by good models
  
  New problem:  averaging different models to get a better fit only works if the models are different
  from one another. 
  New Solution:
    To make sure the models are different, each tree only gets to use a random subset of our
    predictors
    
We create many trees (i.e. a forest), using random resampling of data and random subsets of predictors. Hence, the name 'random forests'

Let's fit random forests on the same dataset, and see how it compares.

## fit random forests
```{r}
rf_fit <- train(
  AHD ~., data = heart_train, method = "rf",
  trControl = trainControl("cv", number = 10)
  )

plot(rf_fit)

```
It looks like randomly selecting two predictors at each step leads to the best predictions. We can verify that below:

```{r}

#what was the best tuning parameter ?
rf_fit$bestTune

```


```{r}
rf_test_preds <- predict(rf_fit, newdata = heart_test)
roc(predictor = as.numeric(rf_test_preds), response = heart_test$AHD)
roc(predictor = full_tree_test_preds, response = heart_test$AHD)

```

Random forests do much better than the original full tree!

## What does the tree look like?

We don't know... there are many trees, so it's hard to gain insight, since there is not 'one model'. Instead, we can figure out what what the 'important variables' are across the different trees.


## Extract important variables

```{r}

varImp(rf_fit)

```
```{r}

# relative importance plot:

varImp(rf_fit) %>% plot()

```




## Try it for yourself: random forests to predict Alzheimer's

Washington University conducted a clinical study to determine if biological measurements made from cerebrospinal fluid (CSF) can be used to diagnose or predict Alzheimer's disease (Craig-Schapiro et al. 2011). These data are a modified version of the values used for the publication.

The R factor vector diagnosis contains the outcome data for 333 of the subjects. The demographic and laboratory results are collected in the data frame predictors.

Let's read in the data. You may have to install the package 'AppliedPredictiveModeling' first.

```{r}

# the data lives in the AppliedPredictiveModeling package, so if not installed, install, then require R to import it
if (!require("AppliedPredictiveModeling")) {install.packages("AppliedPredictiveModeling"); require("AppliedPredictiveModeling")} 


data(AlzheimerDisease)
predictors$E2 <- predictors$E3 <- predictors$E4 <- 0      ## just some variable preparation. dummycode Genotype
predictors$E2[grepl("2", predictors$Genotype)] <- 1
predictors$E3[grepl("3", predictors$Genotype)] <- 1
predictors$E4[grepl("4", predictors$Genotype)] <- 1
predictors$Genotype <- NULL
head(predictors)
dim(predictors)           ## 333 persons, 130 predictors
table(diagnosis)          ## response

```

Perform data splitting: use 80% in your training set, 20% in your testing set

```{r}

```

# train random forests. 
```{r}

```

# evaluate performance on the test set

```{r}

```

# determine which variables were important in this prediction

```{r}

```

  




