---
title: "Day 3-B Activity <br /> (Decision Trees and Random Forests)"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    df_print: paged
editor_options: 
  chunk_output_type: inline
---

# Live Coding 
Goal: Classify likelihood of having a heart attack based on demographics (age, sex) and health variables (type of chest pain, resting blood pressure, cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate, exercise induced angina, etc.). Compare performance of fully grown trees, pruned trees, trees grown with a stopping parameter, and random forests.


## Load Packages
```{r, message=FALSE}
if (!require("tidyverse")) {install.packages("tidyverse"); require("tidyverse")}     
if (!require("caret")) {install.packages("caret"); require("caret")}
if (!require("tidymodels")) {install.packages("tidymodels"); require("tidymodels")}
if (!require("summarytools")) {install.packages("summarytools"); require("summarytools")}
if (!require("psych")) {install.packages("psych"); require("psych")}
if (!require("coefplot")) {install.packages("coefplot"); require("coefplot")}
if (!require("pROC")) {install.packages("pROC"); require("pROC")}
if (!require("rpart")) {install.packages("rpart"); require("rpart")}
if (!require("randomForest")) {install.packages("randomForest"); require("randomForest")}
if (!require("AppliedPredictiveModeling")) {install.packages("AppliedPredictiveModeling"); require("AppliedPredictiveModeling")} 
```


## Load data
```{r}
heart <- read.csv("heart.csv")
```

## Split into train/test sets 
```{r}
set.seed(2021)
trainIndex <- createDataPartition(heart$output, p = 0.8, list = FALSE, times = 1)
heart_train <- heart[trainIndex, ]
heart_test <- heart[-trainIndex, ]

# check to see that data splits were correct
dim(heart_train)
dim(heart_test)
```

## Fit full decision tree on training set

We'll step outside of the caret world for a second to fit this tree. It's something you wouldn't typically do (you'll see why in a minute), which is why it's harder to implement in caret this way.

```{r}
fullTreeFit <- rpart(output ~ ., data = heart_train,
                     control = rpart.control(minsplit = 2, # minimum number of items in a node, in order to try to split
                                             minbucket = 1, # minimum number of items in the terminal leaf
                                             cp = 0.00 #complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. set to 0 so that it splits all the way down
                                             ))

plot(fullTreeFit)
text(fullTreeFit, use.n = TRUE, all = TRUE)
```

## Classification Accuracy: Fully Grown Trees

Let's calculate how well it does on the training set, relative to the test set. This gives us a sense of how over-fit the data is on our training set

```{r}
full_tree_train_preds <- predict(fullTreeFit, newdata = heart_train)
full_tree_test_preds <- predict(fullTreeFit, newdata = heart_test)

head(full_tree_train_preds) 

roc(predictor = full_tree_train_preds, response = heart_train$output)
roc(predictor = full_tree_test_preds, response = heart_test$output)
```

We are definitely overfitting with perfect prediction on our training set! Our AUC drops on our test set ~ .7. Let's see if we can improve on this by having modifying our approach.

## Create Training Recipe
```{r}
heart_recipe <- 
  heart_train %>% 
  recipe(output ~ .) %>%
  step_num2factor(output, transform = function(x) x + 1, levels = c("low_risk", "high_risk")) %>%
  step_dummy(all_nominal_predictors())

# normalize test dataset too 
heart_test_baked <- 
  heart_recipe %>%
  prep(training = heart_train) %>%
  bake(new_data = heart_test)
```

## Set up Training and Tuning Methods
```{r}
# let's use 5-fold cross-validation, repeated 3 times 

# set train control
heart_control <- trainControl(method = 'repeatedcv',
                              number = 5,
                              repeats = 3,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)
```

## Reduce Overfitting by Pruning the Tree
```{r}
pruned_tree_model <- train(heart_recipe, data = heart_train, 
                           method = "rpart",
                           metric = 'ROC',
                           tuneLength = 20,
                           trControl = heart_control)

plot(pruned_tree_model)

plot(pruned_tree_model$finalModel)
text(pruned_tree_model$finalModel)
```

If we wanted to make sure we got the best possible model, what could we do now?

We could create our own array of values that the complexitiy parameter could take on. Let's say .001 to .05, in steps of .001.
We could then run the above again.

## Reduce Overfitting with Stopping Criteria

```{r}
early_stopped_tree_model <- train(heart_recipe, data = heart_train, 
                                  method = "rpart2",
                                  metric = 'ROC',
                                  tuneLength = 20,
                                  trControl = heart_control)

plot(early_stopped_tree_model)

plot(early_stopped_tree_model$finalModel)
text(early_stopped_tree_model$finalModel)
```

## Classification Accuracy: Pruning and Stopping Criteria

Let's calculate how well it does on the training set, relative to the test set. This gives us a sense of how over-fit the data is on our training set

```{r}
full_tree_test_preds <- predict(fullTreeFit, newdata = heart_test)
pruned_tree_test_preds <- predict(pruned_tree_model, newdata = heart_test)
early_stopped_tree_test_preds <- predict(early_stopped_tree_model, newdata = heart_test)

roc(predictor = full_tree_test_preds, response = heart_test$output)
roc(predictor = as.numeric(pruned_tree_test_preds), response = heart_test$output)
roc(predictor = as.numeric(early_stopped_tree_test_preds), response = heart_test$output)
```

A bit better, but not by too much. In other datasets you might see a bigger effect. How can we improve more? 

## Random Forests
```{r}
rf_fit <- train(heart_recipe, data = heart_train, 
                method = "rf",
                metric = 'ROC',
                tuneLength = 20,
                trControl = heart_control)

plot(rf_fit)

```
It looks like randomly selecting two predictors at each step leads to the best predictions. We can verify that below:

```{r}
#what was the best tuning parameter ?
rf_fit$bestTune
```


```{r}
rf_test_preds <- predict(rf_fit, newdata = heart_test)
roc(predictor = as.numeric(rf_test_preds), response = heart_test$output)
roc(predictor = full_tree_test_preds, response = heart_test$output)
```

Random forests do much better than the original full tree!

## What does the tree look like?

We don't know... there are many trees, so it's hard to gain insight, since there is not 'one model'. Instead, we can figure out what what the 'important variables' are across the different trees.

```{r}
ggplot(varImp(rf_fit))
```

## Try it for yourself: random forests to predict Alzheimer's

Washington University conducted a clinical study to determine if biological measurements made from cerebrospinal fluid (CSF) can be used to diagnose or predict Alzheimer's disease (Craig-Schapiro et al. 2011). These data are a modified version of the values used for the publication.

The R factor vector diagnosis contains the outcome data for 333 of the subjects. The demographic and laboratory results are collected in the data frame predictors.

```{r}
data(AlzheimerDisease)
predictors$E2 <- predictors$E3 <- predictors$E4 <- 0      ## just some variable preparation. dummycode Genotype
predictors$E2[grepl("2", predictors$Genotype)] <- 1
predictors$E3[grepl("3", predictors$Genotype)] <- 1
predictors$E4[grepl("4", predictors$Genotype)] <- 1
predictors$Genotype <- NULL
head(predictors)
dim(predictors)           ## 333 persons, 130 predictors
table(diagnosis)          ## response

```

Perform data splitting: use 80% in your training set, 20% in your testing set

```{r}

```

# train random forests. 
```{r}

```

# evaluate performance on the test set

```{r}

```

# determine which variables were important in this prediction

```{r}

```

  




