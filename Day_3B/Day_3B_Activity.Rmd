---
title: "Day 3-B Activity <br /> (Decision Trees and Random Forests)"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    df_print: paged
editor_options: 
  chunk_output_type: inline
---

# Live Coding 
Goal: Classify likelihood of having a heart attack based on demographics (age, sex) and health variables (type of chest pain, resting blood pressure, cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate, exercise induced angina, etc.). Compare performance of fully grown trees, pruned trees, trees grown with a stopping parameter, and random forests.


## Load Packages
```{r, message=FALSE}
if (!require("tidyverse")) {install.packages("tidyverse"); require("tidyverse")}     
if (!require("caret")) {install.packages("caret"); require("caret")}
if (!require("tidymodels")) {install.packages("tidymodels"); require("tidymodels")}
if (!require("summarytools")) {install.packages("summarytools"); require("summarytools")}
if (!require("psych")) {install.packages("psych"); require("psych")}
if (!require("coefplot")) {install.packages("coefplot"); require("coefplot")}
if (!require("pROC")) {install.packages("pROC"); require("pROC")}
if (!require("rpart")) {install.packages("rpart"); require("rpart")}
if (!require("randomForest")) {install.packages("randomForest"); require("randomForest")}
if (!require("AppliedPredictiveModeling")) {install.packages("AppliedPredictiveModeling"); require("AppliedPredictiveModeling")} 
```


## Load data
```{r}
heart <- read_csv("https://bit.ly/amlr-heart", col_types = "dffddffdfdiiif")
```

## Split into train/test sets 
```{r}
set.seed(2021)
trainIndex <- createDataPartition(heart$output, p = 0.8, list = FALSE)
heart_train <- heart[trainIndex, ]
heart_test <- heart[-trainIndex, ]

# check to see that data splits were correct
dim(heart_train)
dim(heart_test)
```

## Create Training Recipe
```{r}
heart_recipe <- 
  heart_train %>% 
  recipe(output ~ .) %>%
  step_mutate(output = factor(output, levels = c(0, 1), labels = c("low_risk", "high_risk"))) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE)

# prep and bake training and test data for later (model evaluation)
heart_train_baked <- 
  heart_recipe %>%
  prep(training = heart_train) %>%
  bake(new_data = heart_train)

heart_test_baked <- 
  heart_recipe %>%
  prep(training = heart_train) %>%
  bake(new_data = heart_test)
```

## Set up Resampling Methods
```{r}
# let's use 5-fold cross-validation, repeated 3 times 

# set train control
heart_control <- trainControl(method = 'repeatedcv',
                              number = 5,
                              repeats = 3)
```


## Full Tree Fit 

To illustrate overfitting (if we don't stop trees from growing), we'll fit an un-pruned tree with no stopping criteria, with a minsplit of 2 (minimum 2 observations needed to create a split) and minbucket of 1 (minimum number observations per node). Note that we would never want to do this in practice; this is just for teaching illustration! 

```{r}
cpgrid_full <- expand.grid(cp = 0)
full_tree <- train(heart_recipe, data = heart_train,
                   method = 'rpart',
                   trControl = heart_control,
                   tuneGrid = cpgrid_full,
                   control = rpart.control(minsplit = 2, minbucket = 1)) 

full_tree

plot(full_tree$finalModel)
text(full_tree$finalModel)
```

## Classification Accuracy: Fully Grown Trees

Let's calculate how well it does on the training set, relative to the test set. This gives us a sense of how over-fit the data is on our training set

```{r}
full_tree_train_preds <- predict(full_tree, newdata = heart_train)
full_tree_test_preds <- predict(full_tree, newdata = heart_test)
full_tree_train_prob <- predict(full_tree, newdata = heart_train, type = "prob")
full_tree_test_prob <- predict(full_tree, newdata = heart_test, type = "prob")

# Collect all predicted and trusted labels in a dataframe (for {yardstick})
collected_labels_train <- bind_cols(
  full_tree_train_prob,
  true = heart_train_baked$output, 
  pred = full_tree_train_preds)

collected_labels_test <- bind_cols(
  full_tree_test_prob,
  true = heart_test_baked$output, 
  pred = full_tree_test_preds)  

cm_train <- conf_mat(collected_labels_train, truth = true, estimate = pred)
cm_test <- conf_mat(collected_labels_test, truth = true, estimate = pred)
summary(cm_train)
summary(cm_test)

roc_auc(collected_labels_train, truth = true, high_risk, event_level = "second")
roc_auc(collected_labels_test, truth = true, high_risk, event_level = "second")
```
We are definitely overfitting with perfect prediction on our training set! Our AUC drops on our test set ~ .7. Let's see if we can improve on this by having modifying our approach.


## Reduce Overfitting by Pruning the Tree
```{r}
pruned_tree <- train(heart_recipe, data = heart_train, 
                     method = "rpart",
                     tuneLength = 20,
                     trControl = heart_control)

pruned_tree

plot(pruned_tree)

plot(pruned_tree$finalModel)
text(pruned_tree$finalModel)
```

## Reduce Overfitting with Stopping Criteria

```{r}
early_stopped_tree <- train(heart_recipe, data = heart_train, 
                            method = "rpart2",
                            tuneLength = 20,
                            trControl = heart_control)

plot(early_stopped_tree)

plot(early_stopped_tree$finalModel)
text(early_stopped_tree$finalModel)
```

## Classification Accuracy: Pruning and Stopping Criteria

```{r}
pruned_tree_preds <- predict(pruned_tree, newdata = heart_test)
stopped_tree_preds <- predict(early_stopped_tree, newdata = heart_test)
pruned_tree_prob <- predict(pruned_tree, newdata = heart_test, type = "prob")
stopped_tree_prob <- predict(early_stopped_tree, newdata = heart_test, type = "prob")

# Collect all predicted and trusted labels in a dataframe (for {yardstick})
collected_labels_pruned <- bind_cols(
  pruned_tree_prob,
  true = heart_test_baked$output, 
  pred = pruned_tree_preds)

collected_labels_stopped <- bind_cols(
  stopped_tree_prob,
  true = heart_test_baked$output, 
  pred = stopped_tree_preds)  

cm_prune <- conf_mat(collected_labels_pruned, truth = true, estimate = pred)
cm_stop <- conf_mat(collected_labels_stopped, truth = true, estimate = pred)
summary(cm_prune)
summary(cm_stop)

roc_auc(collected_labels_pruned, truth = true, high_risk, event_level = "second")
roc_auc(collected_labels_stopped, truth = true, high_risk, event_level = "second")
```

A bit better, but not by too much. In other datasets you might see a bigger effect. How can we improve more? 

## Random Forests
```{r}
rf_fit <- train(heart_recipe, data = heart_train, 
                method = "rf",
                tuneLength = 20,
                trControl = heart_control)

plot(rf_fit)

```

## Random Forests Accuracy

```{r}
rf_preds <- predict(rf_fit, newdata = heart_test)
rf_prob <- predict(rf_fit, newdata = heart_test, type = "prob")

# Collect all predicted and trusted labels in a dataframe (for {yardstick})
collected_labels_rf <- bind_cols(
  rf_prob,
  true = heart_test_baked$output, 
  pred = rf_preds)

cm_rf <- conf_mat(collected_labels_rf, truth = true, estimate = pred)
summary(cm_rf)
roc_auc(collected_labels_rf, truth = true, high_risk, event_level = "second")
```

Random forests do much better than the original full tree!

## What does the tree look like?

We don't know... there are many trees, so it's hard to gain insight, since there is not 'one model'. Instead, we can figure out what what the 'important variables' are across the different trees.

```{r}
ggplot(varImp(rf_fit))
```

## Hands-On Activity

Washington University conducted a clinical study to determine if biological measurements made from cerebrospinal fluid (CSF) can be used to diagnose or predict Alzheimer's disease (Craig-Schapiro et al. 2011). These data include measurements from 333 participants (a modified version of the values used for the publication).

The first 128 columns in the dataframe represent numerical biological measurements (to be used as features). The next two columns `ahd[, 129:130]` are nominal features (male sex {yes, no}; genotype {E2E2, E2E3, E2E4, E3E3, E3E4, E4E4}). The last column `ahd[, 131]` is the categorical label representing presence/absense of Alzheimer's Disease (diagnosis {control, impaired}).

1. Split your data into 80% training and 20% test
2. Create your training recipe, using one-hot encoding for categorical variables. 
3. Fit a random forest model with 5-fold cross-validation. Use a tuning grid with 10 values.
4. Estimate Accuracy and Kappa in the training and testing sets
5. Examine and plot the modelâ€™s variable importance.

**BONUS 1**: Construct a confusion matrix and ROC curve for the model.

**BONUS 2**: Fit a pruned decision tree on the data with 5-fold cross-validation. Compare classification metrics (accuracy, kappa, AUROC) between the random forests and pruned tree models in the testing sets. Which performs better, and why?

```{r}
ahd <- read.csv("https://bit.ly/amlr-ahd", stringsAsFactors = TRUE)
```

# Answer key

<details><summary>Click here to view the answer key to the hands-on activity</summary>

## 1. Split your data into 80% training and 20% test

```{r}
set.seed(2021)
trainIndex <- createDataPartition(ahd$diagnosis, p = 0.8, list = FALSE)
ahd_train <- ahd[trainIndex, ]
ahd_test <- ahd[-trainIndex, ]

# check to see that data splits were correct
dim(ahd_train)
dim(ahd_test)
```

## Create your training recipe, using one-hot encoding for categorical variables.

```{r}
ahd_recipe <- 
  ahd_train %>% 
  recipe(diagnosis ~ .) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE)

ahd_test_baked <- 
  ahd_recipe %>%
  prep(training = ahd_train) %>%
  bake(new_data = ahd_test)
```

## Fit a random forest model with 10-fold cross-validation. Use a tuning grid with 10 values.

```{r}
ahd_control <- trainControl(method = 'cv',
                            number = 5)

rf_fit_ahd <- train(ahd_recipe, data = ahd_train, 
                    method = "rf",
                    tuneLength = 10,
                    trControl = ahd_control)

plot(rf_fit_ahd)
```

## Estimate Accuracy and Kappa in the training and testing sets

```{r}
rf_fit_ahd$results %>% print()

ahd_pred <- predict(rf_fit_ahd, newdata = ahd_test)
postResample(pred = ahd_pred, obs = ahd_test_baked$diagnosis)
```

## Examine and plot the modelâ€™s variable importance.

```{r}
ggplot(varImp(rf_fit_ahd), top = 20)
```

## **BONUS 1**: Construct a confusion matrix and ROC curve for the model.

```{r}
rf_ahd_preds <- predict(rf_fit_ahd, newdata = ahd_test)
rf_ahd_prob <- predict(rf_fit_ahd, newdata = ahd_test, type = "prob")

# Collect all predicted and trusted labels in a dataframe (for {yardstick})
collected_labels_rf_ahd <- bind_cols(
  rf_ahd_prob,
  true = ahd_test$diagnosis, 
  pred = rf_ahd_preds)

cm_rf_ahd <- conf_mat(collected_labels_rf_ahd, truth = true, estimate = pred)
cm_rf_ahd
roc_curve(collected_labels_rf_ahd, truth = true, Impaired, event_level = "second") %>% 
  autoplot()
```

## **BONUS 2**: Fit a pruned decision tree on the data with 5-fold cross-validation. Compare classification metrics (accuracy, kappa, AUROC) between the random forests and pruned tree models in the training and testing sets. Which performs better, and why?

```{r}
pruned_fit_ahd <- train(ahd_recipe, data = ahd_train, 
                        method = "rpart",
                        tuneLength = 10,
                        trControl = ahd_control)

plot(pruned_fit_ahd)
```

```{r}
# pruned tree training accuracy
pruned_ahd_preds <- predict(pruned_fit_ahd, newdata = ahd_test)
pruned_ahd_prob <- predict(pruned_fit_ahd, newdata = ahd_test, type = "prob")

collected_labels_pruned_ahd <- bind_cols(
  pruned_ahd_prob,
  true = ahd_test$diagnosis, 
  pred = pruned_ahd_preds)
cm_pruned_ahd <- conf_mat(collected_labels_pruned_ahd, truth = true, estimate = pred)
summary(cm_pruned_ahd)
roc_auc(collected_labels_pruned_ahd, truth = true, Impaired, event_level = "second")

## rf testing accuracy
summary(cm_rf_ahd) # random forest gives higher accuracy
roc_auc(collected_labels_rf_ahd, truth = true, Impaired, event_level = "second") # random forest also gives higher AUC
```


</details>



