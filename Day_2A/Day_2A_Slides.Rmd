---
title: '<span style="font-size:48pt;">Feature Engineering</span>'
subtitle: 'üë®‚ÄçüîßÔ∏è üìä üõ†Ô∏è'
author: 'Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series'
date: 'Lecture 2-A &emsp; &emsp; July 20, 2021'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
    self_contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  comment = "#>",
  collapse = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  footnote_font_size = "20px",
  footnote_color = "gray",
  text_slide_number_font_size = "18px"
)
```

```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_tachyons()
xaringanExtra::use_clipboard()
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(caret)
library(recipes)
```

class: inverse, center, middle
# Overview

<style type="text/css">
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
.remark-code {
  font-size: 24px;
  border: 1px solid grey;
}
a {
  background-color: lightblue;
}
.remark-inline-code {
  background-color: white;
}
</style>
---
class: onecol
## Feature Engineering
.left-column[
<br />
```{r engineer, echo=FALSE}
include_graphics("../figs/engineer.jpg")
```
]
.right-column[
**Prepare the predictors for analysis**
- *Extract* predictors
- *Transform* predictors
- *Re-encode* predictors
- *Combine* predictors
- *Reduce* predictor dimensionality
- *Impute* missing predictor values
- *Select* and drop predictors
]
---
class: onecol

## Motivation

**Features** are descriptions of the data points that help to predict the outcomes

- We may need to extract features from "raw" or "low-level" data (e.g., images)
- We may need to address issues with missing data and feature distributions

--

There are many potential ways to **encode** or "represent" the features/predictors

- e.g., adding, dropping, transforming, and combining predictors<sup>1</sup>
- predictor encoding can have a big ![:emphasize](impact on predictive performance)<sup>2</sup>
- The optimal encoding depends on both the **algorithm** and the **relationships**

.footnote[
[1] Some algorithms can learn their own, complex feature representations<br />
[2] Some algorithms are more sensitive to feature encoding than others
]

---
class: onecol
## Examples of Feature Encodings
.left-column[
<br />

```{r, echo=FALSE}
include_graphics("../figs/july.jpg")
```

]
.right-column[
**When an event or observation occurred**

- The numeric year *(2021)*
- The numeric month *(7)*
- The numeric day of the month *(20)*
- The numeric day of the year *(201)*
- Days since a reference *(diagnosis +2)*
- The day of the week *(Tuesday)*
- The season of the year *(Summer)*
- The type of day *(weekday)* 
- The presence of a holiday *(FALSE)*

]
---
class: onecol
## Issues to Navigate in Feature Engineering

.left-column[
<br />
```{r, echo=FALSE}
include_graphics("../figs/cones.jpg")
```
]
.right-column[
- predictors with **non-normal** distributions
- predictors with vastly **different scales**
- predictors with extreme **outliers**
- predictors with **missing** or censored values
- predictors that are **correlated** with one another
- predictors that are **redundant** with one another
- predictors that have zero or **near-zero variance**
- predictors that are **uninformative** for a task
- predictors with **uncertainty** or unreliability
]

---

class: onecol
## Recipes for Feature Engineering

.left-column[
<br />
```{r, echo=FALSE}
include_graphics("../figs/chef.jpg")
```

]

.right-column[
All of the predictor engineering steps can be done "by hand" in R

The {caret} package provides some basic convenience tools

We will be learning the {recipes} package from {tidymodels}

1. Initiate a recipe by declaring data and roles using `recipe()`
1. Add one or more preprocessing steps using `step_*()`
1. Prepare/estimate the preprocessing steps using `prep()`
1. Apply these steps to the training and testing data with `bake()`
]

---
## Example Dataset: `titanic`

.pull-left[
**Rows:** 963 passengers&emsp;&emsp;**Columns:** 7 variables

Variable | Description
:------- | :----------
survived&emsp; | Did passenger survive? {FALSE, TRUE}&emsp; 
pclass | Passenger class {1st, 2nd, 3rd}
sex | Passenger sex {female, male}
age | Passenger age (years)
sibsp | Siblings and spouses Aboard (\#)
parch | Parents and children Aboard (\#)
fare | Cost of passenger fare ($)
]

--

.pull-right[
```{r, message=FALSE}
library(tidyverse)
titanic <- read_csv(
  "https://osf.io/juez2/download"
)

library(caret)
index <- createDataPartition(
  y = titanic$survived, 
  p = 0.8, 
  list = FALSE
)
titanic_train <- titanic[index, ]
titanic_test <- titanic[-index, ]
```
]

---
## Recipes for Feature Engineering

```{r}
# There are three equivalent ways to specify variables and roles
library(recipes)

titanic_recipe <- recipe(
  titanic,
  vars = c("survived", "pclass", "sex", "age", "sibsp", "parch", "fare"),
  roles = c("outcome", "predictor", "predictor", "predictor",
            "predictor", "predictor", "predictor")
)

titanic_recipe <- 
  recipe(titanic) %>% 
  update_role(survived, new_role = "outcome")

titanic_recipe <- recipe(titanic, formula = survived ~ .)
```

---
## Recipes for Feature Engineering

```{r, eval=FALSE}
# Use summary() on a recipe to view a table of variables
titanic_recipe %>% summary()
```

```{r,echo=FALSE}
titanic_recipe %>% summary() %>% kable()
```

--

<p style="padding-top:25px;">Now we are ready to add some preprocessing (i.e., feature engineering) steps to the recipe!</p>

---
class: onecol
## Common Steps (Lecture Roadmap)

.pull-left[
- **Adding predictors**
  - Calculated predictors
  - Categorical predictors
  - Interaction Terms
- **Transforming predictors**
  - Centering and Scaling
  - Addressing Non-normality
  - Adding Non-linearity
]
.pull-right[

- **Reducing predictors**
  - Nero-Zero Variance
  - Multicollinearity
  - Dimensionality Reduction
- **Advanced Steps**
  - Feature Extraction
  - Dealing with Missing Values
  - Feature Selection
]

---
class: inverse, center, middle
# Adding predictors
---
class: onecol
## Calculated predictors

Some variables will need to be calculated from existing values and variables

- You may choose to score an instrument from item-level data
- You may choose to encode a predictor as the ratio of two values
- You may choose to calculate sums, means, counts, proportions, etc.

--

<p style="padding-top:25px;">We will show you some basic steps for calculating variables within {recipes}</p>

For more advanced/complex data wrangling, we recommend you read

- <i>[R for Data Science: Visualize, Model, Transform, Tidy, and Import Data](https://r4ds.had.co.nz/)</i><br />by Wickham and Grolemund (book for purchase or online for free)

---
## Calculated predictors

```{r}
# Add a step to calculate new predictors from existing predictors
cp_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_mutate( #<<
    numfamily = sibsp + parch, #<<
    over70 = age > 70 #<<
  ) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

---
## Calculated predictors

```{r, eval=FALSE}
cp_recipe %>% summary()
```

```{r, echo=FALSE}
cp_recipe %>% summary() %>% kable()
```

---
## Calculated predictors

```{r}
# bake() will allow us to generate updated training and testing sets
cp_train <- bake(cp_recipe, new_data = titanic_train)
cp_test <- bake(cp_recipe, new_data = titanic_test)
```


```{r, eval=FALSE}
cp_test
```

```{r, echo=FALSE}
cp_test %>% kable() %>% scroll_box(height = "250px")
```


---
class: onecol
## Categorical predictors

Categorical predictors can be re-encoded into multiple binary (0 or 1) predictors

In `titanic`, the categorical variable `sex` takes on the value *male* or *female*

--

.pull-left[
.center[![:emphasize](One-Hot Encoding)]

sex | sex_female | sex_male
:---|:----------:|:-------:
female | 1  | 0 
male   | 0  | 1

<p style="text-align:center;font-size:20px;">Simple and easy to interpret<br />Good for tree-based methods</p>
]

--

.pull-right[
.center[![:emphasize](Dummy Coding)]

sex | sex_male
:---|:-------:
female | 0  
male   | 1 

<p style="text-align:center;font-size:20px;">Efficient and avoids redundancy<br />Good for GLM-based methods</p>
]

---
## Categorical predictors in R

```{r}
# Add a step to add one hot encoding
oh_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_dummy(pclass, sex, one_hot = TRUE) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

.footnote[[1] As a shortcut, we could also have used `all_nominal_predictors()` instead of `pclass, sex`.]

---
## Categorical predictors in R

```{r}
# Add a step to the recipe to create dummy codes for pclass and sex
dc_recipe <- 
  titanic %>%
  recipe(survived ~ .) %>% 
  step_dummy(pclass, sex, one_hot = FALSE) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

.footnote[[1] As another shortcut, I could leave off `one_hot = FALSE` since that is the default option.]

---
class: onecol
## Interaction Terms

Interaction terms allow the meaning of one predictor to depend on other predictors

In this way, interaction terms allow predictor "effects" to be **contingent** or **conditional**

e.g., perhaps having parents or children on board the Titanic helps you predict survival... but the effects differs depending on whether the passenger is a man or a woman

--

<p style="padding-top:25px;">Interaction terms are literally <b>products</b> (i.e., multiplications) of two or more predictors</p>

In order to include categorical variables in interaction terms, dummy code them first

---
## Interaction Terms in R

```{r}
# Add interaction terms using formula notation
it_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_dummy(pclass, sex) %>% 
  step_interact(~ age:parch + sibsp:starts_with("pclass_")) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

.footnote[[1] The selector function `starts\_with()` allows you to easily capture all dummy codes for a variable.]

---
## Interaction Terms in R

```{r,}
# Bake the recipe and preview the updated training set
it_train_baked <- bake(it_recipe, new_data = titanic_train)
```

```{r, echo=FALSE}
it_train_baked %>% kable() %>% scroll_box(height = "300px")
```

---
## Comprehension Check \#1
.pull-left[
### Question 1
**What is the correct order in which to add the {recipe} functions to a pipeline?**

a) recipe > prep > step(s) > bake

b) recipe > step(s) > prep > bake

c) prep > step > bake > recipe

d) prep > recipe > step > bake
]

.pull-right[
### Question 2
**How many dummy codes are needed to encode a variable with five (5) categorical levels?**

a) Six (6)

b) Five (5)

c) Four (4)

d) One (1)
]

---
class: inverse, center, middle
# Transforming predictors
---
class: onecol
## Normalizing

Predictors with vastly different means and SDs can cause problems for some algorithms

--

![:emphasize](Centering) a predictor involves changing its mean to $0.0$

- This is accomplished by subtracting the mean from every observation 

--

![:emphasize](Scaling) a predictor involves changing its standard deviation (and variance) to $1.0$

- This is accomplished by dividing each observation by the standard deviation

--

![:emphasize](Normalizing) a predictor involves centering it and then scaling it

- This is also sometimes called "standardizing" or $z$-scoring the predictor

---
## Centering Visualized

```{r centering, echo=FALSE}
x <- titanic$age
p1 <- ggplot(tibble(x), aes(x)) + geom_density(na.rm = TRUE) +
  annotate(
    "label", 
    x = mean(x, na.rm = TRUE), 
    y = 0.005, 
    label = glue::glue("M={round(mean(x, na.rm = TRUE), 1)}, SD={round(sd(x, na.rm = TRUE), 1)}"),
    size = 5
    ) +
  labs(x = "age", y = "density") +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
x_c <- x - mean(x, na.rm = TRUE)
p2 <- ggplot(tibble(x_c), aes(x_c)) + geom_density(na.rm = TRUE) + 
  annotate(
    "label", 
    x = mean(x_c, na.rm = TRUE), 
    y = 0.005, 
    label = glue::glue("M={round(mean(x_c, na.rm = TRUE), 1)}, SD={round(sd(x_c, na.rm = TRUE), 1)}"),
    size = 5
    ) +
  labs(x = "age_centered", y = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p1 | p2
```

.footnote[The mean is now 0 but the shape and SD of the distribution are unchanged (i.e., it has been shifted left).]

---
## Scaling Visualized

```{r scaling, echo=FALSE}
x_s <- x / sd(x, na.rm = TRUE)
p3 <- ggplot(tibble(x_s), aes(x_s)) + geom_density(na.rm = TRUE) + 
  annotate(
    "label", 
    x = mean(x_s, na.rm = TRUE), 
    y = 0.075, 
    label = glue::glue("M={round(mean(x_s, na.rm = TRUE), 1)}, SD={round(sd(x_s, na.rm = TRUE), 1)}"),
    size = 5
    ) +
  labs(x = "age_scaled", y = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p1 | p3
```

.footnote[The SD is now 1 and the mean is lower, but the shape of the distribution is unchanged.]

---
## Normalizing Visualized

```{r normalizing, echo=FALSE}
x_n <- (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
p4 <- ggplot(tibble(x_n), aes(x_n)) + geom_density(na.rm = TRUE) + 
  annotate(
    "label", 
    x = mean(x_n, na.rm = TRUE), 
    y = 0.075, 
    label = glue::glue("M={round(mean(x_n, na.rm = TRUE), 1)}, SD={round(sd(x_n, na.rm = TRUE), 1)}"),
    size = 5
    ) +
  labs(x = "age_normalized", y = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p1 | p4
```

.footnote[The mean is now 0 and the SD is now 1, but the shape of the distribution is unchanged.]

---
## Normalizing in R

```{r}
# Normalize the age variable using the training set mean and SD
norm_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_normalize(age) %>% #<<  
  prep(training = titanic_train, log_changes = TRUE)
```

--

```{r}
# Because of prep(), bake() uses the training set mean and SD¬≤
norm_test <- bake(norm_recipe, new_data = titanic_test)
```


.footnote[
[1] We could also have used `step_center()` and/or `step_scale()` instead of `step_normalize()`.<br />
[2] This is important to accurately estimating out-of-sample performance on truly novel data.
]

---
class: onecol
## Addressing Non-normality

A ![:emphasize](skewed) distribution is one that is not symmetric (i.e., it has a "heavy tail")

A ![:emphasize](bounded) distribution is one that cannot go beyond certain boundary values

```{r skew, echo=FALSE, out.width='90%'}
n <- 1e5
skew_ex <- tibble(
  x = c(
    rbeta(n, 1, 8), 
    rbeta(n, 9, 9),
    rbeta(n, 8, 1)
  ),
  type = factor(
    rep(c("Positively Skewed", "Symmetrical", "Negatively Skewed"), each = n),
    levels = c("Positively Skewed", "Symmetrical", "Negatively Skewed")
  )
)

ggplot(skew_ex, aes(x = x, fill = type, linetype = type)) + 
  geom_density(size = 1.25, alpha = 0.4, color = "grey40") + 
  scale_fill_brewer(palette = "BrBG") +
  scale_linetype_manual(values = c("solid", "dotted", "solid"), 
                        guide = "legend") +
  labs(x = NULL, fill = NULL, linetype = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    legend.position = "bottom",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Addressing Non-normality
Specific transformations (e.g., log, inverse, logit) can help address specific issues

The Box-Cox and Yeo-Johnson approaches employ **families of transformations**

Box-Cox cannot be applied to negative or zero values, but ![:emphasize](Yeo-Johnson) can

<br />

$$x_{(yj)}^\star=\begin{cases}((x+1)^\lambda-1)/\lambda & \text{if } \lambda\ne0, x\ge0 \\
\log(x+1) & \text{if } \lambda=0, x\ge0 \\
-[(-x+1)^{2-\lambda}-1)]/(2-\lambda) & \text{if } \lambda\ne2, x<0 \\
-\log(-x+1) & \text{if } \lambda=2, x<0
\end{cases}$$

---
## Addressing Non-normality in R

```{r}
# Add step to apply the Yeo-Johnson transformation to fare
yj_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_YeoJohnson(fare) %>% #<<  
  prep(training = titanic_train, log_changes = TRUE)
```

.footnote[[1] If you would like to use specific transformations, use: `step_log()`, `step_inverse()`, `step_sqrt()`, etc.<br />[2] As with normalizing, use `prep()` to estimate `\\(\lambda\\)` from training set and use it when you `bake()` the test set.]

---
class: onecol
## Addressing Non-normality in R

Bake the recipe using the training data and then plot the transformed variable

```{r yjfare, echo = FALSE}
yj_train <- bake(yj_recipe, new_data = titanic_train)
p1 <- 
  ggplot(titanic_train, aes(x = fare)) + 
  geom_density(size = 1.25, alpha = 0.4, color = "grey40", 
               fill = "#91bfdb", na.rm = TRUE) + 
  labs(x = "Original fare", y = "density") +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p2 <- 
  ggplot(yj_train, aes(x = fare)) + 
  geom_density(size = 1.25, alpha = 0.4, color = "grey40", 
               fill = "#91bfdb", na.rm = TRUE) + 
  labs(x = "Transformed fare (Yeo-Johnson)", y = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p1 | p2
```

---
class: onecol
## Adding Nonlinearity

Many relationships between features and labels are non-linear in nature
- *e.g., perhaps survival was lowest for young adults and higher for children and elders*

<p style="padding-top:25px;">Successful prediction will require us to <b>model that nonlinearity</b> in such cases</p>

--

<p style="padding-top:25px;">Many algorithms can capture nonlinearity easily but others need our help</p>

- For these algorithms, we can provide help through feature engineering

- This typically means adding ![:emphasize](nonlinear expansions) of existing predictors<sup>1</sup>

.footnote[[1] If you are familiar with polynomial (e.g., quadratic or cubic) regression, you already have relevant experience!]

---
## Adding Nonlinearity in R

```{r}
# Add step to add orthogonal polynomial basis functions
nl_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_poly(age, degree = 2) %>% #<<  
  prep(training = titanic_train, log_changes = TRUE)
```

.footnote[[1] Note that, by specifying `degree = 2`, we are creating a quadratic expansion; more flexibility can be added.<br />[2] Additional nonlinear expansions are also available: `step_ns()`, `step_bs()`, and `step_hyperbolic()`.]

---
class: onecol
## Adding Nonlinearity in R

Bake the recipe and plot the polynomial terms against one another (with vertical jitter).

```{r poly, echo=FALSE}
nl_train <- bake(nl_recipe, new_data = titanic_train)
ggplot(nl_train, aes(x = age_poly_1, y = age_poly_2)) + 
  geom_jitter(size = 3, alpha = 0.25, width = 0, height = 0.01) +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---

## Comprehension check \#2
.pull-left[
### Question 1
**I want to transform two predictors to have the same variance. Which would NOT achieve this?**

a) Centering both

b) Scaling both

c) Normalizing both

d) Dividing each by its SD
]

.pull-right[
### Question 2
**Which of the following issues would the Yeo-Johnson transformation NOT help with?**

a) Positive skew

b) Negative skew

c) Outlier values

d) Categorical data
]

---
class: inverse, center, middle
# Reducing predictors
---
class: onecol
## Zero and Near-Zero Variance Predictors
![:emphasize](Zero variance predictors) take on only a single value in the sample

- These predictors are **uninformative** and may lead to **modeling problems**

--

![:emphasize](Near-zero variance predictors) take on only a few unique values with low frequencies

- These predictors can easily become zero-variance predictors during resampling

--

<p style="padding-top:25px;">For many algorithms, we want to <b>detect</b> and <b>remove</b> both types of predictors</p>

(This may not be necessary for algorithms with built-in *predictor selection*)

---
## Zero and Nero-Zero Variance Predictors in R

```{r}
# Detect and remove zero-variance predictors
zv_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_mutate(
    species = "homo sapiens", # will have zero variance
    over70 = age > 70 # will have near-zero variance
  ) %>% 
  step_zv(all_predictors()) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

---
## Zero and Nero-Zero Variance Predictors in R

```{r}
# Detect and remove near-zero-variance predictors
nzv_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_mutate(
    species = "homo sapiens",  # will have zero variance
    over70 = age > 70 # will have near-zero variance
  ) %>% 
  step_nzv(all_predictors()) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

---
class: onecol
## Multicollinearity

![:emphasize](Highly correlated predictors) can lead to problems for some algorithms/procedures

- The model has to randomly choose between the predictors, leading to **instability**
- Model predictions may be fine, but model **interpretation** will often be obfuscated
- The cutoff for "problematically high" correlations varies (e.g., 0.5 to 0.9 or higher)

--

Predictors that are ![:emphasize](linear combinations) of other predictors are similarly problematic

- Occurs if a predictor variable can be predicted from the other predictor variables
- This is why dummy coding is preferred to one-hot encoding for some algorithms

--

<p style="padding-top:25px;">For many algorithms, we want to <b>detect</b> and <b>remove</b> redundant predictors</p>

(This may not be necessary for algorithms with *regularization* or *predictor selection*)

---
class: onecol
## Multicollinearity in R

```{r}
# Add some predictors with high correlations and linear dependency
mc_titanic <- titanic %>% mutate(
  wisdom = 100 + 0.25 * age + rnorm(nrow(.)), # high corr
  numfamily = sibsp + parch # linear combination
)
mc_train <- mc_titanic[index, ]
```

---
class: onecol
## Multicollinearity in R

```{r, eval=FALSE}
library(correlation)
correlation(mc_train) %>% filter(abs(r) > 0.75)
```

```{r, echo=FALSE}
library(correlation)
correlation(mc_train) %>% filter(abs(r) > 0.75) %>% print_md()
```

---
class: onecol
## Multicollinearity in R

```{r}
# Detect and remove predictors that are highly correlated
hc_recipe <- 
  mc_titanic %>% 
  recipe(survived ~ .) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9) %>% #<<
  prep(training = mc_train, log_changes = TRUE)
```

--

.footnote[[1] If we want to consider correlations with categorical variables, we can add `step_dummy()` to the pipeline.<br />[2] We could have also lowered the threshold to 0.8 in order to drop the `family` variable here.]
---
class: onecol
## Multicollinearity in R

```{r}
# Detect and remove predictors that are linear combinations
lc_recipe <- 
  mc_titanic %>% 
  recipe(survived ~ .) %>% 
  step_lincomb(all_numeric_predictors()) %>%  #<<
  prep(training = mc_train, log_changes = TRUE)
```

---
class: onecol
## Dimensionality Reduction

Each feature/predictor included can be considered an additional "dimension"

![:emphasize](Dimensionality reduction) techniques try to find a smaller set of predictors to use

- If successful, little information from the original set of predictors will be lost
- Most techniques create new predictors as *functions of the original predictors*

--

**Principal Components Analysis** (PCA) is a commonly used technique

- The new predictors (PCs) are *linear combinations* of the original predictors
- The PCs are *uncorrelated* with one another, thus addressing multicollinearity
- PCs are extracted until a target amount of variability is explained (e.g., 75%)

.footnote[[1] Predictors should be normalized (i.e., centered and scaled) before PCA is used.<br />[2] PCA is linear and unsupervised but there are nonlinear and supervised techniques.]

---
## Dimensionality Reduction in R

```{r}
# Normalize numeric predictors and then do PCA
pca_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_normalize(all_numeric_predictors()) %>% #<<
  step_pca(all_numeric_predictors(), threshold = 0.75) %>%  #<<
  prep(training = titanic_train, log_changes = TRUE)
```

.footnote[
[1] Note that PCA is most effective when there are many correlated predictors, so this is a weak use of it.]

---
class: inverse, center, middle
# Advanced Topics
---
class: onecol
## Feature Extraction
Feature extraction involves generating features from "raw" data

--

For raw **text** data, natural language processing techniques can be used
- *e.g., sentiment, word frequencies, word relationships, topic modeling, syntax*

--

For raw **image** and video data, computer vision techniques can be used
- *e.g., edges, corners, blobs, ridges, objects, curvature, shape, motion, color*

--

For raw **audio** data, acoustic signal processing techniques can be used
- *e.g., rhythm, stress, intonation, pitch, loudness, glottal flow, spectral density*

--

.footnote[[1] One of the strengths of deep learning is its ability to learn its own feature representations from raw data.]

---
class: onecol
## Dealing with Missing Values

There are several approaches to handling missing values in predictors

- Some algorithms (e.g., tree-based techniques) handle missing data inherently
- Another option is to drop predictors with any (or a lot of) missing values
- Or we can ![:emphasize](impute) or estimate the missing values based on the other predictors

--

There are many techniques for imputing missing predictor values
- Some are very simple (e.g., using the mean or median) and others more complex
- We can also use a linear model or even machine learning to impute missing values
- {recipes} provides functions: `step_impute_mean()`, `step_impute_knn()`, etc.

.footnote[
[1] Missing data tends to be more problematic for inferential modeling than predictive modeling.<br />
[2] When imputing, it is a good idea to use cross-validation to capture the uncertainty in the imputations.]

---
class: onecol
## Feature Selection

Feature selection is focused on removing uninformative or redundant predictors
- Models with fewer predictors may be more interpretable, accurate, and efficient

--

![:emphasize](Wrapper methods) compare models with different combinations of predictors
- These are algorithms that search for combinations that optimize performance
- These methods tend to perform well but can be computationally expensive

--

![:emphasize](Filter methods) evaluate predictors outside of the context of the predictive model<sup>1</sup>
- Only predictors that seem informative, relevant, or unique will be retained
- These methods don't perform as well but are computationally efficient

.footnote[[1] Note that we have already learned some basic filter methods (e.g., `step_corr()` and `step_nzv()`).]

---
class: inverse, center, middle
# Live Coding Activity
---
class: onecol
# Live Coding Activity
I will show you a new dataset and the process of feature engineering in RStudio

- If you have one small screen, I recommend you just watch the process

- With a large screen or multiple screens, you can follow along in RStudio

.footnote[[1] All files can be accessed from the workshop website: https://osf.io/3qhc8/]

<p style="padding-top:25px;">Afterward, there will be a hands-on activity where you will modify my code</p>

If you have questions, please post them in chat or in the workshop Slack channel

---
class: onecol
# Hands-on Activity

Modify the [Live Coding example code](https://osf.io/2f5ad/) to accomplish the following goals:

 1. Use 75% of the data for training and 25% of the data for testing.

 1. Apply the Yeo-Johnson transformation to `flight_distance` (before normalizing)

 1. Instead of using PCA to address multicollinearity, drop highly correlated predictors.

 1. Use one-hot encoding for the nominal predictors instead of dummy codes.

 1. Add an interaction term that conditions `seat_comfort` on `flight_distance`

**BONUS:** Read the [Recommended preprocessing](https://www.tmwr.org/pre-proc-table.html)  appendix from the TMWR book

**FURTHER READING:** [TMWR Recipes Chapter](https://www.tmwr.org/recipes.html),  [Feature Engineering and Selection Book](https://bookdown.org/max/FES/)

---

class: inverse, center, middle
# Time for a Break!

```{r countdown, echo=FALSE}
countdown(
  minutes = 10, 
  seconds = 0, 
  right = "33%", 
  left = "33%",
  bottom = "15%",
  color_background = "white",
  color_text = "black",
  color_running_background = "white",
  color_running_text = "black",
  warn_when = 60
)
```
