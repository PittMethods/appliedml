---
title: '<span style="font-size:48pt;">Predictive Modeling Basics</span>'
subtitle: 'üßÆ üîÆ üë®‚Äçüíª' 
author: 'Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series'
date: 'Lecture 2-B &emsp; &emsp; July 20, 2021'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
    self_contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  comment = "#>",
  collapse = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  footnote_font_size = "20px",
  footnote_color = "gray",
  text_slide_number_font_size = "18px"
)
```

```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_tachyons()
xaringanExtra::use_clipboard()
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(caret)
library(recipes)
```

class: inverse, center, middle
# Overview

<style type="text/css">
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
.remark-code {
  font-size: 24px;
  border: 1px solid grey;
}
a {
  background-color: lightblue;
}
.remark-inline-code {
  background-color: white;
}
</style>
---
class: onecol
## Plan for Day 2-B
First will be a lecture on **Performance Metrics** (one of my favorite topics)

.pt1[
Second will be a **lecture** on training, evaluation, and interpretation in R
]

- We will **adapt familiar (statistical) algorithms** to predictive modeling

- *This will ease the transition to ML and highlight its similarities with classical statistics*

- We will also **foreshadow future topics** (e.g., regularized linear models and tuning)

.pt1[
Finally, we will have a Live Coding Activity and related Hands-on Activity
]

---
class: onecol
## Performance Metrics

.left-column.pt3[
```{r target, echo=FALSE}
include_graphics("../figs/target.jpg")
```
]

.right-column[

**Metrics for Supervised Regression**
- ![:emphasize](Distance) between predicted and trusted values
- ![:emphasize](Correlation) between predicted and trusted values

.pt1[
**Metrics for Supervised Classification**
]
- ![:emphasize](Confusion matrix) between predicted and trusted classes
- Compare predicted ![:emphasize](class probabilities) to trusted classes
]

---
class: inverse, center, middle
# Metrics for Regression
---
class: onecol
## Classic Distance Metrics for Regression

.pull-left[
**Root Mean Squared Error (RMSE)**
- Based on squared loss
- Penalizes severe errors harsher, Sensitive to outliers
- Ranges from $0$ to $+\infty$, lower is better
]

.pull-right.pt4[
$$RMSE=\sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2}$$
]

.footnote[[1] Note that, here, we will refer to the trusted labels as `\\(y\\)` and the predicted labels as `\\(p\\)`.]

--

.pull-left[
**Mean Absolute Error (MAE)**
- Based on absolute loss
- Penalizes error consistently, Robust to outliers
- Ranges from $0$ to $+\infty$, lower is better
]

.pull-right.pt4[
$$MAE=\frac{1}{n} \sum_{i=1}^n \left\lvert y_i - p_i \right\rvert$$
]

---
exclude: true
class: onecol
## Advanced Distance Metrics for Regression
**Huber loss (HL)**

- Combines benefits of RMSE and MAE
 + Easy to computationally optimize AND more robust to outliers
- Requires setting or tuning $\delta$ (controls what to consider an outlier)
- Ranges from $0$ (best) to $+\infty$ (worst), lower is better

$$\begin{split}
HL &= \frac{1}{n} \sum_{i=1}^n L_\delta(y_i, p_i) \\
L_\delta(y_i, p_i) &= \begin{cases}\frac{1}{2}(y_i - p_i)^2 & \text{for } \lvert y_i - p_i \rvert \le \delta \\ \delta (\lvert y_i - p_i \rvert - \frac{1}{2}\delta) & \text{otherwise} \end{cases}
\end{split}$$

---
## Visualizing Regression Loss Functions

```{r losses, echo=FALSE}
tibble(
  error = seq(-2, 2, length.out = 100), 
  sq_err = error^2,
  abs_err = abs(error)
) %>% 
  pivot_longer(-error, names_to = "type", values_to = "loss") %>% 
  mutate(type = factor(
    type, 
    levels = c("sq_err", "abs_err"), 
    labels = c("Squared Loss (RMSE)", "Absolute Loss (MAE)")
  )) %>% 
  ggplot(aes(x = error, y = loss, color = type)) + 
  facet_wrap(~type, nrow = 1) + 
  geom_line(size = 1.5) + 
  labs(x = latex2exp::TeX("Error $(y_i - p_i)$"), y = "Loss") +
  theme_xaringan(text_font_size = 18, title_font_size = 18) +
  theme(
    axis.title = element_text(size = 20),
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
exclude: true
## Comparing Loss Functions

```{r losses3, echo=FALSE}
tibble(
  error = seq(-2, 2, length.out = 100), 
  sq_err = error^2,
  abs_err = abs(error),
  huber1 = ifelse(abs(error) <= 1, 0.5 * error^2, 1 * (abs(error) - 0.5 * 1))
) %>% 
  pivot_longer(-error, names_to = "type", values_to = "loss") %>% 
  mutate(type = factor(
    type, 
    levels = c("sq_err", "abs_err", "huber1"), 
    labels = c("Squared Loss", "Absolute Loss", "Huber Loss (delta=1)")
  )) %>% 
  ggplot(aes(x = error, y = loss, color = type)) + 
  facet_wrap(~type, nrow = 1) + 
  geom_line(size = 1.5) + 
  labs(x = "Error (y - p)", y = "Loss") +
  theme_xaringan(text_font_size = 18, title_font_size = 18) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Correlation Metrics for Regression

**R-Squared $(R^2$ or RSQ)**
- Calculated in ML as the **squared correlation** between the predictions and labels
- Ranges from $0$ to $1$, higher is better

$$R^2 = \left(\frac{\text{cov}(y, p)}{\sigma_y\sigma_p}\right)^2 = \left(\frac{\sum(y_i - \bar{y})(p_i - \bar{p})}{\sqrt{\sum (y_i-\bar{y})^2}\sqrt{\sum(p_i-\bar{p})^2}}\right)^2$$

--

.bg-light-yellow.b--light-red.ba.bw1.br3.pl4[
**Caution!**
+ RSQ is a measure of *consistency* (i.e., linear association) and not distance
+ RSQ can become unstable or undefined when data variability is low
+ RSQ can become unstable when applied in small samples (e.g., test sets)

]

---
class: onecol
## Advanced Correlation Metrics for Regression

**Concordance Correlation Coefficient (CCC)**

- Combines both accuracy (distance) and consistency (correlation) information

- Very similar to certain formulations of the intraclass correlation coefficient

- Ranges from $-1$ to $+1$, where higher is better

.pt1[
$$CCC = \frac{2\rho_{yp}\sigma_y\sigma_p}{\sigma_y^2 + \sigma_p^2 + (\mu_y - \mu_p)^2}
= \frac{\frac{2}{n}\sum (y_i - \bar{y})(p_i - \bar{p})}{\frac{1}{n}\sum(y_i - \bar{y})^2 + \frac{1}{n}\sum(p_i - \bar{p})^2 + (\bar{y} - \bar{p})^2}$$
]
---
class: onecol
## Comparing Regression Performance Metrics

```{r dvc, echo=FALSE}
set.seed(2021)
dvc <- tibble(
  label = rnorm(100, 0, 10),
  pred1 = label + rnorm(100, 0, 5),
  pred2 = 5 * label + rnorm(100, 0, 28)
)

title1 <- paste0(
  "RMSE = ", round(yardstick::rmse_vec(dvc$label, dvc$pred1), 1),
  ", MAE = ", round(yardstick::mae_vec(dvc$label, dvc$pred1), 1),
  "\nRSQ = ", round(yardstick::rsq_vec(dvc$label, dvc$pred1), 2),
  ", CCC = ", round(yardstick::ccc_vec(dvc$label, dvc$pred1), 2)
)

p1 <- ggplot(dvc, aes(x = label, y = pred1)) + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "grey") + 
  geom_point(color = "#1b9e77", size = 2) +
  labs(x = "Trusted Label", y = "Model 1's Predictions", title = title1) +
  theme_xaringan(text_font_size = 20, title_font_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.title.position = "panel",
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )

title2 <- paste0(
  "RMSE = ", round(yardstick::rmse_vec(dvc$label, dvc$pred2), 1),
  ", MAE = ", round(yardstick::mae_vec(dvc$label, dvc$pred2), 1),
  "\nRSQ = ", round(yardstick::rsq_vec(dvc$label, dvc$pred2), 2),
  ", CCC = ", round(yardstick::ccc_vec(dvc$label, dvc$pred2), 2)
)

p2 <- ggplot(dvc, aes(x = label, y = pred2)) + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "grey") + 
  geom_point(color = "#d95f02", size = 2) +
  labs(x = "Trusted Label", y = "Model 2's Predictions", title = title2) +
  theme_xaringan(text_font_size = 20, title_font_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5),
    plot.title.position = "panel",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )

p1 | p2
```

---
class: inverse, center, middle
# Metrics for Classification 
## (Based on Predicted Classes)
---
class: twocol
## Confusion Matrix Metrics

 | Trusted = No<br /> $(y=0)$ | Trusted = Yes<br /> $(y=1)$
:--:| :--: | :--:
**Predicted = No**&emsp;<br /> $(p=0)$ | True Negatives (TN) | False Negative (FN)
**Predicted = Yes**&emsp;<br /> $(p=1)$ | False Positive (FP) | True Positive (TP)

--

.mt4[
$$\text{Accuracy} = \frac{TN + TP}{n}$$
]
.tc[
Ranges from $0$ to $1$, higher is better
]

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Accuracy can be misleading when the classes are highly imbalanced (e.g., more $0$s than $1$s)
]

---
class:onecol
## Confusion Matrix Metrics

With imbalanced classes, predicting the larger class will often be right "by chance"

--

To "correct" accuracy for imbalanced classes, **Cohen's Kappa** is often used

$$\kappa = \frac{\text{Accuracy} - \text{Chance}}{1 - \text{Chance}}$$

--

Chance agreement is estimated using the observed class probabilities from $y$ and $p$

$$\text{Chance} = \Pr(y=0)\cdot\Pr(p=0) + \Pr(y=1)\cdot\Pr(p=1)$$

Kappa also ranges from $0$ to $1$ (technically $-1$ to $1$), higher is better

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Paradoxically, Kappa can be overly conservative when classes are very imbalanced.
]

---
## Additional Confusion Matrix Metrics

 | $y=0$ | $y=1$
:--:| :--: | :--:
&emsp; ** $p=0$**&emsp; | &emsp;True Negatives (TN)&emsp; | &emsp;False Negative (FN)&emsp;
** $p=1$** | False Positive (FP) | True Positive (TP)

.pull-left.pt3[
$$\begin{split}
\text{Sensitivity} &= \frac{TP}{TP+FN} \\
\text{Specificity} &= \frac{TN}{TN + FP} \\
\text{Balanced Accuracy} &= \frac{\text{Sensitivity} + \text{Specificity}}{2} \\
\end{split}$$
]

--

.pull-right.pt3[
$$\begin{split}
\text{Precision} &= \frac{TP}{TP+FP} \\
\text{Recall} &= \frac{TP}{TP+FN} \\
F_1 \text{ Score} &= \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
\end{split}$$
]

.tc[
All range from $0$ to $1$, higher is better
]

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
$F_1$ does not consider $TN$, so only use it when detecting negatives is not important for your application.]



---
## Confusion Matrix Metrics Examples
.pull-left.tc[
&emsp;**Balanced**&emsp; | &emsp; $y=0$ &emsp; | &emsp; $y=1$ &emsp;
:--:| :--: | :--:
** $p=0$ ** | 101 | 54
** $p=1$ ** | 33 | 105

.pt3.f5[
$$\begin{split}
\text{Accuracy} &= 0.70 \\
\kappa &= 0.41\\ \\
\text{Sensitivity} &= 0.66 \\
\text{Specificity} &= 0.75 \\
\text{Balanced Accuracy} &= 0.71 \\ \\
\text{Precision} &= 0.76 \\
\text{Recall} &= 0.66 \\
F_1 &= 0.71
\end{split}$$

]
]

--

.pull-right.tc[
&emsp;**Imbalanced**&emsp; | &emsp; $y=0$ &emsp; | &emsp; $y=1$ &emsp;
:--:| :--: | :--:
** $p=0$ ** | 256 | 11
** $p=1$ ** | 6 | 2

.pt3.f5[

$$\begin{split}
\text{Accuracy} &= 0.94 \\ 
\kappa &= 0.16 \\
\\
\text{Sensitivity} &= 0.15 \\
\text{Specificity} &= 0.98 \\
\text{Balanced Accuracy} &= 0.57 \\
\\
\text{Precision} &= 0.25 \\
\text{Recall} &= 0.15 \\
F_1 &= 0.19
\end{split}$$
]
]

---
exclude: true
class: onecol
## Multiclass Performance Strategies

With more than two classes, you can make a larger (e.g., $3\times 3$) confusion matrix)

 | &emsp; $y$ = Healthy&emsp; | &emsp; $y$ = Depression&emsp; | &emsp; $y$ = Mania&emsp;
:--| :--: | :--: | :--:
** $p$ = Healthy**&emsp; | 100 | 3 | 7
** $p$ = Depression**&emsp; | 30 | 25 | 20 
** $p$ = Mania**&emsp; | 10 | 1 | 10

<!-- -- -->

.pt3[
![:emphasize](Macro-averaging): compute the standard binary metric for each class separately (using a one-vs-rest procedure) and then calculate the average metric score across classes
]

<!-- $$(F_1^H = 0.79, F_1^D = 0.46, F_1^M = 0.34)~\text{Macro }F_1=0.53$$ -->

<!-- -- -->

![:emphasize](Micro-averaging): compute a confusion matrix for each class separately (one-vs-rest), add these matrices together, and calculate the binary metric from the summed matrix

<!-- $$ (\Sigma TN=345, \Sigma TP=135, \Sigma FP=75, \Sigma FN=75)~\text{Micro }F_1=0.64$$ -->

---
class: inverse, center, middle
# Metrics for Classification 
## (Based on Class Probabilities)
---
class: onecol
## Class Probability Metrics for Classification

Some classifiers estimate **the probability of each class** as their prediction $(p_{ij})$

If we consider a higher estimated probability as higher "confidence"

- We can **reward** the classifier for being more **confident when correct**...

- ...and ![:emphasize](penalize) the classifier for being more ![:emphasize](confident when wrong)

--

This gives rise to the Logistic or **Log Loss**, which can be summed or averaged

$$L_{log}(Y,P) = -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^q \left(y_{ij}\log( p_{ij})\right)$$

.pl4[
$y_{ij}\in\{0,1\}$ is a binary indicator of whether observation $i$ is truly in class $j$<br />
$p_{ij}\in(0,1)$ is the estimated probability that observation $i$ is in class $j$
]

---

## Visualizing Log Loss in Binary Classification

```{r logloss, echo=FALSE}
tibble(
  truth = factor(rep(0:1, each = 501), levels = c(1, 0)),
  p = c(seq(0, 1, length.out = 501), seq(1, 0, length.out = 501))
) %>% 
  rowwise() %>% 
  mutate(loss = yardstick::mn_log_loss_vec(truth, p, event_level = "first")) %>% 
  ggplot(aes(x = p, y = loss, color = truth, linetype = truth)) + 
  geom_line(size = 2.25) +
  coord_cartesian(ylim = c(0, 8)) +
  scale_color_brewer(palette = "Set2", guide = guide_legend(
    keywidth = unit(2, "cm"), reverse = TRUE)) +
  scale_linetype_discrete(guide = guide_legend(reverse = TRUE)) +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "Predicted Probability of Class 1", y = "Log Loss", 
       color = "True Class", linetype = "True Class") +
  theme_xaringan(text_font_size = 18, title_font_size = 16) +
  theme(
    axis.title = element_text(size = 18),
    legend.position = "top",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Performance Curves

- When a classifier outputs class probabilities, we can choose any **decision threshold**

- We might naturally consider any probability over 50% positive and all others negative

- But we could choose a threshold more conservative (e.g., 75%) or liberal (e.g., 25%)

--

.pt1[
- **Performance curves** plot the characteristics of different decision thresholds

- This gives us an overview of how the classification system performs in general
]

--

.pt1[
- There are many performance curves<sup>1</sup>, so we'll use the original as an example

- Finally, the **area under the curve (AUC)** is often used as a performance metric
]

.footnote[[1] Popular options include ROC curves, precision-recall curves, gain curves, and lift curves.]

---
class: onecol
## Receiver Operating Characteristic (ROC) Curves

.pull-left[
Each point in a ROC curve corresponds to a possible decision threshold

The performance metrics compared for each point are Sensitivity and Specificity

Better curves are closer to the top-left

The area under the ROC curve (AUC-ROC) ranges from $0.5$ to $1.0$, higher is better.

AUCROC is the probability that a random positive example has a higher estimate than a random negative example.

]

.pull-right[
```{r rocex, echo=FALSE, fig.width=7, fig.height=6.8, out.width='95%'}
test_data <- read_rds("preds.rds")
yardstick::roc_curve(test_data, yes, truth = obs, event_level = "second") %>% 
  arrange(sensitivity) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dotted", size = 2, color = "grey") +
  geom_line(size = 2, color = "darkblue") +
  annotate(geom = "text", x = 0.75, y = 0.125, size = 12, label = "AUC = 0.82", 
           color = "darkblue") +
  coord_fixed() +
  labs(x = "1 - Specificity", y = "Sensitivity") +
  theme_xaringan(text_font_size = 20) +
  theme(
    axis.title = element_text(size = 26),
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```
]

---
class: twocol
## Comprehension Check \#1

<span style="font-size:30px;">Bindi trains Model [A] to predict how many kilometers each bird will migrate this year and Model [B] to predict whether or not it will reproduce this year.</span>

.pull-left[
**1. Which combination of performance metrics would be appropriate to use?**

a) Log Loss for [A] and CCC for [B]

b) Precision for [A] and Recall for [B]

c) MAE for [A] and Balanced Accuracy for [B]

d) None of the above

]

.pull-right[
**2. Which combination of performance scores should Bindi hope to see?**

a) RMSE = 531.6 and AUC-ROC = 0.04

b) RMSE = 1129.7 and AUC-ROC = 0.04

c) RMSE = 531.6 and AUC-ROC = 0.88

d) RMSE = 1129.7 and AUC-ROC = 0.88
]

---
class: inverse, center, middle
# Training and Cross-Validation
---
class: onecol
## `caret::train()`

{caret} standardizes the syntax to train over 200 different ML algorithms

It also plays nicely with the {recipes} package we learned for preprocessing

--

<p style="padding-top:20px;">The train() function will handle model <b>training</b>, <b>resampling</b>, and <b>tuning</b></p>

Because LM and GLM have no hyperparameters, we don't need tuning (yet!)

--

<p style="padding-top:20px;">Today, we will <b>focus on training</b> and just use resampling to estimate performance</p>

We will point to where tuning would be configured but leave that for tomorrow

---
class: onecol
## Main Arguments to `train()`

Argument&emsp; | Description
:------- | :----------
x | A {recipe} object with variable roles and preprocessing steps
data | A data frame to be used for training (prior to prep and bake)
method | A string indicating the algorithm to train (e.g., "lm" or "glm")&emsp;

--

To train any of the 200+ supported algorithms, you just need to change `method`

This makes it *super easy* to implement new algorithms and explore the world of ML!

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Be sure you understand an algorithm before trying to publish a paper using it.
]

---
class: onecol
## Additional Arguments to `train()`

Argument | Description
:------- | :----------
trControl | Controls the resampling procedure used during tuning
metric | Controls which metric to optimize during tuning
tuneGrid | Control which specific tuning values to compare
tuneLength&emsp; | Control how many tuning values to automatically compare&emsp;

These arguments are largely used to configure tuning (discussed tomorrow)

The training set will be resampled and different tuning values will be compared

The "best" tuning values will be used to train a **final model** using all the training data

---
class: onecol
## Resampling options

The `trControl` argument can be configured by `trainControl()`:

Argument&emsp; | Description
:------- | :----------
method | Controls the type of resampling (e.g., "cv", "repeatedcv", "boot")&emsp;
number | Controls the number of folds in cv and iterations in boot
repeats | Controls the number of repetitions in repeatedcv

"cv" will perform resampling through $k$-fold cross-validation

"repeatedcv" will repeat $k$-fold cross-validation multiple times

"boot" will perform resampling through bootstrapping

---
class: onecol
## Pseudo-code for training with resampling

```{r, eval=FALSE}
# Configure resampling options
resampling_options <- trainControl( #<<
  method = "repeatedcv", #<<
  number = 10, #<<
  repeats = 3 #<<
) #<<

# Train model from recipe
trained_model <- train(
  x = my_recipe,
  data = my_training,
  method = "lm",
  trControl = resampling_options #<<
)
```

---
exclude: true
class: onecol
## Advanced Resampling Options

There are many other arguments to `trainControl()` to explore

- **`selectionFunction`** can be used to prioritize less complex models*
- **`predictionBounds`** can be used to constrain predicted values in regression
- **`sampling`** can be used to address imbalanced labels in classification*
- **`seeds`** can be used to make the resampling procedure reproducible

.footnote[*These are both advanced topics that we can revisit on Day 5.]

There are also alternative resampling methods to explore
- Fancier versions of bootstrapping (e.g., "boot632")
- Algorithm-specific methods (e.g., "oob")
- Adaptive methods that tune faster/smarter (e.g., "adaptive_cv")
- The {rsample} package from {tidymodels} provides even more options

---
## Comprehension Check \#2

.pull-left[
```{r, eval=FALSE}
# Part 1
my_recipe <- 
  my_data %>% 
  recipe(outcome ~ .) %>% 
  step_center(all_numeric()) %>% 
  step_zv(all_predictors()) %>% 
  prep(training = my_training)

# Part 2
trained_model <- train(
  x = my_recipe,
  data = my_training_set,
  method = "cv"
)
```

]

.pull-right[
**1. What was the main mistake made in Part 1?**

&emsp; a) The predictors should be listed in `recipe()`

&emsp; b) Numeric predictors cannot be centered

&emsp; c) `step_zv()` only works for numeric predictors

&emsp; d) The recipe should not be prepped yet

**2. What was the main mistake made in Part 2?**

&emsp; a) `data` should be `my_testing_set`

&emsp; b) `x` should be `my_data`

&emsp; c) "cv" is not a method for `train()`

&emsp; d) Forgot to add the `number` argument
]

---
class: onecol
# Applied Example
Let's put what we just learned into practice in R

.pt1[
Let's use {caret} and {recipes} to train a regression model on the `titanic` data
]

- We will load in and split the data

- We will create a recipe for feature engineering

- We will train LM to predict each passenger's fare (how much they paid)

---
class: onecol
## Applied Example

```{r, message=FALSE}
# Read in data
titanic <- read_csv("https://osf.io/juez2/download")

# Create a training and testing set (stratified by fare)
set.seed(2021)
fare_index <- createDataPartition(titanic$fare, p = 0.75, list = FALSE)
fare_train <- titanic[fare_index, ]
fare_test <- titanic[-fare_index, ]
```

--

```{r}
# Check sizes
dim(fare_train)
dim(fare_test)
```

---
class: onecol
## Applied Example

```{r}
# Create a preprocessing recipe (don't prep or bake)
fare_recipe <- 
  titanic %>% 
  recipe(fare ~ .) %>% 
  step_rm(survived) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors()) %>% 
  step_lincomb(all_predictors())
```

---
class: onecol
## Applied Example 

```{r}
set.seed(2021)

# Configure resampling
fare_tc <- trainControl(method = "cv", number = 10)

# Train the model using the recipe, data, and method
fare_lm <- train(
  fare_recipe, 
  data = fare_train,
  method = "lm",
  trControl = fare_tc
)
```

---
class: inverse, center, middle
# Model Evaluation
---
class: onecol
## Training Set Performance

The object created by `train()` will contain lots of information

We can view a summary of training set performance in the `$results` field
- Each row corresponds to one combination or set of hyperparameters
- Columns define the **hyperparameter values** and the set's **performance scores** 
- Results will include the Mean and SD of each metric across resamples

--

```{r, eval=FALSE}
fare_lm$results
```

```{r, echo=FALSE}
kable(fare_lm$results, digits = 2)
```

.footnote[[1] Because there are no real hyperparameters for LM, there is only one row in `results`.]

---
class: onecol
## Test Set Predictions
To evaluate performance on the test set, we need two things:

1. Predictions from the model on the test set (e.g., values, classes, or probabilities)

2. Trusted labels on the test set (after any preprocessing steps from {recipes})

--

.pt1[
We can compare these predicted and trusted labels using {caret} or {yardstick}
]
- {caret} has basic performance metrics built in and is straightforward to use

- {yardstick} offers many more options but requires some coding to work with {caret}

---
class: onecol
## Test Set Predictions

To get predictions from the final model on new data<sup>1</sup>, we can use `predict()`

Argument&emsp; | Description
:------- | :----------
object | A trained model object created by `train()`
newdata | A data frame with the same features (prior to baking)&emsp;
type | Return raw classes ("raw") or probabilities ("prob")?

.footnote[[1] The same process is used for both evaluating performance and deploying the model in the real world.]

--

.pt3[
```{r}
fare_pred <- predict(fare_lm, newdata = fare_test)
glimpse(fare_pred)
```
]



---
class: onecol
## Test Set Labels 
To get labels from the final model in the format expected by R, we can use `bake()`

```{r}
fare_test_baked <- 
  fare_recipe %>% 
  prep(training = fare_train) %>% 
  bake(new_data = fare_test)

fare_true <- fare_test_baked$fare
glimpse(fare_true)
```

--

A few points of potential confusion to avoid:

1. Use `train()` and `predict()` before baking the recipe (i.e., let them do it)
2. `caret::predict()` uses `newdata`, whereas `recipes::bake()` uses `new_data`

---
class: onecol
## Visualizing Predicted and Trusted Labels

```{r, eval=FALSE}
qplot(x = fare_true, y = fare_pred) + geom_abline()
```

```{r quick, echo=FALSE, fig.height=3.25}
qplot(x = fare_true, y = fare_pred) + geom_abline() +
  theme_xaringan(text_font_size = 20) +
  theme(
    axis.title = element_text(size = 20),
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Estimating Test Set Performance

To estimate performance in {caret}, use `postResample()`

Argument&emsp; | Description
:------- | :----------
pred | A vector of predicted labels&emsp;
obs | A vector of trusted labels

--

```{r}
postResample(pred = fare_pred, obs = fare_true)
```

This function will give basic results (e.g., Accuracy) for classification too

---
class: onecol
## More Metrics from {yardstick}

The {yardstick} package provides lots of performance metrics

For regression, I like the **Huber Loss** distance metric and **CCC** correlation metric

```{r}
# Huber Loss (blends RMSE and MAE)
yardstick::huber_loss_vec(truth = fare_true, estimate = fare_pred)
```

```{r}
# Concordance Correlation Coefficient
yardstick::ccc_vec(truth = fare_true, estimate = fare_pred)
```

--

For classification, I like the **MCC** class metric and **Log Loss** probability metric

---
class: inverse, center, middle
# Model Interpretation
---
class: onecol
## Variable Importance

Predictive **accuracy** is emphasized in ML over interpretability and inference

- The main goal of most applied ML studies is to **quantify performance**

--

However, some algorithms can provide insight into their decision-making

- As a model usually used for inference, LM has strong interpretability

- We can examine the model coefficients (intercept and slopes)

--

```{r, eval=FALSE}
fare_lm$finalModel %>% coefficients()
```

```{r, echo=FALSE}
fare_lm$finalModel %>% coefficients() %>% t() %>% kable(digits = 2)
```

---
class: onecol
## Variable Importance

However, these coefficients have different units and levels of uncertainty

Instead, for LM, we use the absolute value of each coefficient's $t$-statistic

--

.pull-left[
```{r, out.width='80%'}
varImp(fare_lm, scale = FALSE)
```
]

--

.pull-right[
```{r, out.width='80%'}
varImp(fare_lm, scale = TRUE)
```
]

--

.footnote[[1] Other algorithms have different ways to estimate variable importance, but `varImp()` will take care of it.]

---
class: onecol
## Live Coding

Now that we have explored the prediction of passenger fare, let's predict survival

Training, evaluating, and interpreting a ![:emphasize](classification) model is very similar but...

- We will be using GLM (logistic regression) instead of LM as our **new algorithm**

- We will have some **different performance metrics** to calculate and interpret

- We can explore the **raw class predictions** and **estimated class probabilities**

--

.pt1[
Finally, as a hands-on activity, you will classify `satisfaction` in the `airsat` dataset
]

---

## Hands-on Activity

Modify the [Live Coding example code](https://osf.io/gtbzq/) to achieve the following goals:

 1. Read in the `airsatisfaction.csv` file from https://osf.io/7sqcn/download
 
 1. Create a 75% training and 25% testing set stratified by `satisfaction`
 
 1. Create a recipe predicting `satisfaction` with the following steps:
  + `step_nzv(all_predictors())`
  + `step_normalize(all_numeric_predictors())`
  + `step_pca(all_numeric_predictors(), threshold = 0.9)`
  + `step_dummy(all_nominal_predictors())`
  
 1. Train a GLM with this recipe with 10-fold cross-validation repeated 3 times

 1. Estimate Accuracy and Kappa in the training and testing sets
 
**BONUS 1:** Examine and plot the model's variable importance. What effect does PCA have on interpretation?
 
**BONUS 2:** Construct a confusion matrix and ROC curve for the model
 