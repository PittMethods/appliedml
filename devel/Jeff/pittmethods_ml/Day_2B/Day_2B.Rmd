---
title: '<span style="font-size:48pt;">Basic Predictive Modeling</span>'
subtitle: 'üßÆ üîÆ üë®‚Äçüíª' 
author: 'Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series'
date: 'Lecture 2-B &emsp; &emsp; July 20, 2021'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  footnote_font_size = "20px",
  footnote_color = "gray",
  text_slide_number_font_size = "18px"
)
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(caret)
library(recipes)
```

class: inverse, center, middle
# Overview

<style type="text/css">
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
</style>
---
class: onecol
## Lecture Plan
<p style="padding-top:15px;">We will begin with <b>overview slides</b> and then dive into an <b>extended live coding</b></p>
- Training and Cross-Validation
- Model Evaluation
- Model Interpretation

--

We will start by **adapting familiar algorithms** to the predictive modeling framework
- Linear modeling (multiple regression) will be used for *regression*
- Generalized linear modeling (logistic regression) will be used for *classification*

--

We will also **foreshadow future topics** (e.g., regularized regression and tuning)

---
class: inverse, center, middle
# Training and Cross-Validation
---
class: onecol
## `caret::train()`

{caret} standardizes the syntax to train over 200 different ML algorithms

It also plays nicely with the {recipes} package we learned for preprocessing

--

<p style="padding-top:20px;">The train() function will handle model <b>training</b>, <b>resampling</b>, and <b>tuning</b></p>

Because LM and GLM have no hyperparameters, we don't need tuning (yet!)

--

<p style="padding-top:20px;">Today, we will <b>focus on training</b> and just use resampling to estimate performance</p>

We will point to where tuning would be configured but leave that for tomorrow

---
class: onecol
## Main Arguments to `train()`

Argument&emsp; | Description
:------- | :----------
`x` | A {recipe} object with variable roles and preprocessing steps
`data` | A data frame to be used for training (prior to baking)
`method` | A string indicating the algorithm to train (e.g., "lm" or "glm")&emsp;

--

To train any of the 200+ supported algorithms, you just need to change `method`<sup>1</sup>

This makes it *super easy* to implement new algorithms and explore the world of ML!

But be sure you understand an algorithm before trying to publish a paper using it

.footnote[[1] Of course, you may also need to adapt the features and tuning procedures to match the new algorithm.]

---
class: onecol
## Pseudocode for simple training

```{r, eval=FALSE}
# Create data splits
set.seed(2021)
index <- createDataPartition(my_data$outcome, p = 0.80, list = FALSE)
my_training_set <- my_data[index, ]
my_testing_set <- my_data[-index, ]

# Set up recipe
my_recipe <- 
  my_data %>% 
  recipe(outcome ~ .) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors())

# Train model from recipe
trained_model <- train( #<<
  x = my_recipe, #<<
  data = my_training_set, #<<
  method = "lm" #<<
) #<<
```

---
class: onecol
## Additional Arguments to `train()`

Argument | Description
:------- | :----------
`trControl` | Controls the resampling procedure used during tuning
`metric` | Controls which metric to optimize during tuning
`tuneGrid` | Control which specific tuning values to compare
`tuneLength`&emsp; | Control how many tuning values to automatically compare&emsp;

These arguments are largely used to configure tuning (discussed tomorrow)

The training set will be resampled and different tuning values will be compared

The "best" tuning values will be used to train a **final model** using all the training data

---
class: onecol
## Resampling options

The `trControl` argument can be configured by `trainControl()`:

Argument&emsp; | Description
:------- | :----------
`method` | Controls the type of resampling (e.g., "cv", "repeatedcv", "boot")&emsp;
`number` | Controls the number of folds in cv and iterations in boot
`repeats` | Controls the number of repetitions in repeatedcv

"cv" will perform resampling through $k$-fold cross-validation

"repeatedcv" will repeat $k$-fold cross-validation multiple times

"boot" will perform resampling through bootstrapping

---
class: onecol
## Pseudocode for training with resampling

```{r, eval=FALSE}
# Configure resampling options
resampling_options <- trainControl( #<<
  method = "repeatedcv", #<<
  number = 10, #<<
  repeats = 3 #<<
) #<<

# Train model from recipe
trained_model <- train(
  x = my_recipe,
  data = my_training_set,
  method = "lm",
  trControl = resampling_options #<<
)
```

---
class: onecol
## Advanced Resampling Options

There are many other arguments to `trainControl()` to explore

- **`selectionFunction`** can be used to prioritize less complex models*
- **`predictionBounds`** can be used to constrain predicted values in regression
- **`sampling`** can be used to address imbalanced labels in classification*
- **`seeds`** can be used to make the resampling procedure reproducible

.footnote[*These are both advanced topics that we plan to revisit on Day 5-A.]

--

There are also alternative resampling methods to explore
- Fancier versions of bootstrapping (e.g., "boot632")
- Algorithm-specific methods (e.g., "oob")
- Adaptive methods that tune faster/smarter (e.g., "adaptive_cv")

---
## Comprehension check

.pull-left[
```{r, eval=FALSE}
# Assume packages and data are loaded

# Part 1
my_recipe <- 
  my_data %>% 
  recipe(outcome ~ .) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  prep(training = my_training_set)

# Part 2
trained_model <- train(
  x = my_recipe,
  data = my_training_set,
  method = "cv"
)
```

]

.pull-right[
**1. What was the main mistake made in Part 1?**

&emsp; a) The predictors should be listed in `recipe()`

&emsp; b) Numeric predictors cannot be centered

&emsp; c) `step_zv()` only works for numeric predictors

&emsp; d) The recipe should not be prepped yet

**2. What was the main mistake made in Part 2?**

&emsp; a) `data` should be `my_testing_set`

&emsp; b) `x` should be `my_data`

&emsp; c) "cv" is not a method for `train()`

&emsp; d) Forgot to add the `number` argument
]

---
class: inverse, center, middle
# Model Evaluation
---
class: onecol
## Performance Metrics
.left-column[
<br />
```{r target, echo=FALSE}
include_graphics("target.jpg")
```
]

.right-column[

**Metrics for Supervised Regression**
- ![:emphasize](Distance) between predicted and trusted values
- ![:emphasize](Correlation) between predicted and trusted values

**Metrics for Supervised Classification**
- ![:emphasize](Confusion matrix) between predicted and trusted classes
- Compare predicted ![:emphasize](class probabilities) to trusted classes
- Construct a ![:emphasize](performance curve) across decision thresholds
]

---
class: onecol
## Classic Distance Metrics for Regression

.pull-left[
**Root Mean Squared Error (RMSE)**
- Based on squared loss
- Easier to computationally optimize
- Ranges from $0$ to $+\infty$, lower is better
]

.pull-right[
<br />
$$RMSE=\sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2}$$
]

.pull-left[
**Mean Absolute Error (MAE)**
- Based on absolute loss
- More robust to outliers
- Ranges from $0$ to $+\infty$, lower is better
]

.pull-right[
<br />
$$MAE=\frac{1}{n} \sum_{i=1}^n \left\lvert y_i - p_i \right\rvert$$
]

.footnote[[1] Note that, here, we will refer to the trusted labels as `\\(y\\)` and the predicted labels as `\\(p\\)`.]

---
class: onecol
exclude: true
## Advanced Distance Metrics for Regression
**Huber loss (HL)**

- Combines benefits of RMSE and MAE
 + Easy to computationally optimize AND more robust to outliers
- Requires setting or tuning $\delta$ (controls what to consider an outlier)
- Ranges from $0$ (best) to $+\infty$ (worst), lower is better

$$\begin{split}
HL &= \frac{1}{n} \sum_{i=1}^n L_\delta(y_i, p_i) \\
L_\delta(y_i, p_i) &= \begin{cases}\frac{1}{2}(y_i - p_i)^2 & \text{for } \lvert y_i - p_i \rvert \le \delta \\ \delta (\lvert y_i - p_i \rvert - \frac{1}{2}\delta) & \text{otherwise} \end{cases}
\end{split}$$

---
## Comparing Loss Functions

```{r losses, echo=FALSE}
tibble(
  error = seq(-3, 3, length.out = 100),
  sq_err = error^2,
  abs_err = abs(error)
) %>%
  pivot_longer(-error, names_to = "type", values_to = "loss") %>%
  mutate(
    type = factor(
      type,
      levels = c("sq_err", "abs_err"),
      labels = c("Squared Loss (RMSE)", "Absolute Loss (MAE)")
    )
  ) %>%
  ggplot(aes(x = error, y = loss, color = type)) +
  facet_wrap(~type, nrow = 1) +
  geom_line(size = 1.5) +
  labs(x = "Error (y - p)", y = "Loss") +
  theme_xaringan(text_font_size = 20, title_font_size = 20) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "white")
  )
```

---
exclude: true
## Comparing Loss Functions

```{r losses3, echo=FALSE}
tibble(
  error = seq(-2, 2, length.out = 100), 
  sq_err = error^2,
  abs_err = abs(error),
  huber1 = ifelse(abs(error) <= 1, 0.5 * error^2, 1 * (abs(error) - 0.5 * 1))
) %>% 
  pivot_longer(-error, names_to = "type", values_to = "loss") %>% 
  mutate(type = factor(
    type, 
    levels = c("sq_err", "abs_err", "huber1"), 
    labels = c("Squared Loss", "Absolute Loss", "Huber Loss (delta=1)")
  )) %>% 
  ggplot(aes(x = error, y = loss, color = type)) + 
  facet_wrap(~type, nrow = 1) + 
  geom_line(size = 1.5) + 
  labs(x = "Error (y - p)", y = "Loss") +
  theme_xaringan(text_font_size = 18, title_font_size = 18) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Correlation Metrics for Regression

**R-Squared $(R^2$ or RSQ)**
- How much of the variability in the trusted labels do the predictions explain?
 + Calculated in ML as the **squared correlation** between the predictions and labels
- Several cautions about using RSQ as a performance metric
 + It is a measure of **consistency** and not accuracy (may disagree with distance)
 + It cannot be calculated if the predicted or trusted labels are constant
 + It is highly sensitive to the amount of label variability (may be unstable)
- Ranges from $0$ to $1$, higher is better

$$R^2 = \left(\frac{\text{cov}(y, p)}{\sigma_y\sigma_p}\right)^2 = \left(\frac{\sum(y_i - \bar{y})(p_i - \bar{p})}{\sqrt{\sum (y_i-\bar{y})^2}\sqrt{\sum(p_i-\bar{p})^2}}\right)^2$$

---
class: onecol
## Comparing Distance and Correlation Metrics

```{r dvc, echo=FALSE}
set.seed(2021)
dvc <- tibble(
  label = rnorm(100, 0, 10),
  pred1 = label + rnorm(100, 0, 5),
  pred2 = 5 * label + rnorm(100, 0, 28)
)

title1 <- paste0(
  "RMSE=", round(yardstick::rmse(dvc, truth = label, estimate = pred1)$.estimate, 1),
  ", MAE=", round(yardstick::mae(dvc, truth = label, estimate = pred1)$.estimate, 1),
  ", RSQ=", round(yardstick::rsq(dvc, truth = label, estimate = pred1)$.estimate, 2)
)

p1 <- ggplot(dvc, aes(x = label, y = pred1)) + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "grey") + 
  geom_point(color = "#1b9e77", size = 2) +
  labs(x = "Trusted Label", y = "Model 1's Predictions", title = title1) +
  theme_xaringan(text_font_size = 20, title_font_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.title.position = "panel",
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )

title2 <- paste0(
  "RMSE=", round(yardstick::rmse(dvc, truth = label, estimate = pred2)$.estimate, 1),
  ", MAE=", round(yardstick::mae(dvc, truth = label, estimate = pred2)$.estimate, 1),
  ", RSQ=", round(yardstick::rsq(dvc, truth = label, estimate = pred2)$.estimate, 2)
)

p2 <- ggplot(dvc, aes(x = label, y = pred2)) + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "grey") + 
  geom_point(color = "#d95f02", size = 2) +
  labs(x = "Trusted Label", y = "Model 2's Predictions", title = title2) +
  theme_xaringan(text_font_size = 20, title_font_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5),
    plot.title.position = "panel",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )

p1 | p2
```


---
exclude: true
class: onecol
## Advanced Correlation Metrics for Regression

**Concordance Correlation Coefficient (CCC)**
- Equivalent to certain formulations of the intraclass correlation coefficient
- Combines both accuracy (distance) and consistency (correlation) information
- Ranges from $-1$ (worst) to $+1$ (best)

$$CCC = \frac{\frac{2}{n}\sum_{i=1}^n (t_i - \bar{t})(p_i - \bar{p})}{\frac{1}{n}\sum_{i=1}^n(t_i - \bar{t})^2 + \frac{1}{n}\sum_{i=1}^n(p_i - \bar{p})^2 + (\bar{t} - \bar{p})^2}$$
---
## Confusion Matrix Metrics for Classification

The classic confusion matrix is for binary (i.e., no/yes or 0/1) classification

 | Trusted = No | Trusted = Yes
:--| :--: | :--:
**Predicted = No** | True Negatives (TN) | False Negative (FN)
**Predicted = Yes** | False Positive (FP) | True Positive (TP)

The counts or proportions can be informative on their own or combined into metrics

--

.pull-left[
$$\begin{split}
\text{Accuracy} &= \frac{TN + TP}{(TN + FN + FP + TP)} \\
\text{Sensitivity} &= \frac{TP}{TP+FN} \\
\text{Specificity} &= \frac{TN}{TN + FP} \\
\text{Balanced Accuracy} &= (\text{Sensitivity} + \text{Specificity})/2 \\
\end{split}$$
]

--

.pull-right[
$$\begin{split}
\text{Precision} &= \frac{TP}{TP+FP} \\
\text{Recall} &= \text{Sensitivity} \\
F_1 \text{ Score} &= \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
PPV &= \text{Precision} \\
NPV &= \frac{TN}{TN + FN}
\end{split}$$
]

---
class: onecol
## Multiclass Confusion Matrix

With more than two classes, the same principles apply (but the math gets harder)

 | Trusted = Healthy | Trusted = Depression | Trusted = Mania
:--| :--: | :--: | :--:
**Predicted = Healthy** | 100 | 3 | 7
**Predicted = Depression** | 2 | 25 | 0 
**Predicted = Mania** | 10 | 1 | 10

$$\text{Average Accuracy} = \frac{1}{k}\sum_{j=1}^k\frac{TP_j + TN_j}{TP_j + TN_j + FP_j + FN_j}$$

.footnote[[1] In multiclass settings, there are micro- and macro-averages but that is a more advanced topic to explore later.]

---
class: onecol
## Class Probability Metrics for Classification

---
class: onecol
## Performance Curves for Classification

---
class: onecol
## Training Set Performance

---
class: onecol
## Test Set Predictions

predict()
- object
- newdata 
- type
- na.action

---

postResample()
- pred
- obs

---

confusionMatrix()

---

yardstick
- ccc
- huber_loss
- mcc
- roc_auc
- metric_set

---
class: inverse, center, middle
# Model Interpretation

varImp()
- object
- scale

train$finalModel