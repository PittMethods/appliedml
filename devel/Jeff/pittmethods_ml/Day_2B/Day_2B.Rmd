---
title: '<span style="font-size:48pt;">Basic Predictive Modeling</span>'
subtitle: 'üßÆ üîÆ üë®‚Äçüíª' 
author: 'Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series'
date: 'Lecture 2-B &emsp; &emsp; July 20, 2021'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  comment = "#>",
  collapse = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  footnote_font_size = "20px",
  footnote_color = "gray",
  text_slide_number_font_size = "18px"
)
```

```{r xaringan-tachyons, echo=FALSE}
xaringanExtra::use_tachyons()
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(caret)
library(recipes)
```

class: inverse, center, middle
# Overview

<style type="text/css">
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
.remark-code {
  font-size: 24px;
  border: 1px solid grey;
}
</style>
---
class: onecol
## Plan for Day 2-B
First will be a lecture on **Performance Metrics** (one of my favorite topics)

Second will be a brief **lecture overview** of training, evaluation, and interpretation in R

Third, we will apply this with a **live coding** session and hands-on activity

--

.pt1[
In parts 2 and 3, we will **adapt familiar algorithms** to predictive modeling
- Linear modeling will be adapted for continuous prediction (regression)
- Generalized linear modeling will be adapted for discrete prediction (classification)

*This will ease the transition to ML and highlight its similarities with classical stats*
]

--

.pt1[
We will also **foreshadow future topics** (e.g., regularized linear models and tuning)
]

---
class: inverse, center, middle
# Performance Metrics
---
class: onecol
## Performance Metrics

.left-column.pt3[
```{r target, echo=FALSE}
include_graphics("target.jpg")
```
]

.right-column[

**Metrics for Supervised Regression**
- ![:emphasize](Distance) between predicted and trusted values
- ![:emphasize](Correlation) between predicted and trusted values

.pt1[
**Metrics for Supervised Classification**
]
- ![:emphasize](Confusion matrix) between predicted and trusted classes
- Compare predicted ![:emphasize](class probabilities) to trusted classes
- Construct a ![:emphasize](performance curve) across decision thresholds
]

---
class: onecol
## Classic Distance Metrics for Regression

.pull-left[
**Root Mean Squared Error (RMSE)**
- Based on squared loss
- Easier to computationally optimize
- Ranges from $0$ to $+\infty$, lower is better
]

.pull-right.pt4[
$$RMSE=\sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2}$$
]

--

.pull-left[
**Mean Absolute Error (MAE)**
- Based on absolute loss
- More robust to outliers
- Ranges from $0$ to $+\infty$, lower is better
]

.pull-right.pt4[
$$MAE=\frac{1}{n} \sum_{i=1}^n \left\lvert y_i - p_i \right\rvert$$
]

.footnote[[1] Note that, here, we will refer to the trusted labels as `\\(y\\)` and the predicted labels as `\\(p\\)`.]

---
exclude: true
class: onecol
## Advanced Distance Metrics for Regression
**Huber loss (HL)**

- Combines benefits of RMSE and MAE
 + Easy to computationally optimize AND more robust to outliers
- Requires setting or tuning $\delta$ (controls what to consider an outlier)
- Ranges from $0$ (best) to $+\infty$ (worst), lower is better

$$\begin{split}
HL &= \frac{1}{n} \sum_{i=1}^n L_\delta(y_i, p_i) \\
L_\delta(y_i, p_i) &= \begin{cases}\frac{1}{2}(y_i - p_i)^2 & \text{for } \lvert y_i - p_i \rvert \le \delta \\ \delta (\lvert y_i - p_i \rvert - \frac{1}{2}\delta) & \text{otherwise} \end{cases}
\end{split}$$

---
## Visualizing Regression Loss Functions

```{r losses, echo=FALSE}
tibble(
  error = seq(-2, 2, length.out = 100), 
  sq_err = error^2,
  abs_err = abs(error)
) %>% 
  pivot_longer(-error, names_to = "type", values_to = "loss") %>% 
  mutate(type = factor(
    type, 
    levels = c("sq_err", "abs_err"), 
    labels = c("Squared Loss (RMSE)", "Absolute Loss (MAE)")
  )) %>% 
  ggplot(aes(x = error, y = loss, color = type)) + 
  facet_wrap(~type, nrow = 1) + 
  geom_line(size = 1.5) + 
  labs(x = latex2exp::TeX("Error $(y_i - p_i)$"), y = "Loss") +
  theme_xaringan(text_font_size = 18, title_font_size = 18) +
  theme(
    axis.title = element_text(size = 20),
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
exclude: true
## Comparing Loss Functions

```{r losses3, echo=FALSE}
tibble(
  error = seq(-2, 2, length.out = 100), 
  sq_err = error^2,
  abs_err = abs(error),
  huber1 = ifelse(abs(error) <= 1, 0.5 * error^2, 1 * (abs(error) - 0.5 * 1))
) %>% 
  pivot_longer(-error, names_to = "type", values_to = "loss") %>% 
  mutate(type = factor(
    type, 
    levels = c("sq_err", "abs_err", "huber1"), 
    labels = c("Squared Loss", "Absolute Loss", "Huber Loss (delta=1)")
  )) %>% 
  ggplot(aes(x = error, y = loss, color = type)) + 
  facet_wrap(~type, nrow = 1) + 
  geom_line(size = 1.5) + 
  labs(x = "Error (y - p)", y = "Loss") +
  theme_xaringan(text_font_size = 18, title_font_size = 18) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Correlation Metrics for Regression

**R-Squared $(R^2$ or RSQ)**
- Calculated in ML as the **squared correlation** between the predictions and labels
- Ranges from $0$ to $1$, higher is better

$$R^2 = \left(\frac{\text{cov}(y, p)}{\sigma_y\sigma_p}\right)^2 = \left(\frac{\sum(y_i - \bar{y})(p_i - \bar{p})}{\sqrt{\sum (y_i-\bar{y})^2}\sqrt{\sum(p_i-\bar{p})^2}}\right)^2$$

--

.bg-light-yellow.b--light-red.ba.bw1.br3.pl4[
**Caution!**
+ RSQ is a measure of *consistency* (i.e., linear association) and not distance
+ RSQ can become unstable or undefined when data variability is low
+ RSQ can become unstable when applied in small samples (e.g., test sets)

]

---
class: onecol
## Advanced Correlation Metrics for Regression

**Concordance Correlation Coefficient (CCC)**

- Combines both accuracy (distance) and consistency (correlation) information

- Very similar to certain formulations of the intraclass correlation coefficient

- Ranges from $-1$ to $+1$, where higher is better

.pt1[
$$CCC = \frac{2\rho_{yp}\sigma_y\sigma_p}{\sigma_y^2 + \sigma_p^2 + (\mu_y - \mu_p)^2}
= \frac{\frac{2}{n}\sum (y_i - \bar{y})(p_i - \bar{p})}{\frac{1}{n}\sum(y_i - \bar{y})^2 + \frac{1}{n}\sum(p_i - \bar{p})^2 + (\bar{y} - \bar{p})^2}$$
]
---
class: onecol
## Comparing Regression Performance Metrics

```{r dvc, echo=FALSE}
set.seed(2021)
dvc <- tibble(
  label = rnorm(100, 0, 10),
  pred1 = label + rnorm(100, 0, 5),
  pred2 = 5 * label + rnorm(100, 0, 28)
)

title1 <- paste0(
  "RMSE = ", round(yardstick::rmse_vec(dvc$label, dvc$pred1), 1),
  ", MAE = ", round(yardstick::mae_vec(dvc$label, dvc$pred1), 1),
  "\nRSQ = ", round(yardstick::rsq_vec(dvc$label, dvc$pred1), 2),
  ", CCC = ", round(yardstick::ccc_vec(dvc$label, dvc$pred1), 2)
)

p1 <- ggplot(dvc, aes(x = label, y = pred1)) + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "grey") + 
  geom_point(color = "#1b9e77", size = 2) +
  labs(x = "Trusted Label", y = "Model 1's Predictions", title = title1) +
  theme_xaringan(text_font_size = 20, title_font_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.title.position = "panel",
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )

title2 <- paste0(
  "RMSE = ", round(yardstick::rmse_vec(dvc$label, dvc$pred2), 1),
  ", MAE = ", round(yardstick::mae_vec(dvc$label, dvc$pred2), 1),
  "\nRSQ = ", round(yardstick::rsq_vec(dvc$label, dvc$pred2), 2),
  ", CCC = ", round(yardstick::ccc_vec(dvc$label, dvc$pred2), 2)
)

p2 <- ggplot(dvc, aes(x = label, y = pred2)) + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "grey") + 
  geom_point(color = "#d95f02", size = 2) +
  labs(x = "Trusted Label", y = "Model 2's Predictions", title = title2) +
  theme_xaringan(text_font_size = 20, title_font_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5),
    plot.title.position = "panel",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )

p1 | p2
```

---
class: twocol
## Confusion Matrix Metrics for Classification

 | Trusted = No<br /> $(y=0)$ | Trusted = Yes<br /> $(y=1)$
:--:| :--: | :--:
**Predicted = No**&emsp;<br /> $(p=0)$ | True Negatives (TN) | False Negative (FN)
**Predicted = Yes**&emsp;<br /> $(p=1)$ | False Positive (FP) | True Positive (TP)

.mt4[
$$\text{Accuracy} = \frac{TN + TP}{(TN + FN + FP + TP)}$$
]
.tc[
Ranges from $0$ to $1$, higher is better
]

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Accuracy can be misleading when the classes are highly imbalanced (e.g., if $TN \gg TP$)
]

---
## Additional Confusion Matrix Metrics

 | $y=0$ | $y=1$
:--:| :--: | :--:
&emsp; ** $p=0$**&emsp; | &emsp;True Negatives (TN)&emsp; | &emsp;False Negative (FN)&emsp;
** $p=1$** | False Positive (FP) | True Positive (TP)

.pull-left.pt3[
$$\begin{split}
\text{Sensitivity} &= \frac{TP}{TP+FN} \\
\text{Specificity} &= \frac{TN}{TN + FP} \\
\text{Balanced Accuracy} &= \frac{\text{Sensitivity} + \text{Specificity}}{2} \\
\end{split}$$
]

--

.pull-right.pt3[
$$\begin{split}
\text{Precision} &= \frac{TP}{TP+FP} \\
\text{Recall} &= \frac{TP}{TP+FN} \\
F_1 \text{ Score} &= \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
\end{split}$$
]

.tc[
All range from $0$ to $1$, higher is better
]

--

.footnote[[1] Note that the formulas for Sensitivity and Recall are the same. Many of these metrics enjoy many names.<br />[2] `\\(F_1\\)` does not consider `\\(TN\\)`, which is sometimes a reason to use it and sometimes a reason to avoid it.]

---
## Confusion Matrix Metrics Examples
.pull-left.tc[
**Balanced Classes**

 | $y=0$ | $y=1$
:--:| :--: | :--:
** $p=0$ ** | 101 | 54
** $p=1$ ** | 33 | 105
.pt3[

$$\begin{split}
\text{Accuracy} &= 0.70 \\
\\
\text{Sensitivity} &= 0.66 \\
\text{Specificity} &= 0.75 \\
\text{Balanced Accuracy} &= 0.71 \\
\\
\text{Precision} &= 0.76 \\
\text{Recall} &= 0.66 \\
F_1 &= 0.71
\end{split}$$

]
]

--

.pull-right.tc[
**Imbalanced Classes**

 | $y=0$ | $y=1$
:--:| :--: | :--:
** $p=0$ ** | 256 | 11
** $p=1$ ** | 6 | 2
.pt3[

$$\begin{split}
\text{Accuracy} &= 0.94 \\ 
\\
\text{Sensitivity} &= 0.15 \\
\text{Specificity} &= 0.98 \\
\text{Balanced Accuracy} &= 0.57 \\
\\
\text{Precision} &= 0.25 \\
\text{Recall} &= 0.15 \\
F_1 &= 0.19
\end{split}$$
]
]

---
class: onecol
## Multiclass Performance Strategies

 | &emsp; $y$ = Healthy&emsp; | &emsp; $y$ = Depression&emsp; | &emsp; $y$ = Mania&emsp;
:--| :--: | :--: | :--:
** $p$ = Healthy**&emsp; | 100 | 3 | 7
** $p$ = Depression**&emsp; | 2 | 25 | 0 
** $p$ = Mania**&emsp; | 10 | 1 | 10

--

.pt3[
![:emphasize](Macro-averaging): compute the performance for each class in a one-versus-all manner using standard binary metrics, then calculate the unweighted average across classes
]

--

![:emphasize](Macro-weighting): same as macro-averaging but use a weighted average across classes such that performance on larger/more frequent classes is emphasized

--

![:emphasize](Micro-averaging): compute a confusion matrix for each class in a one-versus-all manner, then calculate binary metrics using sums of each cell (e.g., $\sum {TP}_j$)

---
class: onecol
## Class Probability Metrics for Classification

Some classifiers estimate **the probability of each class** as their prediction $(p_{ij})$

If we consider a higher estimated probability as higher "confidence"

- We can **reward** the classifier for being more **confident when correct**...

- ...and ![:emphasize](penalize) the classifier for being more ![:emphasize](confident when wrong)

--

This gives rise to the Logistic or **Log Loss**, which can be summed or averaged

$$L_{log}(Y,P) = -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^q \left(y_{ij}\log( p_{ij})\right)$$

.pl4[
$y_{ij}\in\{0,1\}$ is a binary indicator of whether observation $i$ is truly in class $j$<br />
$p_{ij}\in(0,1)$ is the estimated probability that observation $i$ is in class $j$
]

---

## Visualizing Log Loss in Binary Classification

```{r logloss, echo=FALSE}
tibble(
  truth = factor(rep(0:1, each = 501)),
  p = c(seq(0, 1, length.out = 501), seq(1, 0, length.out = 501))
) %>% 
  rowwise() %>% 
  mutate(loss = yardstick::mn_log_loss_vec(truth, p, event_level = "second")) %>% 
  ggplot(aes(x = p, y = loss, color = truth, linetype = truth)) + 
  geom_line(size = 2.25) +
  coord_cartesian(ylim = c(0, 8)) +
  scale_color_brewer(palette = "Set2", guide = guide_legend(keywidth = unit(2, "cm"))) +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "Predicted Probability of Class 1", y = "Log Loss", 
       color = "True Class", linetype = "True Class") +
  theme_xaringan(text_font_size = 18, title_font_size = 16) +
  theme(
    axis.title = element_text(size = 18),
    legend.position = "top",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Performance Curves

- When a classifier outputs class probabilities, we can choose any **decision threshold**

- We might naturally consider any probability over 50% positive and all others negative

- But we could choose a threshold more conservative (e.g., 75%) or liberal (e.g., 25%)

--

.pt1[
- **Performance curves** plot the characteristics of different decision thresholds

- This gives us an overview of how the classification system performs in general
]

--

.pt1[
- There are many performance curves<sup>1</sup>, so we'll use the original as an example

- Finally, the **area under the curve (AUC)** is often used as a performance metric
]

.footnote[[1] Popular options include ROC curves, precision-recall curves, gain curves, and lift curves.]

---
class: onecol
## Receiver Operating Characteristic (ROC) Curves

.pull-left[
Each point in a ROC curve corresponds to a possible decision threshold

The performance metrics compared for each point are Sensitivity and Specificity

Better curves are closer to the top-left

The area under the ROC curve (AUC-ROC) ranges from $0.5$ to $1.0$, higher is better.

AUCROC is the probability that a random positive example has a higher estimate than a random negative example.

]

--

.pull-right[
```{r rocex, echo=FALSE, fig.width=7, fig.height=6.8, out.width='95%'}
test_data <- read_rds("preds.rds")
yardstick::roc_curve(test_data, yes, truth = obs, event_level = "second") %>% 
  arrange(sensitivity) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dotted", size = 2, color = "grey") +
  geom_line(size = 2, color = "darkblue") +
  annotate(geom = "text", x = 0.75, y = 0.125, size = 12, label = "AUC = 0.82", 
           color = "darkblue") +
  coord_fixed() +
  labs(x = "1 - Specificity", y = "Sensitivity") +
  theme_xaringan(text_font_size = 20) +
  theme(
    axis.title = element_text(size = 26),
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```
]

---
class: twocol
## Comprehension Check \#1

<span style="font-size:30px;">Bindi trains Model [A] to predict how many kilometers each bird will migrate this year and Model [B] to predict whether or not it will reproduce this year.</span>

.pull-left[
**1. Which combination of performance metrics would be appropriate to use?**

a) ROC-Curve for [A] and CCC for [B]

b) Precision for [A] and Recall for [B]

c) MAE for [A] and Balanced Accuracy for [B]

d) None of the above

]

.pull-right[
**2. Which combination of performance scores should Bindi hope to see?**

a) RMSE = 531.6 and AUC-ROC = 0.04

b) RMSE = 1129.7 and AUC-ROC = 0.04

c) RMSE = 531.6 and AUC-ROC = 0.88

d) RMSE = 1129.7 and AUC-ROC = 0.88
]

---
class: inverse, center, middle
# Training and Cross-Validation
---
class: onecol
## `caret::train()`

{caret} standardizes the syntax to train over 200 different ML algorithms

It also plays nicely with the {recipes} package we learned for preprocessing

--

<p style="padding-top:20px;">The train() function will handle model <b>training</b>, <b>resampling</b>, and <b>tuning</b></p>

Because LM and GLM have no hyperparameters, we don't need tuning (yet!)

--

<p style="padding-top:20px;">Today, we will <b>focus on training</b> and just use resampling to estimate performance</p>

We will point to where tuning would be configured but leave that for tomorrow

---
class: onecol
## Main Arguments to `train()`

Argument&emsp; | Description
:------- | :----------
`x` | A {recipe} object with variable roles and preprocessing steps
`data` | A data frame to be used for training (prior to baking)
`method` | A string indicating the algorithm to train (e.g., "lm" or "glm")&emsp;

--

To train any of the 200+ supported algorithms, you just need to change `method`

This makes it *super easy* to implement new algorithms and explore the world of ML!

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Be sure you understand an algorithm before trying to publish a paper using it.
]

---
class: onecol
## Additional Arguments to `train()`

Argument | Description
:------- | :----------
`trControl` | Controls the resampling procedure used during tuning
`metric` | Controls which metric to optimize during tuning
`tuneGrid` | Control which specific tuning values to compare
`tuneLength`&emsp; | Control how many tuning values to automatically compare&emsp;

These arguments are largely used to configure tuning (discussed tomorrow)

The training set will be resampled and different tuning values will be compared

The "best" tuning values will be used to train a **final model** using all the training data

---
class: onecol
## Resampling options

The `trControl` argument can be configured by `trainControl()`:

Argument&emsp; | Description
:------- | :----------
`method` | Controls the type of resampling (e.g., "cv", "repeatedcv", "boot")&emsp;
`number` | Controls the number of folds in cv and iterations in boot
`repeats` | Controls the number of repetitions in repeatedcv

"cv" will perform resampling through $k$-fold cross-validation

"repeatedcv" will repeat $k$-fold cross-validation multiple times

"boot" will perform resampling through bootstrapping

---
class: onecol
## Pseudo-code for training with resampling

```{r, eval=FALSE}
# Configure resampling options
resampling_options <- trainControl( #<<
  method = "repeatedcv", #<<
  number = 10, #<<
  repeats = 3 #<<
) #<<

# Train model from recipe
trained_model <- train(
  x = my_recipe,
  data = my_training,
  method = "lm",
  trControl = resampling_options #<<
)
```

---
class: onecol
## Advanced Resampling Options

There are many other arguments to `trainControl()` to explore

- **`selectionFunction`** can be used to prioritize less complex models*
- **`predictionBounds`** can be used to constrain predicted values in regression
- **`sampling`** can be used to address imbalanced labels in classification*
- **`seeds`** can be used to make the resampling procedure reproducible

.footnote[*These are both advanced topics that we can revisit on Day 5.]

--

There are also alternative resampling methods to explore
- Fancier versions of bootstrapping (e.g., "boot632")
- Algorithm-specific methods (e.g., "oob")
- Adaptive methods that tune faster/smarter (e.g., "adaptive_cv")
- The {rsample} package from {tidymodels} provides even more options

---
## Comprehension Check \#2

.pull-left[
```{r, eval=FALSE}
# Part 1
my_recipe <- 
  my_data %>% 
  recipe(outcome ~ .) %>% 
  step_center(all_numeric()) %>% 
  step_zv(all_predictors()) %>% 
  prep(training = my_training)

# Part 2
trained_model <- train(
  x = my_recipe,
  data = my_training_set,
  method = "cv"
)
```

]

.pull-right[
**1. What was the main mistake made in Part 1?**

&emsp; a) The predictors should be listed in `recipe()`

&emsp; b) Numeric predictors cannot be centered

&emsp; c) `step_zv()` only works for numeric predictors

&emsp; d) The recipe should not be prepped yet

**2. What was the main mistake made in Part 2?**

&emsp; a) `data` should be `my_testing_set`

&emsp; b) `x` should be `my_data`

&emsp; c) "cv" is not a method for `train()`

&emsp; d) Forgot to add the `number` argument
]

---
class: onecol
# Live Coding: Part 1
Let's put what we just learned into practice in R

.pt1[
We will use {caret} and {recipes} to train two models on the `titanic` data
]

- We will load in and split the data

- We will create two recipes for feature engineering

- We will use LM to try to predict each passenger's fare (how much they paid)

.pt1[
You can follow along on your computer (the code is online) or just watch me code
]

---
class: onecol
## Live Coding Summary

```{r}
# Read in data
titanic <- read.csv("titanic.csv")

# Create a training and testing set (stratified by fare)
set.seed(2021)
fare_index <- createDataPartition(titanic$fare, p = 0.75, list = FALSE)
fare_train <- titanic[fare_index, ]
fare_test <- titanic[-fare_index, ]
```

```{r}
# Check sizes
dim(fare_train)
dim(fare_test)
```

---
class: onecol
## Live Coding Summary

```{r}
# Create a preprocessing recipe (no prep)
fare_recipe <- 
  titanic %>% 
  recipe(fare ~ .) %>% 
  step_rm(survived) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors())
```

---
class: onecol

## Live Coding Summary

```{r}
set.seed(2021)

# Configure resampling
fare_tc <- trainControl(method = "cv", number = 10)

# Train the model using the recipe, data, and method
fare_lm <- train(
  fare_recipe, 
  data = fare_train,
  method = "lm",
  trControl = fare_tc
)
```

---
class: inverse, center, middle
# Model Evaluation
---
class: onecol
## Training Set Performance

The object created by `train()` will contain lots of information

We can view a summary of training set performance in the **`results`** field
- Each row corresponds to one combination or set of hyperparameters
- Columns define the **hyperparameter values** and the set's **performance scores** 
- Results will include the Mean and SD of each metric across resamples

--

```{r, eval=FALSE}
fare_lm$results
```

```{r, echo=FALSE}
kable(fare_lm$results, digits = 2)
```

.footnote[[1] Because there are no real hyperparameters for LM, there is only one result row.]

---
class: onecol
## Test Set Predictions
To evaluate performance on the test set, we need two things:

1. Predictions from the model on the test set (e.g., values, classes, or probabilities)

2. Trusted labels on the test set (after any preprocessing steps from {recipes})

--

.pt1[
We can compare these predicted and trusted labels using {caret} or {yardstick}
]
- {caret} has basic performance metrics built in and is straightforward to use

- {yardstick} offers many more options but requires some coding to work with {caret}

---
class: onecol
## Test Set Predictions

To get predictions from the final model on new data<sup>1</sup>, we can use `predict()`

--

Argument&emsp; | Description
:------- | :----------
`object` | A trained model object created by `train()`
`newdata` | A data frame with the same features (prior to baking)&emsp;
`type` | Return raw classes ("raw") or probabilities ("prob")?

.pt3[
```{r}
fare_pred <- predict(fare_lm, newdata = fare_test)
glimpse(fare_pred)
```
]

.footnote[[1] The same process is used for both evaluating performance and deploying the model in the real world.]

---
class: onecol
## Test Set Labels 
To get labels from the final model in the format expected by R, we can use `bake()`

```{r}
fare_test_baked <- 
  fare_recipe %>% 
  prep(training = fare_train) %>% 
  bake(new_data = fare_test)

fare_labs <- fare_test_baked$fare
glimpse(fare_labs)
```

--

A few points of potential confusion to avoid:

1. Use `train()` and `predict()` before baking the recipe (i.e., let them do it)
2. `caret::predict()` uses `newdata`, whereas `recipes::bake()` uses `new_data`

---
class: onecol
## Visualizing Predicted and Trusted Labels

```{r, eval=FALSE}
# Simple Version
qplot(x = fare_labs, y = fare_pred)
```

```{r quick, echo=FALSE, fig.height=3.25}
# Quick Version
qplot(x = fare_labs, y = fare_pred) +
  theme_xaringan(text_font_size = 20) +
  theme(
    axis.title = element_text(size = 20),
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
## Visualizing Predicted and Trusted Labels
```{r, eval=FALSE}
# Fancy Version
fare_test_baked %>% 
  mutate(fare_pred = fare_pred) %>% 
  ggplot(aes(x = fare, y = fare_pred)) + 
  geom_abline(slope = 1, intercept = 0, color = "grey", size = 2) +
  geom_point(color = "darkred", size = 2, alpha = 1/3) +
  coord_cartesian(xlim = c(0, 520), ylim = c(0, 520)) +
  labs(x = "True Fare", y = "Predicted Fare")
```

---
## Visualizing Predicted and Trusted Labels
```{r fancy, echo=FALSE, fig.height=4}
# Fancy Version
fare_test_baked %>% 
  mutate(fare_pred = fare_pred) %>% 
  ggplot(aes(x = fare, y = fare_pred)) + 
  geom_abline(slope = 1, intercept = 0, color = "grey", size = 2) +
  geom_point(color = "darkblue", size = 3, alpha = 1/3) +
  coord_cartesian(xlim = c(0, 520), ylim = c(0, 520)) +
  labs(x = "True Fare", y = "Predicted Fare") +
  theme_xaringan(text_font_size = 20) +
  theme(
    axis.title = element_text(size = 20),
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Estimating Test Set Performance

To estimate performance in {caret}, use `postResample()`

Argument&emsp; | Description
:------- | :----------
`pred` | A vector of predicted labels&emsp;
`obs` | A vector of trusted labels

```{r}
postResample(pred = fare_pred, obs = fare_labs)
```

This function will give basic results (e.g., Accuracy) for classification too

---
class: onecol
## More Metrics from {yardstick}

The {yardstick} package provides lots of performance metrics

For regression, I like the **Huber Loss** distance metric and **CCC** correlation metric

```{r}
# Huber Loss (blends RMSE and MAE)
yardstick::huber_loss_vec(truth = fare_labs, estimate = fare_pred)
```

```{r}
# Concordance Correlation Coefficient
yardstick::ccc_vec(truth = fare_labs, estimate = fare_pred)
```

For classification, I like **MCC**, **Log Loss**, and **ROC Curve** (shown in next Live Coding)

---
class: inverse, center, middle
# Model Interpretation
---
class: onecol
## Variable Importance

Predictive accuracy is emphasized in ML over interpretability

- The main goal of most applied ML studies is to quantify performance

--

However, some algorithms can provide insight into their decision-making

- As a model usually used for inference, LM has strong interpretability

- We can examine the model coefficients (intercept and slopes)

--

```{r, eval=FALSE}
fare_lm$finalModel %>% coefficients()
```

```{r, echo=FALSE}
fare_lm$finalModel %>% coefficients() %>% t() %>% kable(digits = 2)
```

---
class: onecol
## Variable Importance

However, these coefficients have different units and levels of uncertainty

Instead, for LM, we use the absolute value of each coefficient's $t$-statistic

```{r, out.width='80%'}
varImp(fare_lm, scale = FALSE)
```

.footnote[[1] Other algorithms have different ways to estimate variable importance, but `varImp()` will take care of it.<br />[2] Some algorithms have no easy way to estimate variable importance and are considered true "black boxes."]

---
class: onecol
## Live Coding Part 2

Now that we have explored the prediction of passenger fare, let's predict survival

Training, evaluating, and interpreting a ![:emphasize](classification) model is very similar but...

- We will be using GLM (logistic regression) instead of LM as our **new algorithm**

- We will have some **different performance metrics** to calculate and interpret

- We can explore the **raw class predictions** and **estimated class probabilities**

---
class: onecol
## Live Coding Summary

```{r}
# Create a training and testing set (stratified by survived)
set.seed(2021)
surv_index <- createDataPartition(titanic$survived, p = 0.75, list = FALSE)
surv_train <- titanic[surv_index, ]
surv_test <- titanic[-surv_index, ]
```

```{r}
# Create a preprocessing recipe (no prep)
surv_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~sex_male:starts_with("pclass")) %>% 
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors())
```

---
class: onecol
## Live Coding Summary

```{r}
set.seed(2021)
#  Train the model using the recipe, data, and method
surv_glm <- train(
  surv_recipe, 
  data = surv_train,
  method = "glm",
  metric = "Accuracy",
  trControl = trainControl(method = "cv", number = 10)
)
surv_glm$results
```

---
## Live Coding Summary

```{r}
# Extract model predictions as classes
surv_pred <- predict(surv_glm, newdata = surv_test)
glimpse(surv_pred)
```

```{r}
# Extract model predictions as probabilities
surv_prob <- predict(surv_glm, newdata = surv_test, type = "prob")
glimpse(surv_prob)
```

---
## Live Coding Summary

```{r}
# Bake test set
surv_test_baked <- 
  surv_recipe %>% 
  prep(training = surv_train) %>% 
  bake(new_data = surv_test)

surv_labs <- surv_test_baked$survived
glimpse(surv_labs)
```

```{r}
postResample(pred = surv_pred, obs = surv_labs)
```

---
## Live Coding Summary

```{r, eval=FALSE}
test_data <- bind_cols(
  surv_prob, 
  obs = surv_labs, 
  pred = surv_pred
)

library(yardstick)
cm <- conf_mat(test_data, truth = obs, estimate = pred)
autoplot(cm, type = "heatmap")
summary(cm)

mcc(test_data, truth = obs, estimate = pred)

roc <- roc_curve(test_data, yes, truth = obs, event_level = "second")
autoplot(roc)
roc_auc(test_data, yes, truth = obs, event_level = "second")
# Results too large to show here
```

---
## Live Coding Summary

```{r glm_varimp}
varImp(surv_glm, scale = FALSE) %>% plot()
```

