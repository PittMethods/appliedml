---
title: '<span style="font-size:48pt;">Basic Predictive Modeling</span>'
subtitle: 'üßÆ üîÆ üë®‚Äçüíª' 
author: 'Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series'
date: 'Lecture 2-B &emsp; &emsp; July 20, 2021'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  footnote_font_size = "20px",
  footnote_color = "gray",
  text_slide_number_font_size = "18px"
)
```

```{r xaringan-tachyons, echo=FALSE}
xaringanExtra::use_tachyons()
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(caret)
library(recipes)
```

class: inverse, center, middle
# Overview

<style type="text/css">
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
</style>
---
class: onecol
## Plan for Day 2-B
First will be an extended lecture on **Performance Metrics** (one of my favorite topics)

Second will be a brief **lecture overview** of training, evaluation, and interpretation in R

Third, we will apply this with an extended **live coding** session and hands-on activity

--

.pt1[
In the live coding, we will **adapt familiar algorithms** to predictive modeling
- Linear modeling will be adapted for continuous prediction (regression)
- Generalized linear modeling will be adapted for discrete prediction (classification)

*This will ease the transition to ML and highlight its similarities with classical stats*
]

--

.pt1[
We will also **foreshadow future topics** (e.g., regularized linear models and tuning)
]

---
class: inverse, center, middle
# Performance Metrics
---
class: onecol
## Performance Metrics

.left-column.pt3[
```{r target, echo=FALSE}
include_graphics("target.jpg")
```
]

.right-column[

**Metrics for Supervised Regression**
- ![:emphasize](Distance) between predicted and trusted values
- ![:emphasize](Correlation) between predicted and trusted values

.pt1[
**Metrics for Supervised Classification**
]
- ![:emphasize](Confusion matrix) between predicted and trusted classes
- Compare predicted ![:emphasize](class probabilities) to trusted classes
- Construct a ![:emphasize](performance curve) across decision thresholds
]

---
class: onecol
## Classic Distance Metrics for Regression

.pull-left[
**Root Mean Squared Error (RMSE)**
- Based on squared loss
- Easier to computationally optimize
- Ranges from $0$ to $+\infty$, lower is better
]

.pull-right.pt4[
$$RMSE=\sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2}$$
]

--

.pull-left[
**Mean Absolute Error (MAE)**
- Based on absolute loss
- More robust to outliers
- Ranges from $0$ to $+\infty$, lower is better
]

.pull-right.pt4[
$$MAE=\frac{1}{n} \sum_{i=1}^n \left\lvert y_i - p_i \right\rvert$$
]

.footnote[[1] Note that, here, we will refer to the trusted labels as `\\(y\\)` and the predicted labels as `\\(p\\)`.]

---
exclude: true
class: onecol
## Advanced Distance Metrics for Regression
**Huber loss (HL)**

- Combines benefits of RMSE and MAE
 + Easy to computationally optimize AND more robust to outliers
- Requires setting or tuning $\delta$ (controls what to consider an outlier)
- Ranges from $0$ (best) to $+\infty$ (worst), lower is better

$$\begin{split}
HL &= \frac{1}{n} \sum_{i=1}^n L_\delta(y_i, p_i) \\
L_\delta(y_i, p_i) &= \begin{cases}\frac{1}{2}(y_i - p_i)^2 & \text{for } \lvert y_i - p_i \rvert \le \delta \\ \delta (\lvert y_i - p_i \rvert - \frac{1}{2}\delta) & \text{otherwise} \end{cases}
\end{split}$$

---
## Visualizing Regression Loss Functions

```{r losses, echo=FALSE}
tibble(
  error = seq(-2, 2, length.out = 100), 
  sq_err = error^2,
  abs_err = abs(error)
) %>% 
  pivot_longer(-error, names_to = "type", values_to = "loss") %>% 
  mutate(type = factor(
    type, 
    levels = c("sq_err", "abs_err"), 
    labels = c("Squared Loss (RMSE)", "Absolute Loss (MAE)")
  )) %>% 
  ggplot(aes(x = error, y = loss, color = type)) + 
  facet_wrap(~type, nrow = 1) + 
  geom_line(size = 1.5) + 
  labs(x = latex2exp::TeX("Error $(y_i - p_i)$"), y = "Loss") +
  theme_xaringan(text_font_size = 18, title_font_size = 18) +
  theme(
    axis.title = element_text(size = 20),
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
exclude: true
## Comparing Loss Functions

```{r losses3, echo=FALSE}
tibble(
  error = seq(-2, 2, length.out = 100), 
  sq_err = error^2,
  abs_err = abs(error),
  huber1 = ifelse(abs(error) <= 1, 0.5 * error^2, 1 * (abs(error) - 0.5 * 1))
) %>% 
  pivot_longer(-error, names_to = "type", values_to = "loss") %>% 
  mutate(type = factor(
    type, 
    levels = c("sq_err", "abs_err", "huber1"), 
    labels = c("Squared Loss", "Absolute Loss", "Huber Loss (delta=1)")
  )) %>% 
  ggplot(aes(x = error, y = loss, color = type)) + 
  facet_wrap(~type, nrow = 1) + 
  geom_line(size = 1.5) + 
  labs(x = "Error (y - p)", y = "Loss") +
  theme_xaringan(text_font_size = 18, title_font_size = 18) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Correlation Metrics for Regression

**R-Squared $(R^2$ or RSQ)**
- Calculated in ML as the **squared correlation** between the predictions and labels
- Ranges from $0$ to $1$, higher is better

$$R^2 = \left(\frac{\text{cov}(y, p)}{\sigma_y\sigma_p}\right)^2 = \left(\frac{\sum(y_i - \bar{y})(p_i - \bar{p})}{\sqrt{\sum (y_i-\bar{y})^2}\sqrt{\sum(p_i-\bar{p})^2}}\right)^2$$

--

.bg-light-yellow.b--light-red.ba.bw1.br3.pl4[
**Caution!**
+ RSQ is a measure of *consistency* (i.e., linear association) and not distance
+ RSQ can become unstable or undefined when data variability is low
+ RSQ can become unstable when applied in small samples (e.g., test sets)

]

---
class: onecol
## Advanced Correlation Metrics for Regression

**Concordance Correlation Coefficient (CCC)**

- Combines both accuracy (distance) and consistency (correlation) information

- Very similar to certain formulations of the intraclass correlation coefficient

- Ranges from $-1$ to $+1$, where higher is better

.pt1[
$$CCC = \frac{2\rho_{yp}\sigma_y\sigma_p}{\sigma_y^2 + \sigma_p^2 + (\mu_y - \mu_p)^2}
= \frac{\frac{2}{n}\sum (y_i - \bar{y})(p_i - \bar{p})}{\frac{1}{n}\sum(y_i - \bar{y})^2 + \frac{1}{n}\sum(p_i - \bar{p})^2 + (\bar{y} - \bar{p})^2}$$
]
---
class: onecol
## Comparing Regression Performance Metrics

```{r dvc, echo=FALSE}
set.seed(2021)
dvc <- tibble(
  label = rnorm(100, 0, 10),
  pred1 = label + rnorm(100, 0, 5),
  pred2 = 5 * label + rnorm(100, 0, 28)
)

title1 <- paste0(
  "RMSE = ", round(yardstick::rmse_vec(dvc$label, dvc$pred1), 1),
  ", MAE = ", round(yardstick::mae_vec(dvc$label, dvc$pred1), 1),
  "\nRSQ = ", round(yardstick::rsq_vec(dvc$label, dvc$pred1), 2),
  ", CCC = ", round(yardstick::ccc_vec(dvc$label, dvc$pred1), 2)
)

p1 <- ggplot(dvc, aes(x = label, y = pred1)) + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "grey") + 
  geom_point(color = "#1b9e77", size = 2) +
  labs(x = "Trusted Label", y = "Model 1's Predictions", title = title1) +
  theme_xaringan(text_font_size = 20, title_font_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.title.position = "panel",
    legend.position = "none",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )

title2 <- paste0(
  "RMSE = ", round(yardstick::rmse_vec(dvc$label, dvc$pred2), 1),
  ", MAE = ", round(yardstick::mae_vec(dvc$label, dvc$pred2), 1),
  "\nRSQ = ", round(yardstick::rsq_vec(dvc$label, dvc$pred2), 2),
  ", CCC = ", round(yardstick::ccc_vec(dvc$label, dvc$pred2), 2)
)

p2 <- ggplot(dvc, aes(x = label, y = pred2)) + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "grey") + 
  geom_point(color = "#d95f02", size = 2) +
  labs(x = "Trusted Label", y = "Model 2's Predictions", title = title2) +
  theme_xaringan(text_font_size = 20, title_font_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5),
    plot.title.position = "panel",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )

p1 | p2
```

---
class: twocol
## Confusion Matrix Metrics for Classification

 | Trusted = No<br /> $(y=0)$ | Trusted = Yes<br /> $(y=1)$
:--:| :--: | :--:
**Predicted = No**&emsp;<br /> $(p=0)$ | True Negatives (TN) | False Negative (FN)
**Predicted = Yes**&emsp;<br /> $(p=1)$ | False Positive (FP) | True Positive (TP)

.mt4[
$$\text{Accuracy} = \frac{TN + TP}{(TN + FN + FP + TP)}$$
]
.tc[
Ranges from $0$ to $1$, higher is better
]

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Accuracy can be misleading when the classes are highly imbalanced (e.g., if $TN \gg TP$)
]

---
## Additional Confusion Matrix Metrics

 | $y=0$ | $y=1$
:--:| :--: | :--:
&emsp; ** $p=0$**&emsp; | &emsp;True Negatives (TN)&emsp; | &emsp;False Negative (FN)&emsp;
** $p=1$** | False Positive (FP) | True Positive (TP)

.pull-left.pt3[
$$\begin{split}
\text{Sensitivity} &= \frac{TP}{TP+FN} \\
\text{Specificity} &= \frac{TN}{TN + FP} \\
\text{Balanced Accuracy} &= \frac{\text{Sensitivity} + \text{Specificity}}{2} \\
\end{split}$$
]

--

.pull-right.pt3[
$$\begin{split}
\text{Precision} &= \frac{TP}{TP+FP} \\
\text{Recall} &= \frac{TP}{TP+FN} \\
F_1 \text{ Score} &= \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
\end{split}$$
]

.tc[
All range from $0$ to $1$, higher is better
]

--

.footnote[[1] Note that the formulas for Sensitivity and Recall are the same. Many of these metrics enjoy many names.<br />[2] `\\(F_1\\)` does not consider `\\(TN\\)`, which is sometimes a reason to use it and sometimes a reason to avoid it.]

---
## Confusion Matrix Metrics Examples
.pull-left.tc[
**Balanced Classes**

 | $y=0$ | $y=1$
:--:| :--: | :--:
** $p=0$ ** | 101 | 54
** $p=1$ ** | 33 | 105
.pt3[

$$\begin{split}
\text{Accuracy} &= 0.70 \\
\\
\text{Sensitivity} &= 0.66 \\
\text{Specificity} &= 0.75 \\
\text{Balanced Accuracy} &= 0.71 \\
\\
\text{Precision} &= 0.76 \\
\text{Recall} &= 0.66 \\
F_1 &= 0.71
\end{split}$$

]
]

--

.pull-right.tc[
**Imbalanced Classes**

 | $y=0$ | $y=1$
:--:| :--: | :--:
** $p=0$ ** | 256 | 11
** $p=1$ ** | 6 | 2
.pt3[

$$\begin{split}
\text{Accuracy} &= 0.94 \\ 
\\
\text{Sensitivity} &= 0.15 \\
\text{Specificity} &= 0.98 \\
\text{Balanced Accuracy} &= 0.57 \\
\\
\text{Precision} &= 0.25 \\
\text{Recall} &= 0.15 \\
F_1 &= 0.19
\end{split}$$
]
]

---
class: onecol
## Multiclass Performance Strategies

 | &emsp; $y$ = Healthy&emsp; | &emsp; $y$ = Depression&emsp; | &emsp; $y$ = Mania&emsp;
:--| :--: | :--: | :--:
** $p$ = Healthy**&emsp; | 100 | 3 | 7
** $p$ = Depression**&emsp; | 2 | 25 | 0 
** $p$ = Mania**&emsp; | 10 | 1 | 10

--

.pt3[
![:emphasize](Macro-averaging): compute the performance for each class in a one-versus-all manner using standard binary metrics, then calculate the unweighted average across classes
]

--

![:emphasize](Macro-weighting): same as macro-averaging but use a weighted average across classes such that performance on larger/more frequent classes is emphasized

--

![:emphasize](Micro-averaging): compute a confusion matrix for each class in a one-versus-all manner, then calculate binary metrics using sums of each cell (e.g., $\sum {TP}_j$)

---
class: onecol
## Class Probability Metrics for Classification

Some classifiers estimate **the probability of each class** as their prediction $(p_{ij})$

If we consider a higher estimated probability as higher "confidence"

- We can **reward** the classifier for being more **confident when correct**...

- ...and ![:emphasize](penalize) the classifier for being more ![:emphasize](confident when wrong)

--

This gives rise to the Logistic or **Log Loss**, which can be summed or averaged

$$L_{log}(Y,P) = -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^q \left(y_{ij}\log( p_{ij})\right)$$

.pl4[
$y_{ij}\in\{0,1\}$ is a binary indicator of whether observation $i$ is truly in class $j$<br />
$p_{ij}\in(0,1)$ is the estimated probability that observation $i$ is in class $j$
]

---

## Visualizing Log Loss in Binary Classification

```{r logloss, echo=FALSE}
tibble(
  truth = factor(rep(0:1, each = 501)),
  p = c(seq(0, 1, length.out = 501), seq(1, 0, length.out = 501))
) %>% 
  rowwise() %>% 
  mutate(loss = yardstick::mn_log_loss_vec(truth, p, event_level = "second")) %>% 
  ggplot(aes(x = p, y = loss, color = truth, linetype = truth)) + 
  geom_line(size = 2.25) +
  coord_cartesian(ylim = c(0, 8)) +
  scale_color_brewer(palette = "Set2", guide = guide_legend(keywidth = unit(2, "cm"))) +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "Predicted Probability of Class 1", y = "Log Loss", 
       color = "True Class", linetype = "True Class") +
  theme_xaringan(text_font_size = 18, title_font_size = 16) +
  theme(
    axis.title = element_text(size = 18),
    legend.position = "top",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Performance Curves

- When a classifier outputs class probabilities, we can choose any **decision threshold**

- We might naturally consider any probability over 50% positive and all others negative

- But we could choose a threshold more conservative (e.g., 75%) or liberal (e.g., 25%)

--

.pt1[
- **Performance curves** plot the characteristics of different decision thresholds

- This gives us an overview of how the classification system performs in general
]

--

.pt1[
- There are many performance curves<sup>1</sup>, so we'll use the original as an example

- Finally, the **area under the curve (AUC)** is often used as a performance metric
]

.footnote[[1] Popular options include ROC curves, precision-recall curves, gain curves, and lift curves.]

---
class: onecol
## Receiver Operating Characteristic (ROC) Curves

.pull-left[
Each point in a ROC curve corresponds to a possible decision threshold

The performance metrics compared for each point are Sensitivity and Specificity

Better curves are closer to the top-left

The area under the ROC curve (AUC-ROC) ranges from $0.5$ to $1.0$, higher is better.

AUCROC is the probability that a random positive example has a higher estimate than a random negative example.

]

--

.pull-right[
```{r rocex, echo=FALSE, fig.width=7, fig.height=6.8, out.width='95%'}
test_data <- read_rds("preds.rds")
yardstick::roc_curve(test_data, yes, truth = obs, event_level = "second") %>% 
  arrange(sensitivity) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dotted", size = 2, color = "grey") +
  geom_line(size = 2, color = "darkblue") +
  annotate(geom = "text", x = 0.75, y = 0.125, size = 12, label = "AUC = 0.82", 
           color = "darkblue") +
  coord_fixed() +
  labs(x = "1 - Specificity", y = "Sensitivity") +
  theme_xaringan(text_font_size = 20) +
  theme(
    axis.title = element_text(size = 26),
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```
]

---
class: onecol
## Comprehension Check \#1



---
class: inverse, center, middle
# Training and Cross-Validation
---
class: onecol
## `caret::train()`

{caret} standardizes the syntax to train over 200 different ML algorithms

It also plays nicely with the {recipes} package we learned for preprocessing

--

<p style="padding-top:20px;">The train() function will handle model <b>training</b>, <b>resampling</b>, and <b>tuning</b></p>

Because LM and GLM have no hyperparameters, we don't need tuning (yet!)

--

<p style="padding-top:20px;">Today, we will <b>focus on training</b> and just use resampling to estimate performance</p>

We will point to where tuning would be configured but leave that for tomorrow

---
class: onecol
## Main Arguments to `train()`

Argument&emsp; | Description
:------- | :----------
`x` | A {recipe} object with variable roles and preprocessing steps
`data` | A data frame to be used for training (prior to baking)
`method` | A string indicating the algorithm to train (e.g., "lm" or "glm")&emsp;

--

To train any of the 200+ supported algorithms, you just need to change `method`<sup>1</sup>

This makes it *super easy* to implement new algorithms and explore the world of ML!

But be sure you understand an algorithm before trying to publish a paper using it

.footnote[[1] Of course, you may also need to adapt the features and tuning procedures to match the new algorithm.]

---
class: onecol
## Pseudocode for simple training

```{r, eval=FALSE}
# Create data splits
set.seed(2021)
index <- createDataPartition(my_data$outcome, p = 0.80, list = FALSE)
my_training_set <- my_data[index, ]
my_testing_set <- my_data[-index, ]

# Set up recipe
my_recipe <- 
  my_data %>% 
  recipe(outcome ~ .) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors())

# Train model from recipe
trained_model <- train( #<<
  x = my_recipe, #<<
  data = my_training_set, #<<
  method = "lm" #<<
) #<<
```

---
class: onecol
## Additional Arguments to `train()`

Argument | Description
:------- | :----------
`trControl` | Controls the resampling procedure used during tuning
`metric` | Controls which metric to optimize during tuning
`tuneGrid` | Control which specific tuning values to compare
`tuneLength`&emsp; | Control how many tuning values to automatically compare&emsp;

These arguments are largely used to configure tuning (discussed tomorrow)

The training set will be resampled and different tuning values will be compared

The "best" tuning values will be used to train a **final model** using all the training data

---
class: onecol
## Resampling options

The `trControl` argument can be configured by `trainControl()`:

Argument&emsp; | Description
:------- | :----------
`method` | Controls the type of resampling (e.g., "cv", "repeatedcv", "boot")&emsp;
`number` | Controls the number of folds in cv and iterations in boot
`repeats` | Controls the number of repetitions in repeatedcv

"cv" will perform resampling through $k$-fold cross-validation

"repeatedcv" will repeat $k$-fold cross-validation multiple times

"boot" will perform resampling through bootstrapping

---
class: onecol
## Pseudocode for training with resampling

```{r, eval=FALSE}
# Configure resampling options
resampling_options <- trainControl( #<<
  method = "repeatedcv", #<<
  number = 10, #<<
  repeats = 3 #<<
) #<<

# Train model from recipe
trained_model <- train(
  x = my_recipe,
  data = my_training_set,
  method = "lm",
  trControl = resampling_options #<<
)
```

---
class: onecol
## Advanced Resampling Options

There are many other arguments to `trainControl()` to explore

- **`selectionFunction`** can be used to prioritize less complex models*
- **`predictionBounds`** can be used to constrain predicted values in regression
- **`sampling`** can be used to address imbalanced labels in classification*
- **`seeds`** can be used to make the resampling procedure reproducible

.footnote[*These are both advanced topics that we plan to revisit on Day 5-A.]

--

There are also alternative resampling methods to explore
- Fancier versions of bootstrapping (e.g., "boot632")
- Algorithm-specific methods (e.g., "oob")
- Adaptive methods that tune faster/smarter (e.g., "adaptive_cv")

---
## Comprehension Check \#2

.pull-left[
```{r, eval=FALSE}
# Assume packages and data are loaded

# Part 1
my_recipe <- 
  my_data %>% 
  recipe(outcome ~ .) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  prep(training = my_training_set)

# Part 2
trained_model <- train(
  x = my_recipe,
  data = my_training_set,
  method = "cv"
)
```

]

.pull-right[
**1. What was the main mistake made in Part 1?**

&emsp; a) The predictors should be listed in `recipe()`

&emsp; b) Numeric predictors cannot be centered

&emsp; c) `step_zv()` only works for numeric predictors

&emsp; d) The recipe should not be prepped yet

**2. What was the main mistake made in Part 2?**

&emsp; a) `data` should be `my_testing_set`

&emsp; b) `x` should be `my_data`

&emsp; c) "cv" is not a method for `train()`

&emsp; d) Forgot to add the `number` argument
]

---
class: inverse, center, middle
# Model Evaluation
---
class: onecol
## Training Set Performance

---
class: onecol
## Test Set Predictions

predict()
- object
- newdata 
- type
- na.action

---

postResample()
- pred
- obs

---

confusionMatrix()

---

yardstick
- ccc
- huber_loss
- mcc
- roc_auc
- metric_set

---
class: inverse, center, middle
# Model Interpretation
---
varImp()
- object
- scale

train$finalModel
---
