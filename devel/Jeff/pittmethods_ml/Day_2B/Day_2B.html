<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Basic Predictive Modeling</title>
    <meta charset="utf-8" />
    <meta name="author" content="Applied Machine Learning in R  Pittsburgh Summer Methodology Series" />
    <script src="Day_2B_files/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <span style="font-size:48pt;">Basic Predictive Modeling</span>
## üßÆ üîÆ üë®‚Äçüíª
### Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series
### Lecture 2-B ‚ÄÉ ‚ÄÉ July 20, 2021

---








class: inverse, center, middle
# Overview

&lt;style type="text/css"&gt;
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
&lt;/style&gt;
---
class: onecol
## Lecture Plan
&lt;p style="padding-top:15px;"&gt;We will begin with &lt;b&gt;overview slides&lt;/b&gt; and then dive into an &lt;b&gt;extended live coding&lt;/b&gt;&lt;/p&gt;
- Training and Cross-Validation
- Model Evaluation
- Model Interpretation

--

We will start by **adapting familiar algorithms** to the predictive modeling framework
- Linear modeling (multiple regression) will be used for *regression*
- Generalized linear modeling (logistic regression) will be used for *classification*

--

We will also **foreshadow future topics** (e.g., regularized regression and tuning)

---
class: inverse, center, middle
# Training and Cross-Validation
---
class: onecol
## `caret::train()`

{caret} standardizes the syntax to train over 200 different ML algorithms

It also plays nicely with the {recipes} package we learned for preprocessing

--

&lt;p style="padding-top:20px;"&gt;The train() function will handle model &lt;b&gt;training&lt;/b&gt;, &lt;b&gt;resampling&lt;/b&gt;, and &lt;b&gt;tuning&lt;/b&gt;&lt;/p&gt;

Because LM and GLM have no hyperparameters, we don't need tuning (yet!)

--

&lt;p style="padding-top:20px;"&gt;Today, we will &lt;b&gt;focus on training&lt;/b&gt; and just use resampling to estimate performance&lt;/p&gt;

We will point to where tuning would be configured but leave that for tomorrow

---
class: onecol
## Main Arguments to `train()`

Argument&amp;emsp; | Description
:------- | :----------
`x` | A {recipe} object with variable roles and preprocessing steps
`data` | A data frame to be used for training (prior to baking)
`method` | A string indicating the algorithm to train (e.g., "lm" or "glm")&amp;emsp;

--

To train any of the 200+ supported algorithms, you just need to change `method`&lt;sup&gt;1&lt;/sup&gt;

This makes it *super easy* to implement new algorithms and explore the world of ML!

But be sure you understand an algorithm before trying to publish a paper using it

.footnote[[1] Of course, you may also need to adapt the features and tuning procedures to match the new algorithm.]

---
class: onecol
## Pseudocode for simple training


```r
# Create data splits
set.seed(2021)
index &lt;- createDataPartition(my_data$outcome, p = 0.80, list = FALSE)
my_training_set &lt;- my_data[index, ]
my_testing_set &lt;- my_data[-index, ]

# Set up recipe
my_recipe &lt;- 
  my_data %&gt;% 
  recipe(outcome ~ .) %&gt;% 
  step_dummy(all_nominal_predictors()) %&gt;% 
  step_nzv(all_predictors()) %&gt;% 
  step_corr(all_predictors())

# Train model from recipe
*trained_model &lt;- train(
* x = my_recipe,
* data = my_training_set,
* method = "lm"
*) 
```

---
class: onecol
## Additional Arguments to `train()`

Argument | Description
:------- | :----------
`trControl` | Controls the resampling procedure used during tuning
`metric` | Controls which metric to optimize during tuning
`tuneGrid` | Control which specific tuning values to compare
`tuneLength`&amp;emsp; | Control how many tuning values to automatically compare&amp;emsp;

These arguments are largely used to configure tuning (discussed tomorrow)

The training set will be resampled and different tuning values will be compared

The "best" tuning values will be used to train a **final model** using all the training data

---
class: onecol
## Resampling options

The `trControl` argument can be configured by `trainControl()`:

Argument&amp;emsp; | Description
:------- | :----------
`method` | Controls the type of resampling (e.g., "cv", "repeatedcv", "boot")&amp;emsp;
`number` | Controls the number of folds in cv and iterations in boot
`repeats` | Controls the number of repetitions in repeatedcv

"cv" will perform resampling through `\(k\)`-fold cross-validation

"repeatedcv" will repeat `\(k\)`-fold cross-validation multiple times

"boot" will perform resampling through bootstrapping

---
class: onecol
## Pseudocode for training with resampling


```r
# Configure resampling options
*resampling_options &lt;- trainControl(
* method = "repeatedcv",
* number = 10,
* repeats = 3
*) 

# Train model from recipe
trained_model &lt;- train(
  x = my_recipe,
  data = my_training_set,
  method = "lm",
* trControl = resampling_options
)
```

---
class: onecol
## Advanced Resampling Options

There are many other arguments to `trainControl()` to explore

- **`selectionFunction`** can be used to prioritize less complex models*
- **`predictionBounds`** can be used to constrain predicted values in regression
- **`sampling`** can be used to address imbalanced labels in classification*
- **`seeds`** can be used to make the resampling procedure reproducible

.footnote[*These are both advanced topics that we plan to revisit on Day 5-A.]

--

There are also alternative resampling methods to explore
- Fancier versions of bootstrapping (e.g., "boot632")
- Algorithm-specific methods (e.g., "oob")
- Adaptive methods that tune faster/smarter (e.g., "adaptive_cv")

---
## Comprehension check

.pull-left[

```r
# Assume packages and data are loaded

# Part 1
my_recipe &lt;- 
  my_data %&gt;% 
  recipe(outcome ~ .) %&gt;% 
  step_center(all_numeric_predictors()) %&gt;% 
  step_zv(all_predictors()) %&gt;% 
  prep(training = my_training_set)

# Part 2
trained_model &lt;- train(
  x = my_recipe,
  data = my_training_set,
  method = "cv"
)
```

]

.pull-right[
**1. What was the main mistake made in Part 1?**

&amp;emsp; a) The predictors should be listed in `recipe()`

&amp;emsp; b) Numeric predictors cannot be centered

&amp;emsp; c) `step_zv()` only works for numeric predictors

&amp;emsp; d) The recipe should not be prepped yet

**2. What was the main mistake made in Part 2?**

&amp;emsp; a) `data` should be `my_testing_set`

&amp;emsp; b) `x` should be `my_data`

&amp;emsp; c) "cv" is not a method for `train()`

&amp;emsp; d) Forgot to add the `number` argument
]

---
class: inverse, center, middle
# Model Evaluation
---
class: onecol
## Performance Metrics
.left-column[
&lt;br /&gt;
&lt;img src="target.jpg" width="100%" /&gt;
]

.right-column[

**Metrics for Supervised Regression**
- ![:emphasize](Distance) between predicted and trusted values
- ![:emphasize](Correlation) between predicted and trusted values

**Metrics for Supervised Classification**
- ![:emphasize](Confusion matrix) between predicted and trusted classes
- Compare predicted ![:emphasize](class probabilities) to trusted classes
- Construct a ![:emphasize](performance curve) across decision thresholds
]

---
class: onecol
## Classic Distance Metrics for Regression

.pull-left[
**Root Mean Squared Error (RMSE)**
- Based on squared loss
- Easier to computationally optimize
- Ranges from `\(0\)` to `\(+\infty\)`, lower is better
]

.pull-right[
&lt;br /&gt;
`$$RMSE=\sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2}$$`
]

.pull-left[
**Mean Absolute Error (MAE)**
- Based on absolute loss
- More robust to outliers
- Ranges from `\(0\)` to `\(+\infty\)`, lower is better
]

.pull-right[
&lt;br /&gt;
`$$MAE=\frac{1}{n} \sum_{i=1}^n \left\lvert y_i - p_i \right\rvert$$`
]

.footnote[[1] Note that, here, we will refer to the trusted labels as `\\(y\\)` and the predicted labels as `\\(p\\)`.]

---
class: onecol
exclude: true
## Advanced Distance Metrics for Regression
**Huber loss (HL)**

- Combines benefits of RMSE and MAE
 + Easy to computationally optimize AND more robust to outliers
- Requires setting or tuning `\(\delta\)` (controls what to consider an outlier)
- Ranges from `\(0\)` (best) to `\(+\infty\)` (worst), lower is better

`$$\begin{split}
HL &amp;= \frac{1}{n} \sum_{i=1}^n L_\delta(y_i, p_i) \\
L_\delta(y_i, p_i) &amp;= \begin{cases}\frac{1}{2}(y_i - p_i)^2 &amp; \text{for } \lvert y_i - p_i \rvert \le \delta \\ \delta (\lvert y_i - p_i \rvert - \frac{1}{2}\delta) &amp; \text{otherwise} \end{cases}
\end{split}$$`

---
## Comparing Loss Functions

&lt;img src="Day_2B_files/figure-html/losses-1.png" width="100%" /&gt;

---
exclude: true
## Comparing Loss Functions

&lt;img src="Day_2B_files/figure-html/losses3-1.png" width="100%" /&gt;

---
class: onecol
## Correlation Metrics for Regression

**R-Squared `\((R^2\)` or RSQ)**
- How much of the variability in the trusted labels do the predictions explain?
 + Calculated in ML as the **squared correlation** between the predictions and labels
- Several cautions about using RSQ as a performance metric
 + It is a measure of **consistency** and not accuracy (may disagree with distance)
 + It cannot be calculated if the predicted or trusted labels are constant
 + It is highly sensitive to the amount of label variability (may be unstable)
- Ranges from `\(0\)` to `\(1\)`, higher is better

`$$R^2 = \left(\frac{\text{cov}(y, p)}{\sigma_y\sigma_p}\right)^2 = \left(\frac{\sum(y_i - \bar{y})(p_i - \bar{p})}{\sqrt{\sum (y_i-\bar{y})^2}\sqrt{\sum(p_i-\bar{p})^2}}\right)^2$$`

---
class: onecol
## Comparing Distance and Correlation Metrics

&lt;img src="Day_2B_files/figure-html/dvc-1.png" width="100%" /&gt;


---
exclude: true
class: onecol
## Advanced Correlation Metrics for Regression

**Concordance Correlation Coefficient (CCC)**
- Equivalent to certain formulations of the intraclass correlation coefficient
- Combines both accuracy (distance) and consistency (correlation) information
- Ranges from `\(-1\)` (worst) to `\(+1\)` (best)

`$$CCC = \frac{\frac{2}{n}\sum_{i=1}^n (t_i - \bar{t})(p_i - \bar{p})}{\frac{1}{n}\sum_{i=1}^n(t_i - \bar{t})^2 + \frac{1}{n}\sum_{i=1}^n(p_i - \bar{p})^2 + (\bar{t} - \bar{p})^2}$$`
---
## Confusion Matrix Metrics for Classification

The classic confusion matrix is for binary (i.e., no/yes or 0/1) classification

 | Trusted = No | Trusted = Yes
:--| :--: | :--:
**Predicted = No** | True Negatives (TN) | False Negative (FN)
**Predicted = Yes** | False Positive (FP) | True Positive (TP)

The counts or proportions can be informative on their own or combined into metrics

--

.pull-left[
`$$\begin{split}
\text{Accuracy} &amp;= \frac{TN + TP}{(TN + FN + FP + TP)} \\
\text{Sensitivity} &amp;= \frac{TP}{TP+FN} \\
\text{Specificity} &amp;= \frac{TN}{TN + FP} \\
\text{Balanced Accuracy} &amp;= (\text{Sensitivity} + \text{Specificity})/2 \\
\end{split}$$`
]

--

.pull-right[
`$$\begin{split}
\text{Precision} &amp;= \frac{TP}{TP+FP} \\
\text{Recall} &amp;= \text{Sensitivity} \\
F_1 \text{ Score} &amp;= \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
PPV &amp;= \text{Precision} \\
NPV &amp;= \frac{TN}{TN + FN}
\end{split}$$`
]

---
class: onecol
## Multiclass Confusion Matrix

With more than two classes, the same principles apply (but the math gets harder)

 | Trusted = Healthy | Trusted = Depression | Trusted = Mania
:--| :--: | :--: | :--:
**Predicted = Healthy** | 100 | 3 | 7
**Predicted = Depression** | 2 | 25 | 0 
**Predicted = Mania** | 10 | 1 | 10

`$$\text{Average Accuracy} = \frac{1}{k}\sum_{j=1}^k\frac{TP_j + TN_j}{TP_j + TN_j + FP_j + FN_j}$$`

.footnote[[1] In multiclass settings, there are micro- and macro-averages but that is a more advanced topic to explore later.]

---
class: onecol
## Class Probability Metrics for Classification

---
class: onecol
## Performance Curves for Classification

---
class: onecol
## Training Set Performance

---
class: onecol
## Test Set Predictions

predict()
- object
- newdata 
- type
- na.action

---

postResample()
- pred
- obs

---

confusionMatrix()

---

yardstick
- ccc
- huber_loss
- mcc
- roc_auc
- metric_set

---
class: inverse, center, middle
# Model Interpretation

varImp()
- object
- scale

train$finalModel
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current% / %total%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
