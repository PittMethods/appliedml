<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Basic Predictive Modeling</title>
    <meta charset="utf-8" />
    <meta name="author" content="Applied Machine Learning in R  Pittsburgh Summer Methodology Series" />
    <script src="Day_2B_files/header-attrs/header-attrs.js"></script>
    <link href="Day_2B_files/tachyons/tachyons.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <span style="font-size:48pt;">Basic Predictive Modeling</span>
## üßÆ üîÆ üë®‚Äçüíª
### Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series
### Lecture 2-B ‚ÄÉ ‚ÄÉ July 20, 2021

---










class: inverse, center, middle
# Overview

&lt;style type="text/css"&gt;
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
.remark-code {
  font-size: 24px;
  border: 1px solid grey;
}
a {
  background-color: lightblue;
}
.remark-inline-code {
  background-color: white;
}
&lt;/style&gt;
---
class: onecol
## Plan for Day 2-B
First will be a lecture on **Performance Metrics** (one of my favorite topics)

.pt1[
Second will be a **lecture** on training, evaluation, and interpretation in R
]

- We will **adapt familiar (statistical) algorithms** to predictive modeling

- *This will ease the transition to ML and highlight its similarities with classical statistics*

- We will also **foreshadow future topics** (e.g., regularized linear models and tuning)

.pt1[
Finally, we will have a Live Coding Activity and related Hands-on Activity
]

---
class: onecol
## Performance Metrics

.left-column.pt3[
&lt;img src="target.jpg" width="100%" /&gt;
]

.right-column[

**Metrics for Supervised Regression**
- ![:emphasize](Distance) between predicted and trusted values
- ![:emphasize](Correlation) between predicted and trusted values

.pt1[
**Metrics for Supervised Classification**
]
- ![:emphasize](Confusion matrix) between predicted and trusted classes
- Compare predicted ![:emphasize](class probabilities) to trusted classes
]

---
class: inverse, center, middle
# Metrics for Regression
---
class: onecol
## Classic Distance Metrics for Regression

.pull-left[
**Root Mean Squared Error (RMSE)**
- Based on squared loss
- Penalizes severe errors harsher, Sensitive to outliers
- Ranges from `\(0\)` to `\(+\infty\)`, lower is better
]

.pull-right.pt4[
`$$RMSE=\sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2}$$`
]

.footnote[[1] Note that, here, we will refer to the trusted labels as `\\(y\\)` and the predicted labels as `\\(p\\)`.]

--

.pull-left[
**Mean Absolute Error (MAE)**
- Based on absolute loss
- Penalizes error consistently, Robust to outliers
- Ranges from `\(0\)` to `\(+\infty\)`, lower is better
]

.pull-right.pt4[
`$$MAE=\frac{1}{n} \sum_{i=1}^n \left\lvert y_i - p_i \right\rvert$$`
]

---
exclude: true
class: onecol
## Advanced Distance Metrics for Regression
**Huber loss (HL)**

- Combines benefits of RMSE and MAE
 + Easy to computationally optimize AND more robust to outliers
- Requires setting or tuning `\(\delta\)` (controls what to consider an outlier)
- Ranges from `\(0\)` (best) to `\(+\infty\)` (worst), lower is better

`$$\begin{split}
HL &amp;= \frac{1}{n} \sum_{i=1}^n L_\delta(y_i, p_i) \\
L_\delta(y_i, p_i) &amp;= \begin{cases}\frac{1}{2}(y_i - p_i)^2 &amp; \text{for } \lvert y_i - p_i \rvert \le \delta \\ \delta (\lvert y_i - p_i \rvert - \frac{1}{2}\delta) &amp; \text{otherwise} \end{cases}
\end{split}$$`

---
## Visualizing Regression Loss Functions

&lt;img src="Day_2B_files/figure-html/losses-1.png" width="100%" /&gt;

---
exclude: true
## Comparing Loss Functions

&lt;img src="Day_2B_files/figure-html/losses3-1.png" width="100%" /&gt;

---
class: onecol
## Correlation Metrics for Regression

**R-Squared `\((R^2\)` or RSQ)**
- Calculated in ML as the **squared correlation** between the predictions and labels
- Ranges from `\(0\)` to `\(1\)`, higher is better

`$$R^2 = \left(\frac{\text{cov}(y, p)}{\sigma_y\sigma_p}\right)^2 = \left(\frac{\sum(y_i - \bar{y})(p_i - \bar{p})}{\sqrt{\sum (y_i-\bar{y})^2}\sqrt{\sum(p_i-\bar{p})^2}}\right)^2$$`

--

.bg-light-yellow.b--light-red.ba.bw1.br3.pl4[
**Caution!**
+ RSQ is a measure of *consistency* (i.e., linear association) and not distance
+ RSQ can become unstable or undefined when data variability is low
+ RSQ can become unstable when applied in small samples (e.g., test sets)

]

---
class: onecol
## Advanced Correlation Metrics for Regression

**Concordance Correlation Coefficient (CCC)**

- Combines both accuracy (distance) and consistency (correlation) information

- Very similar to certain formulations of the intraclass correlation coefficient

- Ranges from `\(-1\)` to `\(+1\)`, where higher is better

.pt1[
`$$CCC = \frac{2\rho_{yp}\sigma_y\sigma_p}{\sigma_y^2 + \sigma_p^2 + (\mu_y - \mu_p)^2}
= \frac{\frac{2}{n}\sum (y_i - \bar{y})(p_i - \bar{p})}{\frac{1}{n}\sum(y_i - \bar{y})^2 + \frac{1}{n}\sum(p_i - \bar{p})^2 + (\bar{y} - \bar{p})^2}$$`
]
---
class: onecol
## Comparing Regression Performance Metrics

&lt;img src="Day_2B_files/figure-html/dvc-1.png" width="100%" /&gt;

---
class: inverse, center, middle
# Metrics for Classification
---
class: twocol
## Confusion Matrix Metrics

 | Trusted = No&lt;br /&gt; `\((y=0)\)` | Trusted = Yes&lt;br /&gt; `\((y=1)\)`
:--:| :--: | :--:
**Predicted = No**&amp;emsp;&lt;br /&gt; `\((p=0)\)` | True Negatives (TN) | False Negative (FN)
**Predicted = Yes**&amp;emsp;&lt;br /&gt; `\((p=1)\)` | False Positive (FP) | True Positive (TP)

--

.mt4[
`$$\text{Accuracy} = \frac{TN + TP}{n}$$`
]
.tc[
Ranges from `\(0\)` to `\(1\)`, higher is better
]

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Accuracy can be misleading when the classes are highly imbalanced (e.g., more `\(0\)`s than `\(1\)`s)
]

---
class:onecol
## Confusion Matrix Metrics

With imbalanced classes, predicting the larger class will often be right "by chance"

To "correct" accuracy for imbalanced classes, **Cohen's Kappa** is often used

`$$\kappa = \frac{\text{Accuracy} - \text{Chance}}{1 - \text{Chance}}$$`

--

Chance agreement is estimated using the observed class probabilities from `\(y\)` and `\(p\)`

`$$\text{Chance} = \Pr(y=0)\cdot\Pr(p=0) + \Pr(y=1)\cdot\Pr(p=1)$$`

Kappa also ranges from `\(0\)` to `\(1\)` (technically `\(-1\)` to `\(1\)`), higher is better

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Paradoxically, Kappa can be overly conservative when classes are very imbalanced.
]

---
## Additional Confusion Matrix Metrics

 | `\(y=0\)` | `\(y=1\)`
:--:| :--: | :--:
&amp;emsp; ** `\(p=0\)`**&amp;emsp; | &amp;emsp;True Negatives (TN)&amp;emsp; | &amp;emsp;False Negative (FN)&amp;emsp;
** `\(p=1\)`** | False Positive (FP) | True Positive (TP)

.pull-left.pt3[
`$$\begin{split}
\text{Sensitivity} &amp;= \frac{TP}{TP+FN} \\
\text{Specificity} &amp;= \frac{TN}{TN + FP} \\
\text{Balanced Accuracy} &amp;= \frac{\text{Sensitivity} + \text{Specificity}}{2} \\
\end{split}$$`
]

--

.pull-right.pt3[
`$$\begin{split}
\text{Precision} &amp;= \frac{TP}{TP+FP} \\
\text{Recall} &amp;= \frac{TP}{TP+FN} \\
F_1 \text{ Score} &amp;= \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
\end{split}$$`
]

.tc[
All range from `\(0\)` to `\(1\)`, higher is better
]

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
`\(F_1\)` does not consider `\(TN\)`, so only use it when detecting negatives is not important for your application.]



---
## Confusion Matrix Metrics Examples
.pull-left.tc[
&amp;emsp;**Balanced**&amp;emsp; | &amp;emsp; `\(y=0\)` &amp;emsp; | &amp;emsp; `\(y=1\)` &amp;emsp;
:--:| :--: | :--:
** `\(p=0\)` ** | 101 | 54
** `\(p=1\)` ** | 33 | 105

.pt3.f5[
`$$\begin{split}
\text{Accuracy} &amp;= 0.70 \\
\kappa &amp;= 0.41\\ \\
\text{Sensitivity} &amp;= 0.66 \\
\text{Specificity} &amp;= 0.75 \\
\text{Balanced Accuracy} &amp;= 0.71 \\ \\
\text{Precision} &amp;= 0.76 \\
\text{Recall} &amp;= 0.66 \\
F_1 &amp;= 0.71
\end{split}$$`

]
]

--

.pull-right.tc[
&amp;emsp;**Imbalanced**&amp;emsp; | &amp;emsp; `\(y=0\)` &amp;emsp; | &amp;emsp; `\(y=1\)` &amp;emsp;
:--:| :--: | :--:
** `\(p=0\)` ** | 256 | 11
** `\(p=1\)` ** | 6 | 2

.pt3.f5[

`$$\begin{split}
\text{Accuracy} &amp;= 0.94 \\ 
\kappa &amp;= 0.16 \\
\\
\text{Sensitivity} &amp;= 0.15 \\
\text{Specificity} &amp;= 0.98 \\
\text{Balanced Accuracy} &amp;= 0.57 \\
\\
\text{Precision} &amp;= 0.25 \\
\text{Recall} &amp;= 0.15 \\
F_1 &amp;= 0.19
\end{split}$$`
]
]

---
class: onecol
## Multiclass Performance Strategies

With more than two classes, you can make a larger (e.g., `\(3\times 3\)`) confusion matrix)

 | &amp;emsp; `\(y\)` = Healthy&amp;emsp; | &amp;emsp; `\(y\)` = Depression&amp;emsp; | &amp;emsp; `\(y\)` = Mania&amp;emsp;
:--| :--: | :--: | :--:
** `\(p\)` = Healthy**&amp;emsp; | 100 | 3 | 7
** `\(p\)` = Depression**&amp;emsp; | 30 | 25 | 20 
** `\(p\)` = Mania**&amp;emsp; | 10 | 1 | 10

--

.pt3[
![:emphasize](Macro-averaging): compute the standard binary metric for each class separately (using a one-vs-rest procedure) and then calculate the average metric score across classes
]

&lt;!-- `$$(F_1^H = 0.79, F_1^D = 0.46, F_1^M = 0.34)~\text{Macro }F_1=0.53$$` --&gt;

--

![:emphasize](Micro-averaging): compute a confusion matrix for each class separately (one-vs-rest), add these matrices together, and calculate the binary metric from the summed matrix

&lt;!-- $$ (\Sigma TN=345, \Sigma TP=135, \Sigma FP=75, \Sigma FN=75)~\text{Micro }F_1=0.64$$ --&gt;

---
class: onecol
## Class Probability Metrics for Classification

Some classifiers estimate **the probability of each class** as their prediction `\((p_{ij})\)`

If we consider a higher estimated probability as higher "confidence"

- We can **reward** the classifier for being more **confident when correct**...

- ...and ![:emphasize](penalize) the classifier for being more ![:emphasize](confident when wrong)

--

This gives rise to the Logistic or **Log Loss**, which can be summed or averaged

`$$L_{log}(Y,P) = -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^q \left(y_{ij}\log( p_{ij})\right)$$`

.pl4[
`\(y_{ij}\in\{0,1\}\)` is a binary indicator of whether observation `\(i\)` is truly in class `\(j\)`&lt;br /&gt;
`\(p_{ij}\in(0,1)\)` is the estimated probability that observation `\(i\)` is in class `\(j\)`
]

---

## Visualizing Log Loss in Binary Classification

&lt;img src="Day_2B_files/figure-html/logloss-1.png" width="100%" /&gt;

---
exclude: true
class: onecol
## Performance Curves

- When a classifier outputs class probabilities, we can choose any **decision threshold**

- We might naturally consider any probability over 50% positive and all others negative

- But we could choose a threshold more conservative (e.g., 75%) or liberal (e.g., 25%)

.pt1[
- **Performance curves** plot the characteristics of different decision thresholds

- This gives us an overview of how the classification system performs in general
]

.pt1[
- There are many performance curves&lt;sup&gt;1&lt;/sup&gt;, so we'll use the original as an example

- Finally, the **area under the curve (AUC)** is often used as a performance metric
]

.footnote[[1] Popular options include ROC curves, precision-recall curves, gain curves, and lift curves.]

---
exclude: true
class: onecol
## Receiver Operating Characteristic (ROC) Curves

.pull-left[
Each point in a ROC curve corresponds to a possible decision threshold

The performance metrics compared for each point are Sensitivity and Specificity

Better curves are closer to the top-left

The area under the ROC curve (AUC-ROC) ranges from `\(0.5\)` to `\(1.0\)`, higher is better.

AUCROC is the probability that a random positive example has a higher estimate than a random negative example.

]

.pull-right[
&lt;img src="Day_2B_files/figure-html/rocex-1.png" width="95%" /&gt;
]

---
class: twocol
## Comprehension Check \#1

&lt;span style="font-size:30px;"&gt;Bindi trains Model [A] to predict how many kilometers each bird will migrate this year and Model [B] to predict whether or not it will reproduce this year.&lt;/span&gt;

.pull-left[
**1. Which combination of performance metrics would be appropriate to use?**

a) Log Loss for [A] and CCC for [B]

b) Precision for [A] and Recall for [B]

c) MAE for [A] and Balanced Accuracy for [B]

d) None of the above

]

.pull-right[
**2. Which combination of performance scores should Bindi hope to see?**

a) RMSE = 531.6 and Kappa = 0.04

b) RMSE = 1129.7 and Kappa = 0.04

c) RMSE = 531.6 and Kappa = 0.88

d) RMSE = 1129.7 and Kappa = 0.88
]

---
class: inverse, center, middle
# Training and Cross-Validation
---
class: onecol
## `caret::train()`

{caret} standardizes the syntax to train over 200 different ML algorithms

It also plays nicely with the {recipes} package we learned for preprocessing

--

&lt;p style="padding-top:20px;"&gt;The train() function will handle model &lt;b&gt;training&lt;/b&gt;, &lt;b&gt;resampling&lt;/b&gt;, and &lt;b&gt;tuning&lt;/b&gt;&lt;/p&gt;

Because LM and GLM have no hyperparameters, we don't need tuning (yet!)

--

&lt;p style="padding-top:20px;"&gt;Today, we will &lt;b&gt;focus on training&lt;/b&gt; and just use resampling to estimate performance&lt;/p&gt;

We will point to where tuning would be configured but leave that for tomorrow

---
class: onecol
## Main Arguments to `train()`

Argument&amp;emsp; | Description
:------- | :----------
x | A {recipe} object with variable roles and preprocessing steps
data | A data frame to be used for training (prior to baking)
method | A string indicating the algorithm to train (e.g., "lm" or "glm")&amp;emsp;

--

To train any of the 200+ supported algorithms, you just need to change `method`

This makes it *super easy* to implement new algorithms and explore the world of ML!

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Be sure you understand an algorithm before trying to publish a paper using it.
]

---
class: onecol
## Additional Arguments to `train()`

Argument | Description
:------- | :----------
trControl | Controls the resampling procedure used during tuning
metric | Controls which metric to optimize during tuning
tuneGrid | Control which specific tuning values to compare
tuneLength&amp;emsp; | Control how many tuning values to automatically compare&amp;emsp;

These arguments are largely used to configure tuning (discussed tomorrow)

The training set will be resampled and different tuning values will be compared

The "best" tuning values will be used to train a **final model** using all the training data

---
class: onecol
## Resampling options

The `trControl` argument can be configured by `trainControl()`:

Argument&amp;emsp; | Description
:------- | :----------
method | Controls the type of resampling (e.g., "cv", "repeatedcv", "boot")&amp;emsp;
number | Controls the number of folds in cv and iterations in boot
repeats | Controls the number of repetitions in repeatedcv

"cv" will perform resampling through `\(k\)`-fold cross-validation

"repeatedcv" will repeat `\(k\)`-fold cross-validation multiple times

"boot" will perform resampling through bootstrapping

---
class: onecol
## Pseudo-code for training with resampling


```r
# Configure resampling options
*resampling_options &lt;- trainControl(
* method = "repeatedcv",
* number = 10,
* repeats = 3
*) 

# Train model from recipe
trained_model &lt;- train(
  x = my_recipe,
  data = my_training,
  method = "lm",
* trControl = resampling_options
)
```

---
exclude: true
class: onecol
## Advanced Resampling Options

There are many other arguments to `trainControl()` to explore

- **`selectionFunction`** can be used to prioritize less complex models*
- **`predictionBounds`** can be used to constrain predicted values in regression
- **`sampling`** can be used to address imbalanced labels in classification*
- **`seeds`** can be used to make the resampling procedure reproducible

.footnote[*These are both advanced topics that we can revisit on Day 5.]

There are also alternative resampling methods to explore
- Fancier versions of bootstrapping (e.g., "boot632")
- Algorithm-specific methods (e.g., "oob")
- Adaptive methods that tune faster/smarter (e.g., "adaptive_cv")
- The {rsample} package from {tidymodels} provides even more options

---
## Comprehension Check \#2

.pull-left[

```r
# Part 1
my_recipe &lt;- 
  my_data %&gt;% 
  recipe(outcome ~ .) %&gt;% 
  step_center(all_numeric()) %&gt;% 
  step_zv(all_predictors()) %&gt;% 
  prep(training = my_training)

# Part 2
trained_model &lt;- train(
  x = my_recipe,
  data = my_training_set,
  method = "cv"
)
```

]

.pull-right[
**1. What was the main mistake made in Part 1?**

&amp;emsp; a) The predictors should be listed in `recipe()`

&amp;emsp; b) Numeric predictors cannot be centered

&amp;emsp; c) `step_zv()` only works for numeric predictors

&amp;emsp; d) The recipe should not be prepped yet

**2. What was the main mistake made in Part 2?**

&amp;emsp; a) `data` should be `my_testing_set`

&amp;emsp; b) `x` should be `my_data`

&amp;emsp; c) "cv" is not a method for `train()`

&amp;emsp; d) Forgot to add the `number` argument
]

---
class: onecol
# Applied Example
Let's put what we just learned into practice in R

.pt1[
Let's use {caret} and {recipes} to train two models on the `titanic` data
]

- We will load in and split the data

- We will create two recipes for feature engineering

- We will use LM to try to predict each passenger's fare (how much they paid)

---
class: onecol
## Applied Example


```r
# Read in data
titanic &lt;- read_csv("titanic.csv")

# Create a training and testing set (stratified by fare)
set.seed(2021)
fare_index &lt;- createDataPartition(titanic$fare, p = 0.75, list = FALSE)
fare_train &lt;- titanic[fare_index, ]
fare_test &lt;- titanic[-fare_index, ]
```

--


```r
# Check sizes
dim(fare_train)
#&gt; [1] 723   7
dim(fare_test)
#&gt; [1] 240   7
```

---
class: onecol
## Applied Example


```r
# Create a preprocessing recipe (no prep)
fare_recipe &lt;- 
  titanic %&gt;% 
  recipe(fare ~ .) %&gt;% 
  step_rm(survived) %&gt;%
  step_normalize(all_numeric_predictors()) %&gt;% 
  step_dummy(all_nominal_predictors()) %&gt;% 
  step_nzv(all_predictors()) %&gt;% 
  step_corr(all_predictors())
```

---
class: onecol
## Applied Example 


```r
set.seed(2021)

# Configure resampling
fare_tc &lt;- trainControl(method = "cv", number = 10)

# Train the model using the recipe, data, and method
fare_lm &lt;- train(
  fare_recipe, 
  data = fare_train,
  method = "lm",
  trControl = fare_tc
)
```

---
class: inverse, center, middle
# Model Evaluation
---
class: onecol
## Training Set Performance

The object created by `train()` will contain lots of information

We can view a summary of training set performance in the `$results` field
- Each row corresponds to one combination or set of hyperparameters
- Columns define the **hyperparameter values** and the set's **performance scores** 
- Results will include the Mean and SD of each metric across resamples

--


```r
fare_lm$results
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; intercept &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; RMSE &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Rsquared &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; MAE &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; RMSESD &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; RsquaredSD &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; MAESD &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; TRUE &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 41.31 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.48 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20.96 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12.84 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.83 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

.footnote[[1] Because there are no real hyperparameters for LM, there is only one row in `results`.]

---
class: onecol
## Test Set Predictions
To evaluate performance on the test set, we need two things:

1. Predictions from the model on the test set (e.g., values, classes, or probabilities)

2. Trusted labels on the test set (after any preprocessing steps from {recipes})

--

.pt1[
We can compare these predicted and trusted labels using {caret} or {yardstick}
]
- {caret} has basic performance metrics built in and is straightforward to use

- {yardstick} offers many more options but requires some coding to work with {caret}

---
class: onecol
## Test Set Predictions

To get predictions from the final model on new data&lt;sup&gt;1&lt;/sup&gt;, we can use `predict()`

--

Argument&amp;emsp; | Description
:------- | :----------
object | A trained model object created by `train()`
newdata | A data frame with the same features (prior to baking)&amp;emsp;
type | Return raw classes ("raw") or probabilities ("prob")?

.pt3[

```r
fare_pred &lt;- predict(fare_lm, newdata = fare_test)
glimpse(fare_pred)
#&gt;  num [1:240] 114 103.6 101.6 93.7 92.9 ...
```
]

.footnote[[1] The same process is used for both evaluating performance and deploying the model in the real world.]

---
class: onecol
## Test Set Labels 
To get labels from the final model in the format expected by R, we can use `bake()`


```r
fare_test_baked &lt;- 
  fare_recipe %&gt;% 
  prep(training = fare_train) %&gt;% 
  bake(new_data = fare_test)

fare_true &lt;- fare_test_baked$fare
glimpse(fare_true)
#&gt;  num [1:240] 151.6 51.5 227.5 78.8 76.3 ...
```

--

A few points of potential confusion to avoid:

1. Use `train()` and `predict()` before baking the recipe (i.e., let them do it)
2. `caret::predict()` uses `newdata`, whereas `recipes::bake()` uses `new_data`

---
class: onecol
## Visualizing Predicted and Trusted Labels


```r
qplot(x = fare_true, y = fare_pred) + geom_abline()
```

&lt;img src="Day_2B_files/figure-html/quick-1.png" width="100%" /&gt;

---
class: onecol
## Estimating Test Set Performance

To estimate performance in {caret}, use `postResample()`

Argument&amp;emsp; | Description
:------- | :----------
pred | A vector of predicted labels&amp;emsp;
obs | A vector of trusted labels


```r
postResample(pred = fare_pred, obs = fare_true)
#&gt;       RMSE   Rsquared        MAE 
#&gt; 44.2923446  0.4240302 21.5320223
```

This function will give basic results (e.g., Accuracy) for classification too

---
class: onecol
## More Metrics from {yardstick}

The {yardstick} package provides lots of performance metrics

For regression, I like the **Huber Loss** distance metric and **CCC** correlation metric


```r
# Huber Loss (blends RMSE and MAE)
yardstick::huber_loss_vec(truth = fare_true, estimate = fare_pred)
#&gt; [1] 21.04084
```


```r
# Concordance Correlation Coefficient
yardstick::ccc_vec(truth = fare_true, estimate = fare_pred)
#&gt; [1] 0.593931
```

For classification, I like **MCC**, **Log Loss**, and **ROC Curve** (shown in Live Coding)

---
class: inverse, center, middle
# Model Interpretation
---
class: onecol
## Variable Importance

Predictive accuracy is emphasized in ML over interpretability

- The main goal of most applied ML studies is to quantify performance

--

However, some algorithms can provide insight into their decision-making

- As a model usually used for inference, LM has strong interpretability

- We can examine the model coefficients (intercept and slopes)

--


```r
fare_lm$finalModel %&gt;% coefficients()
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; (Intercept) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; age &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; sibsp &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; parch &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; pclass_X2nd &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; pclass_X3rd &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; sex_male &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 99.59 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.78 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.36 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.53 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -71.78 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -80.65 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -9.88 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
class: onecol
## Variable Importance

However, these coefficients have different units and levels of uncertainty

Instead, for LM, we use the absolute value of each coefficient's `\(t\)`-statistic


```r
varImp(fare_lm, scale = FALSE)
#&gt; lm variable importance
#&gt; 
#&gt;             Overall
#&gt; pclass_X3rd  19.028
#&gt; pclass_X2nd  15.867
#&gt; parch         4.941
#&gt; sibsp         3.120
#&gt; sex_male      2.824
#&gt; age           1.002
```

.footnote[[1] Other algorithms have different ways to estimate variable importance, but `varImp()` will take care of it.&lt;br /&gt;[2] Some algorithms have no easy way to estimate variable importance and are considered true "black boxes."]

---
class: onecol
## Live Coding

Now that we have explored the prediction of passenger fare, let's predict survival

Training, evaluating, and interpreting a ![:emphasize](classification) model is very similar but...

- We will be using GLM (logistic regression) instead of LM as our **new algorithm**

- We will have some **different performance metrics** to calculate and interpret

- We can explore the **raw class predictions** and **estimated class probabilities**

--

.pt1[
Finally, as a hands-on activity, you will classify `satisfaction` in the `airsat` dataset
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current% / %total%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
