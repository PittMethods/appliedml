<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Basic Predictive Modeling</title>
    <meta charset="utf-8" />
    <meta name="author" content="Applied Machine Learning in R  Pittsburgh Summer Methodology Series" />
    <script src="Day_2B_files/header-attrs/header-attrs.js"></script>
    <link href="Day_2B_files/tachyons/tachyons.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <span style="font-size:48pt;">Basic Predictive Modeling</span>
## üßÆ üîÆ üë®‚Äçüíª
### Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series
### Lecture 2-B ‚ÄÉ ‚ÄÉ July 20, 2021

---










class: inverse, center, middle
# Overview

&lt;style type="text/css"&gt;
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
&lt;/style&gt;
---
class: onecol
## Plan for Day 2-B
First will be an extended lecture on **Performance Metrics** (one of my favorite topics)

Second will be a brief **lecture overview** of training, evaluation, and interpretation in R

Third, we will apply this with an extended **live coding** session and hands-on activity

--

.pt1[
In the live coding, we will **adapt familiar algorithms** to predictive modeling
- Linear modeling will be adapted for continuous prediction (regression)
- Generalized linear modeling will be adapted for discrete prediction (classification)

*This will ease the transition to ML and highlight its similarities with classical stats*
]

--

.pt1[
We will also **foreshadow future topics** (e.g., regularized linear models and tuning)
]

---
class: inverse, center, middle
# Performance Metrics
---
class: onecol
## Performance Metrics

.left-column.pt3[
&lt;img src="target.jpg" width="100%" /&gt;
]

.right-column[

**Metrics for Supervised Regression**
- ![:emphasize](Distance) between predicted and trusted values
- ![:emphasize](Correlation) between predicted and trusted values

.pt1[
**Metrics for Supervised Classification**
]
- ![:emphasize](Confusion matrix) between predicted and trusted classes
- Compare predicted ![:emphasize](class probabilities) to trusted classes
- Construct a ![:emphasize](performance curve) across decision thresholds
]

---
class: onecol
## Classic Distance Metrics for Regression

.pull-left[
**Root Mean Squared Error (RMSE)**
- Based on squared loss
- Easier to computationally optimize
- Ranges from `\(0\)` to `\(+\infty\)`, lower is better
]

.pull-right.pt4[
`$$RMSE=\sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2}$$`
]

--

.pull-left[
**Mean Absolute Error (MAE)**
- Based on absolute loss
- More robust to outliers
- Ranges from `\(0\)` to `\(+\infty\)`, lower is better
]

.pull-right.pt4[
`$$MAE=\frac{1}{n} \sum_{i=1}^n \left\lvert y_i - p_i \right\rvert$$`
]

.footnote[[1] Note that, here, we will refer to the trusted labels as `\\(y\\)` and the predicted labels as `\\(p\\)`.]

---
exclude: true
class: onecol
## Advanced Distance Metrics for Regression
**Huber loss (HL)**

- Combines benefits of RMSE and MAE
 + Easy to computationally optimize AND more robust to outliers
- Requires setting or tuning `\(\delta\)` (controls what to consider an outlier)
- Ranges from `\(0\)` (best) to `\(+\infty\)` (worst), lower is better

`$$\begin{split}
HL &amp;= \frac{1}{n} \sum_{i=1}^n L_\delta(y_i, p_i) \\
L_\delta(y_i, p_i) &amp;= \begin{cases}\frac{1}{2}(y_i - p_i)^2 &amp; \text{for } \lvert y_i - p_i \rvert \le \delta \\ \delta (\lvert y_i - p_i \rvert - \frac{1}{2}\delta) &amp; \text{otherwise} \end{cases}
\end{split}$$`

---
## Visualizing Regression Loss Functions

&lt;img src="Day_2B_files/figure-html/losses-1.png" width="100%" /&gt;

---
exclude: true
## Comparing Loss Functions

&lt;img src="Day_2B_files/figure-html/losses3-1.png" width="100%" /&gt;

---
class: onecol
## Correlation Metrics for Regression

**R-Squared `\((R^2\)` or RSQ)**
- Calculated in ML as the **squared correlation** between the predictions and labels
- Ranges from `\(0\)` to `\(1\)`, higher is better

`$$R^2 = \left(\frac{\text{cov}(y, p)}{\sigma_y\sigma_p}\right)^2 = \left(\frac{\sum(y_i - \bar{y})(p_i - \bar{p})}{\sqrt{\sum (y_i-\bar{y})^2}\sqrt{\sum(p_i-\bar{p})^2}}\right)^2$$`

--

.bg-light-yellow.b--light-red.ba.bw1.br3.pl4[
**Caution!**
+ RSQ is a measure of *consistency* (i.e., linear association) and not distance
+ RSQ can become unstable or undefined when data variability is low
+ RSQ can become unstable when applied in small samples (e.g., test sets)

]

---
class: onecol
## Advanced Correlation Metrics for Regression

**Concordance Correlation Coefficient (CCC)**

- Combines both accuracy (distance) and consistency (correlation) information

- Very similar to certain formulations of the intraclass correlation coefficient

- Ranges from `\(-1\)` to `\(+1\)`, where higher is better

.pt1[
`$$CCC = \frac{2\rho_{yp}\sigma_y\sigma_p}{\sigma_y^2 + \sigma_p^2 + (\mu_y - \mu_p)^2}
= \frac{\frac{2}{n}\sum (y_i - \bar{y})(p_i - \bar{p})}{\frac{1}{n}\sum(y_i - \bar{y})^2 + \frac{1}{n}\sum(p_i - \bar{p})^2 + (\bar{y} - \bar{p})^2}$$`
]
---
class: onecol
## Comparing Regression Performance Metrics

&lt;img src="Day_2B_files/figure-html/dvc-1.png" width="100%" /&gt;

---
class: twocol
## Confusion Matrix Metrics for Classification

 | Trusted = No&lt;br /&gt; `\((y=0)\)` | Trusted = Yes&lt;br /&gt; `\((y=1)\)`
:--:| :--: | :--:
**Predicted = No**&amp;emsp;&lt;br /&gt; `\((p=0)\)` | True Negatives (TN) | False Negative (FN)
**Predicted = Yes**&amp;emsp;&lt;br /&gt; `\((p=1)\)` | False Positive (FP) | True Positive (TP)

.mt4[
`$$\text{Accuracy} = \frac{TN + TP}{(TN + FN + FP + TP)}$$`
]
.tc[
Ranges from `\(0\)` to `\(1\)`, higher is better
]

--

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Accuracy can be misleading when the classes are highly imbalanced (e.g., if `\(TN \gg TP\)`)
]

---
## Additional Confusion Matrix Metrics

 | `\(y=0\)` | `\(y=1\)`
:--:| :--: | :--:
&amp;emsp; ** `\(p=0\)`**&amp;emsp; | &amp;emsp;True Negatives (TN)&amp;emsp; | &amp;emsp;False Negative (FN)&amp;emsp;
** `\(p=1\)`** | False Positive (FP) | True Positive (TP)

.pull-left.pt3[
`$$\begin{split}
\text{Sensitivity} &amp;= \frac{TP}{TP+FN} \\
\text{Specificity} &amp;= \frac{TN}{TN + FP} \\
\text{Balanced Accuracy} &amp;= \frac{\text{Sensitivity} + \text{Specificity}}{2} \\
\end{split}$$`
]

--

.pull-right.pt3[
`$$\begin{split}
\text{Precision} &amp;= \frac{TP}{TP+FP} \\
\text{Recall} &amp;= \frac{TP}{TP+FN} \\
F_1 \text{ Score} &amp;= \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
\end{split}$$`
]

.tc[
All range from `\(0\)` to `\(1\)`, higher is better
]

--

.footnote[[1] Note that the formulas for Sensitivity and Recall are the same. Many of these metrics enjoy many names.&lt;br /&gt;[2] `\\(F_1\\)` does not consider `\\(TN\\)`, which is sometimes a reason to use it and sometimes a reason to avoid it.]

---
## Confusion Matrix Metrics Examples
.pull-left.tc[
**Balanced Classes**

 | `\(y=0\)` | `\(y=1\)`
:--:| :--: | :--:
** `\(p=0\)` ** | 101 | 54
** `\(p=1\)` ** | 33 | 105
.pt3[

`$$\begin{split}
\text{Accuracy} &amp;= 0.70 \\
\\
\text{Sensitivity} &amp;= 0.66 \\
\text{Specificity} &amp;= 0.75 \\
\text{Balanced Accuracy} &amp;= 0.71 \\
\\
\text{Precision} &amp;= 0.76 \\
\text{Recall} &amp;= 0.66 \\
F_1 &amp;= 0.71
\end{split}$$`

]
]

--

.pull-right.tc[
**Imbalanced Classes**

 | `\(y=0\)` | `\(y=1\)`
:--:| :--: | :--:
** `\(p=0\)` ** | 256 | 11
** `\(p=1\)` ** | 6 | 2
.pt3[

`$$\begin{split}
\text{Accuracy} &amp;= 0.94 \\ 
\\
\text{Sensitivity} &amp;= 0.15 \\
\text{Specificity} &amp;= 0.98 \\
\text{Balanced Accuracy} &amp;= 0.57 \\
\\
\text{Precision} &amp;= 0.25 \\
\text{Recall} &amp;= 0.15 \\
F_1 &amp;= 0.19
\end{split}$$`
]
]

---
class: onecol
## Multiclass Performance Strategies

 | &amp;emsp; `\(y\)` = Healthy&amp;emsp; | &amp;emsp; `\(y\)` = Depression&amp;emsp; | &amp;emsp; `\(y\)` = Mania&amp;emsp;
:--| :--: | :--: | :--:
** `\(p\)` = Healthy**&amp;emsp; | 100 | 3 | 7
** `\(p\)` = Depression**&amp;emsp; | 2 | 25 | 0 
** `\(p\)` = Mania**&amp;emsp; | 10 | 1 | 10

--

.pt3[
![:emphasize](Macro-averaging): compute the performance for each class in a one-versus-all manner using standard binary metrics, then calculate the unweighted average across classes
]

--

![:emphasize](Macro-weighting): same as macro-averaging but use a weighted average across classes such that performance on larger/more frequent classes is emphasized

--

![:emphasize](Micro-averaging): compute a confusion matrix for each class in a one-versus-all manner, then calculate binary metrics using sums of each cell (e.g., `\(\sum {TP}_j\)`)

---
class: onecol
## Class Probability Metrics for Classification

Some classifiers estimate **the probability of each class** as their prediction `\((p_{ij})\)`

If we consider a higher estimated probability as higher "confidence"

- We can **reward** the classifier for being more **confident when correct**...

- ...and ![:emphasize](penalize) the classifier for being more ![:emphasize](confident when wrong)

--

This gives rise to the Logistic or **Log Loss**, which can be summed or averaged

`$$L_{log}(Y,P) = -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^q \left(y_{ij}\log( p_{ij})\right)$$`

.pl4[
`\(y_{ij}\in\{0,1\}\)` is a binary indicator of whether observation `\(i\)` is truly in class `\(j\)`&lt;br /&gt;
`\(p_{ij}\in(0,1)\)` is the estimated probability that observation `\(i\)` is in class `\(j\)`
]

---

## Visualizing Log Loss in Binary Classification

&lt;img src="Day_2B_files/figure-html/logloss-1.png" width="100%" /&gt;

---
class: onecol
## Performance Curves

- When a classifier outputs class probabilities, we can choose any **decision threshold**

- We might naturally consider any probability over 50% positive and all others negative

- But we could choose a threshold more conservative (e.g., 75%) or liberal (e.g., 25%)

--

.pt1[
- **Performance curves** plot the characteristics of different decision thresholds

- This gives us an overview of how the classification system performs in general
]

--

.pt1[
- There are many performance curves&lt;sup&gt;1&lt;/sup&gt;, so we'll use the original as an example

- Finally, the **area under the curve (AUC)** is often used as a performance metric
]

.footnote[[1] Popular options include ROC curves, precision-recall curves, gain curves, and lift curves.]

---
class: onecol
## Receiver Operating Characteristic (ROC) Curves

.pull-left[
Each point in a ROC curve corresponds to a possible decision threshold

The performance metrics compared for each point are Sensitivity and Specificity

Better curves are closer to the top-left

The area under the ROC curve (AUC-ROC) ranges from `\(0.5\)` to `\(1.0\)`, higher is better.

AUCROC is the probability that a random positive example has a higher estimate than a random negative example.

]

--

.pull-right[
&lt;img src="Day_2B_files/figure-html/rocex-1.png" width="95%" /&gt;
]

---
class: twocol
## Comprehension Check \#1

&lt;span style="font-size:30px;"&gt;Bindi trains Model [A] to predict how many kilometers each bird will migrate this year and Model [B] to predict whether or not it will reproduce this year.&lt;/span&gt;

.pull-left[
**1. Which combination of performance metrics would be appropriate to use?**

a) ROC-Curve for [A] and CCC for [B]

b) Precision for [A] and Recall for [B]

c) MAE for [A] and Balanced Accuracy for [B]

d) None of the above

]

.pull-right[
**2. Which combination of performance scores should Bindi hope to see?**

a) RMSE = 531.6 and AUC-ROC = 0.04

b) RMSE = 1129.7 and AUC-ROC = 0.04

c) RMSE = 531.6 and AUC-ROC = 0.88

d) RMSE = 1129.7 and AUC-ROC = 0.88
]

---
class: inverse, center, middle
# Training and Cross-Validation
---
class: onecol
## `caret::train()`

{caret} standardizes the syntax to train over 200 different ML algorithms

It also plays nicely with the {recipes} package we learned for preprocessing

--

&lt;p style="padding-top:20px;"&gt;The train() function will handle model &lt;b&gt;training&lt;/b&gt;, &lt;b&gt;resampling&lt;/b&gt;, and &lt;b&gt;tuning&lt;/b&gt;&lt;/p&gt;

Because LM and GLM have no hyperparameters, we don't need tuning (yet!)

--

&lt;p style="padding-top:20px;"&gt;Today, we will &lt;b&gt;focus on training&lt;/b&gt; and just use resampling to estimate performance&lt;/p&gt;

We will point to where tuning would be configured but leave that for tomorrow

---
class: onecol
## Main Arguments to `train()`

Argument&amp;emsp; | Description
:------- | :----------
`x` | A {recipe} object with variable roles and preprocessing steps
`data` | A data frame to be used for training (prior to baking)
`method` | A string indicating the algorithm to train (e.g., "lm" or "glm")&amp;emsp;

--

To train any of the 200+ supported algorithms, you just need to change `method`&lt;sup&gt;1&lt;/sup&gt;

This makes it *super easy* to implement new algorithms and explore the world of ML!

.bg-light-yellow.b--light-red.ba.bw1.br3.mt4.pl4[
Be sure you understand an algorithm before trying to publish a paper using it.
]
.footnote[[1] Of course, you may also need to adapt the features and tuning procedures to match the new algorithm.]

---
class: onecol
## Pseudocode for simple training


```r
# Create data splits
set.seed(2021)
index &lt;- createDataPartition(my_data$outcome, p = 0.80, list = FALSE)
my_training_set &lt;- my_data[index, ]
my_testing_set &lt;- my_data[-index, ]

# Set up recipe
my_recipe &lt;- 
  my_data %&gt;% 
  recipe(outcome ~ .) %&gt;% 
  step_dummy(all_nominal_predictors()) %&gt;% 
  step_nzv(all_predictors()) %&gt;% 
  step_corr(all_predictors())

# Train model from recipe
*trained_model &lt;- train(
* x = my_recipe,
* data = my_training_set,
* method = "lm"
*) 
```

---
class: onecol
## Additional Arguments to `train()`

Argument | Description
:------- | :----------
`trControl` | Controls the resampling procedure used during tuning
`metric` | Controls which metric to optimize during tuning
`tuneGrid` | Control which specific tuning values to compare
`tuneLength`&amp;emsp; | Control how many tuning values to automatically compare&amp;emsp;

These arguments are largely used to configure tuning (discussed tomorrow)

The training set will be resampled and different tuning values will be compared

The "best" tuning values will be used to train a **final model** using all the training data

---
class: onecol
## Resampling options

The `trControl` argument can be configured by `trainControl()`:

Argument&amp;emsp; | Description
:------- | :----------
`method` | Controls the type of resampling (e.g., "cv", "repeatedcv", "boot")&amp;emsp;
`number` | Controls the number of folds in cv and iterations in boot
`repeats` | Controls the number of repetitions in repeatedcv

"cv" will perform resampling through `\(k\)`-fold cross-validation

"repeatedcv" will repeat `\(k\)`-fold cross-validation multiple times

"boot" will perform resampling through bootstrapping

---
class: onecol
## Pseudocode for training with resampling


```r
# Configure resampling options
*resampling_options &lt;- trainControl(
* method = "repeatedcv",
* number = 10,
* repeats = 3
*) 

# Train model from recipe
trained_model &lt;- train(
  x = my_recipe,
  data = my_training_set,
  method = "lm",
* trControl = resampling_options
)
```

---
class: onecol
## Advanced Resampling Options

There are many other arguments to `trainControl()` to explore

- **`selectionFunction`** can be used to prioritize less complex models*
- **`predictionBounds`** can be used to constrain predicted values in regression
- **`sampling`** can be used to address imbalanced labels in classification*
- **`seeds`** can be used to make the resampling procedure reproducible

.footnote[*These are both advanced topics that we can revisit on Day 5.]

--

There are also alternative resampling methods to explore
- Fancier versions of bootstrapping (e.g., "boot632")
- Algorithm-specific methods (e.g., "oob")
- Adaptive methods that tune faster/smarter (e.g., "adaptive_cv")
- The {rsample} package from {tidymodels} provides even more options

---
## Comprehension Check \#2

.pull-left[
Luca wrote the following code to train a model but keeps getting errors and warnings from R. Luca then sent me this message: üëøüî™üíªüî™ü•ï 


```r
# Part 1
my_recipe &lt;- 
  my_data %&gt;% 
  recipe(outcome ~ .) %&gt;% 
  step_center(all_numeric_predictors()) %&gt;% 
  step_zv(all_predictors()) %&gt;% 
  prep(training = my_training_set)

# Part 2
trained_model &lt;- train(
  x = my_recipe,
  data = my_training_set,
  method = "cv"
)
```

]

.pull-right[
**1. What was the main mistake made in Part 1?**

&amp;emsp; a) The predictors should be listed in `recipe()`

&amp;emsp; b) Numeric predictors cannot be centered

&amp;emsp; c) `step_zv()` only works for numeric predictors

&amp;emsp; d) The recipe should not be prepped yet

**2. What was the main mistake made in Part 2?**

&amp;emsp; a) `data` should be `my_testing_set`

&amp;emsp; b) `x` should be `my_data`

&amp;emsp; c) "cv" is not a method for `train()`

&amp;emsp; d) Forgot to add the `number` argument
]

---
class: onecol
# Live Coding: Part 1
Let's put what we just learned into practice in R

.pt1[
We will use {caret} and {recipes} to train two models on the `titanic` data
]

- We will load in and split the data

- We will create two recipes for feature engineering

- We will use LM to try to predict each passenger's fare (how much they paid)

- We will use GLM to try to predict whether each passenger survived the disaster

.pt1[
You can follow along on your computer (the code is online) or just watch me code
]

---
class: inverse, center, middle
# Model Evaluation
---
class: onecol
## Training Set Performance

The object created by `train()` will contain lots of information

.pt1[
We can view a summary of training set performance in the **`results`** field

- e.g., we can print `trained_model$results` to the console to see it

- Each row corresponds to be one combination or set of hyperparameters

- Columns define the **hyperparameter values** and the set's **performance scores** 
]

--
.pt1[
Results will include the Mean and SD of performance metrics across resamples

- By default, it includes {RMSE, RSQ, and MAE} or {Accuracy and Kappa&lt;sup&gt;1&lt;/sup&gt;}
]

.footnote[[1] We did not discuss kappa, but it is basically accuracy with an adjustment for class imbalance.]

---
class: onecol
## Test Set Predictions
To evaluate performance on the test set, we need two things:

1. Predictions from the model on the test set (e.g., values, classes, or probabilities)

2. Trusted labels on the test set (after any preprocessing steps from {recipes})

--

.pt1[
We can compare these predicted and trusted labels using {caret} or {yardstick}
]
- {caret} has basic performance metrics built in and is straightforward to use

- {yardstick} offers many more options but requires some coding to work with {caret}

---
class: onecol
## Test Set Predictions

To get predictions from the final model on new data&lt;sup&gt;1&lt;/sup&gt;, we can use `predict()`

--

Argument&amp;emsp; | Description
:------- | :----------
`object` | A trained model object created by `train()`
`newdata` | A data frame with the same raw features as the training set&lt;br /&gt;(prior to baking, if using {recipes})
`type` | A string indicating whether to return raw predictions (i.e., predicted values or classes) or class probabilities (i.e., "raw" or "prob")

*Note that class probabilities are not available for all classification algorithms*


```r
test_pred &lt;- predict(trained_model, newdata = my_testing_set, type = "raw")
```


.footnote[[1] The same process is used for both evaluating performance and deploying the model in the real world.]

---
class: onecol
## Test Set Labels 
To get labels from the final model in the format expected by R, we can use `bake()`


```r
my_testing_set_baked &lt;- 
  my_recipe %&gt;% 
  prep(training = my_training_set) %&gt;% 
  bake(new_data = my_testing_set)

test_labs &lt;- my_testing_set_baked$outcome
```

--

.pt1[
A few points of potential confusion to avoid:
]
1. Use `train()` and `predict()` before baking the recipe (i.e., let them do it)

2. `caret::predict()` uses `newdata`, whereas `recipes::bake()` uses `new_data`

---
class: onecol
## Estimating Test Set Performance

To estimate performance in {caret}, use `postResample()`

Argument&amp;emsp; | Description
:------- | :----------
`pred` | A vector of predicted labels&amp;emsp;
`obs` | A vector of trusted labels


```r
postResample(pred = test_pred, obs = test_labs)
```

This will calculate the same performance metrics that were in the `results` field
- RMSE, MAE, and RSQ for regression and Accuracy and Kappa for classification

---

confusionMatrix()

---

yardstick
- ccc
- huber_loss
- mcc
- roc_auc
- metric_set

---
class: inverse, center, middle
# Model Interpretation
---
varImp()
- object
- scale

train$finalModel
---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current% / %total%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
