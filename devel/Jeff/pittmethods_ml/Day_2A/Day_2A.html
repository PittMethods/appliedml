<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Feature Engineering</title>
    <meta charset="utf-8" />
    <meta name="author" content="Applied Machine Learning in R  Pittsburgh Summer Methodology Series" />
    <script src="Day_2A_files/header-attrs/header-attrs.js"></script>
    <link href="Day_2A_files/countdown/countdown.css" rel="stylesheet" />
    <script src="Day_2A_files/countdown/countdown.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <span style="font-size:48pt;">Feature Engineering</span>
## .big[üë∑ üóÑÔ∏è üõ†Ô∏è]
### Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series
### Lecture 2-A ‚ÄÉ ‚ÄÉ July 20, 2021

---








class: inverse, center, middle
# Overview

&lt;style type="text/css"&gt;
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
&lt;/style&gt;
---
class: onecol
## Feature Engineering
.left-column[
&lt;br /&gt;
&lt;img src="engineer.jpg" width="100%" /&gt;
]
.right-column[
**Prepare the predictors for analysis**
- *Extract* predictors
- *Transform* predictors
- *Re-encode* predictors
- *Combine* predictors
- *Reduce* predictor dimensionality
- *Impute* missing predictor values
- *Select* and drop predictors
]
---
class: onecol

## Motivation

**Features** are descriptions of the data points that help to predict the outcomes

- We may need to extract features from "raw" or "low-level" data (e.g., images)
- We may need to address issues with missing data and feature distributions

--

There are many potential ways to **encode** or "represent" the features/predictors

- e.g., adding, dropping, transforming, and combining predictors&lt;sup&gt;1&lt;/sup&gt;
- predictor encoding can have a big ![:emphasize](impact on predictive performance)&lt;sup&gt;2&lt;/sup&gt;
- The optimal encoding depends on both the **algorithm** and the **relationships**

.footnote[
[1] Some algorithms can learn their own, complex feature representations&lt;br /&gt;
[2] Some algorithms are more sensitive to feature encoding than others
]

---
class: onecol
## Examples of Feature Encodings
.left-column[
&lt;br /&gt;

&lt;img src="july.jpg" width="100%" /&gt;

]
.right-column[
**When an event or observation occurred**

- The numeric year *(2021)*
- The numeric month *(7)*
- The numeric day of the month *(20)*
- The numeric day of the year *(201)*
- Days since a reference *(diagnosis +2)*
- The day of the week *(Tuesday)*
- The season of the year *(Summer)*
- The type of day *(weekday)* 
- The presence of a holiday *(FALSE)*

]
---
class: onecol
## Issues to Navigate in Feature Engineering

.left-column[
&lt;br /&gt;
&lt;img src="cones.jpg" width="100%" /&gt;
]
.right-column[
- predictors with **non-normal** distributions
- predictors with vastly **different scales**
- predictors with extreme **outliers**
- predictors with **missing** or censored values
- predictors that are **correlated** with one another
- predictors that are **redundant** with one another
- predictors that have zero or **near-zero variance**
- predictors that are **uninformative** for a task
- predictors with **uncertainty** or unreliability
]

---

class: onecol
## Recipes for Feature Engineering

.left-column[
&lt;br /&gt;
&lt;img src="chef.jpg" width="100%" /&gt;

]

.right-column[
All of the predictor engineering steps can be done "by hand" in R

The `caret` package provides some basic convenience tools

We will be learning the &lt;b&gt;`recipes`&lt;/b&gt; package from `tidymodels`

1. Initiate a recipe by declaring data and roles using `recipe()`
1. Add one or more preprocessing steps using `step_*()`
1. Prepare/estimate the preprocessing steps using `prep()`
1. Apply these steps to the training and testing data with `bake()`
]

---
## Example Dataset: `titanic`


```r
# Import and preview the titanic dataset with no missing values
titanic &lt;- read.csv("titanic_complete.csv")
glimpse(titanic)
```

```
## Rows: 963
## Columns: 7
## $ survived &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TRU~
## $ pclass   &lt;chr&gt; "1st", "1st", "1st", "1st", "1st", "1st", "1st", "1st", "1st"~
## $ sex      &lt;chr&gt; "female", "male", "male", "female", "male", "female", "female~
## $ age      &lt;dbl&gt; 29.0000, 0.9167, 30.0000, 25.0000, 48.0000, 63.0000, 53.0000,~
## $ sibsp    &lt;int&gt; 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0~
## $ parch    &lt;int&gt; 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0~
## $ fare     &lt;dbl&gt; 211.3375, 151.5500, 151.5500, 151.5500, 26.5500, 77.9583, 51.~
```

.pull-left[
- `survived`: *Did passenger survive? {FALSE, TRUE}*
- `pclass`: *Passenger class {1st, 2nd, 3rd}*
- `sex`: *Passenger sex {female, male}*
- `age`: *Passenger age (years)*
]

.pull-right[
- `sibsp`: *Siblings/spouses Aboard (\#)*
- `parch`: *Parents/children Aboard (\#)*
- `fare`: *Cost of passenger fare ($)*
]

---
## Recipes for Feature Engineering


```r
# First, let's split the data into a training and testing set
index &lt;- caret::createDataPartition(y = titanic$survived, list = FALSE, p = 0.8)
titanic_train &lt;- titanic[index, ]
titanic_test &lt;- titanic[-index, ]
```

--


```r
# Initiate a recipe for predicting survived from all other variables
titanic_recipe &lt;- recipe(titanic, formula = survived ~ .)

# We could also be explicit with: formula = survived ~ pclass + sex + age + sibsp + parch + fare
```


```r
titanic_recipe %&gt;% print()
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          6
```


---
## Recipes for Feature Engineering


```r
# summary() provides a bit more information than print()
titanic_recipe %&gt;% summary()
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; variable &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; type &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; role &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; source &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; pclass &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nominal &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sex &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nominal &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; age &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sibsp &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; parch &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; fare &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; survived &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; logical &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; outcome &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

--

&lt;p style="padding-top:25px;"&gt;Now we are ready to add some preprocessing (i.e., feature engineering) steps to the recipe!&lt;/p&gt;

---
class: onecol
## Common Steps

.pull-left[
- **Adding predictors**
  - Calculated predictors
  - Categorical predictors
  - Interaction Terms
- **Transforming predictors**
  - Centering and Scaling
  - Addressing Non-normality
  - Adding Non-linearity
]
.pull-right[

- **Reducing predictors**
  - Nero-Zero Variance
  - Multicollinearity
  - Dimensionality Reduction
- **Advanced Steps**
  - Feature Extraction
  - Dealing with Missing Values
  - Feature Selection
]

---
class: inverse, center, middle
# Adding predictors
---
class: onecol
## Calculated predictors

Some variables will need to be calculated from existing values and variables

- You may choose to score an instrument from item-level data
- You may choose to encode a predictor as the ratio of two values
- You may choose to calculate sums, means, counts, proportions, etc.

--

&lt;p style="padding-top:25px;"&gt;We will show you some basic steps for calculating variables within {recipes}&lt;/p&gt;

For more advanced/complex data wrangling, we recommend you read

- &lt;i&gt;[R for Data Science: Visualize, Model, Transform, Tidy, and Import Data](https://r4ds.had.co.nz/)&lt;/i&gt;&lt;br /&gt;by Wickham and Grolemund (book for purchase or online for free)

---
## Calculated predictors


```r
# Add a step to the recipe to calculate new predictors from existing predictors
cp_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_mutate(
*   numfamily = sibsp + parch,
*   over70 = age &gt; 70
* ) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_mutate (mutate_rXn7y): 
##  new (2): numfamily, over70
```

---
## Calculated predictors


```r
# We can ask for a summary of cp_recipe to see what happened
cp_recipe %&gt;% summary()
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; variable &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; type &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; role &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; source &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; pclass &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nominal &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sex &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nominal &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; age &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sibsp &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; parch &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; fare &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; survived &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; logical &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; outcome &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; numfamily &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; derived &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; over70 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; logical &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; derived &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
## Calculated predictors


```r
# If this is the only step in our recipe, we can bake() the recipe
# This will allow us to generate updated training and testing sets
cp_train &lt;- bake(cp_recipe, new_data = titanic_train)
cp_test &lt;- bake(cp_recipe, new_data = titanic_test)
```

--


```r
# Let's see if it worked
glimpse(cp_test)
```

```
## Rows: 191
## Columns: 9
## $ pclass    &lt;fct&gt; 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, ~
## $ sex       &lt;fct&gt; female, female, male, female, female, female, male, male, fe~
## $ age       &lt;dbl&gt; 32, 29, 25, 19, 58, 45, 48, 36, 76, 46, 33, 45, 27, 31, 23, ~
## $ sibsp     &lt;int&gt; 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 3, 1, 1, 0, 0, ~
## $ parch     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, ~
## $ fare      &lt;dbl&gt; 76.2917, 221.7792, 91.0792, 91.0792, 26.5500, 262.3750, 50.4~
## $ survived  &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE~
## $ numfamily &lt;int&gt; 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 5, 2, 1, 0, 1, ~
## $ over70    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE~
```

---
class: onecol
## Categorical predictors

Categorical predictors can be re-encoded into multiple binary (0 or 1) predictors

In `titanic`, the categorical variable &lt;b&gt;`sex`&lt;/b&gt; takes on the value *male* or *female*

--

.pull-left[
.center[![:emphasize](Dummy Coding)]

sex | sex_male
:---|:-------:
female | 0  
male   | 1 

&lt;p style="text-align:center;font-size:20px;"&gt;Efficient and avoids redundancy&lt;br /&gt;Good for GLM-based methods&lt;/p&gt;
]

--

.pull-right[
.center[![:emphasize](One-Hot Encoding)]

sex | sex_female | sex_male
:---|:----------:|:-------:
female | 1  | 0 
male   | 0  | 1

&lt;p style="text-align:center;font-size:20px;"&gt;Simple and easy to interpret&lt;br /&gt;Good for tree-based methods&lt;/p&gt;
]


---
## Categorical predictors in R


```r
# Add a step to the recipe to create dummy codes for pclass and sex
dc_recipe &lt;- 
  titanic %&gt;%
  recipe(survived ~ .) %&gt;% 
* step_dummy(pclass, sex) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_dummy (dummy_q1GbV): 
##  new (3): pclass_X2nd, pclass_X3rd, sex_male
##  removed (2): pclass, sex
```

--

.footnote[[1] As a shortcut, we could also have used `step_dummy(all_nominal_predictors())`.]

---
## Categorical predictors in R


```r
# It is easy to modify this recipe for one-hot encoding
oh_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_dummy(pclass, sex, one_hot = TRUE) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_dummy (dummy_5sMAo): 
##  new (5): pclass_X1st, pclass_X2nd, pclass_X3rd, sex_female, sex_male
##  removed (2): pclass, sex
```

.footnote[[1] Note that step_dummy() is used for both dummy coding and one-hot encoding (with different arguments).]

---
class: onecol
## Interaction Terms

Interaction terms allow the meaning of one predictor to depend on other predictors

In this way, interaction terms allow predictor "effects" to be **contingent** or **conditional**

e.g., perhaps having parents or children on board the Titanic helps you predict survival... but the effects differs depending on whether the passenger is a man or a woman

--

&lt;p style="padding-top:25px;"&gt;Interaction terms are literally &lt;b&gt;products&lt;/b&gt; (i.e., multiplications) of two or more predictors&lt;/p&gt;

In order to include categorical variables in interaction terms, dummy code them first

`$$survived \sim parch + sex\_dummy + parch \times sex\_dummy$$`

---
## Interaction Terms in R


```r
# Add interaction term between parch and the sex_male dummy code
it_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
  step_dummy(pclass, sex) %&gt;% 
* step_interact(~ parch:sex_male) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_dummy (dummy_9QcRD): 
##  new (3): pclass_X2nd, pclass_X3rd, sex_male
##  removed (2): pclass, sex
## 
## step_interact (interact_SbuVE): 
##  new (1): parch_x_sex_male
```

.footnote[[1] step_interact() requires you to use tilde (`~`) and the colon operator (`:`) to specify interaction terms.&lt;br /&gt;[2] We also need to anticipate what the name of the dummy code variable will be and include that.&lt;br /&gt;[3] We could include higher-order interactions by adding more colons and predictors (e.g., `x1:x2:x3`).]

---
## Interaction Terms in R


```r
# Bake the recipe and preview the updated training set
bake(it_recipe, new_data = titanic_train) %&gt;% glimpse()
```

```
## Rows: 772
## Columns: 9
## $ age              &lt;dbl&gt; 29.0000, 0.9167, 30.0000, 25.0000, 48.0000, 63.0000, ~
## $ sibsp            &lt;int&gt; 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,~
## $ parch            &lt;int&gt; 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,~
## $ fare             &lt;dbl&gt; 211.3375, 151.5500, 151.5500, 151.5500, 26.5500, 77.9~
## $ survived         &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FA~
## $ pclass_X2nd      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ pclass_X3rd      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ sex_male         &lt;dbl&gt; 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,~
## $ parch_x_sex_male &lt;dbl&gt; 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,~
```

---
class: inverse, center, middle
# Transforming predictors
---
class: onecol
## Normalizing

Predictors with vastly different means and SDs can cause problems for some algorithms

--

![:emphasize](Centering) a predictor involves changing its mean to `\(0.0\)`

- This is accomplished by subtracting the mean from every observation 

--

![:emphasize](Scaling) a predictor involves changing its standard deviation (and variance) to `\(1.0\)`

- This is accomplished by dividing each observation by the standard deviation

--

![:emphasize](Normalizing) a predictor involves centering it and then scaling it

- This is also sometimes called "standardizing" or `\(z\)`-scoring the predictor

---
## Centering Visualized

&lt;img src="Day_2A_files/figure-html/centering-1.png" width="100%" /&gt;

.footnote[The mean is now 0 but the shape and SD of the distribution are unchanged (i.e., it has been shifted left).]

---
## Scaling Visualized

&lt;img src="Day_2A_files/figure-html/scaling-1.png" width="100%" /&gt;

.footnote[The SD is now 1 and the mean is lower, but the shape of the distribution is unchanged.]

---
## Normalizing Visualized

&lt;img src="Day_2A_files/figure-html/normalizing-1.png" width="100%" /&gt;

.footnote[The mean is now 0 and the SD is now 1, but the shape of the distribution is unchanged.]

---
## Normalizing in R


```r
# Normalize the age variable using the training set mean and SD
norm_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_normalize(age) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_normalize (normalize_mis2Z): same number of columns
```

--


```r
# Because of prep(), bake() uses the training set mean and SD to normalize the testing data¬≤
norm_test &lt;- bake(norm_recipe, new_data = titanic_test)
```


.footnote[
[1] We could also have used `step_center()` and/or `step_scale()` instead of `step_normalize()`.&lt;br /&gt;
[2] This is important to accurately estimating out-of-sample performance on truly novel data.
]

---
class: onecol
## Addressing Non-normality

A ![:emphasize](skewed) distribution is one that is not symmetric (i.e., it has a "heavy tail")

A ![:emphasize](bounded) distribution is one that cannot go beyond certain boundary values

&lt;img src="Day_2A_files/figure-html/skew-1.png" width="90%" /&gt;

---
class: onecol
## Addressing Non-normality
Specific transformations (e.g., log, inverse, logit) can help address specific issues

The Box-Cox and Yeo-Johnson approaches employ **families of transformations**

Box-Cox cannot be applied to negative or zero values, but ![:emphasize](Yeo-Johnson) can

&lt;br /&gt;

`$$x_{(yj)}^\star=\begin{cases}((x+1)^\lambda-1)/\lambda &amp; \text{if } \lambda\ne0, x\ge0 \\
\log(x+1) &amp; \text{if } \lambda=0, x\ge0 \\
-[(-x+1)^{2-\lambda}-1)]/(2-\lambda) &amp; \text{if } \lambda\ne2, x&lt;0 \\
-\log(-x+1) &amp; \text{if } \lambda=2, x&lt;0
\end{cases}$$`

---
## Addressing Non-normality in R


```r
# Add step to apply the Yeo-Johnson transformation to fare
yj_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_YeoJohnson(fare) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_YeoJohnson (YeoJohnson_rxA3z): same number of columns
```

.footnote[[1] If you would like to use specific transformations, use: `step_log()`, `step_inverse()`, `step_sqrt()`, etc.&lt;br /&gt;[2] As with normalizing, use `prep` to estimate Œª from training set and use it when you `bake` the testing set.]

---
class: onecol
## Addressing Non-normality in R

Bake the recipe using the training data and then plot the transformed variable

&lt;img src="Day_2A_files/figure-html/yjfare-1.png" width="100%" /&gt;

---
class: onecol
## Adding Nonlinearity

Many relationships between features and labels are non-linear in nature
- *e.g., perhaps survival was lowest for young adults and higher for children and elders*

&lt;p style="padding-top:25px;"&gt;Successful prediction will require us to &lt;b&gt;model that nonlinearity&lt;/b&gt; in such cases&lt;/p&gt;

--

&lt;p style="padding-top:25px;"&gt;Many algorithms can capture nonlinearity easily but others need our help&lt;/p&gt;

- For these algorithms, we can provide help through feature engineering

- This typically means adding ![:emphasize](nonlinear expansions) of existing predictors&lt;sup&gt;1&lt;/sup&gt;

.footnote[[1] If you are familiar with polynomial (e.g., quadratic or cubic) regression, you already have relevant experience!]

---
## Adding Nonlinearity in R


```r
# Add step to add orthogonal polynomial basis functions
nl_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_poly(age, degree = 2) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_poly (poly_YXyR0): 
##  new (2): age_poly_1, age_poly_2
##  removed (1): age
```

.footnote[[1] Note that, by specifying `degree = 2`, we are creating a quadratic expansion; more flexibility can be added.&lt;br /&gt;[2] Additional nonlinear expansions are also available: `step_ns()`, `step_bs()`, and `step_hyperbolic()`.]

---
class: onecol
## Adding Nonlinearity in R

Bake the recipe and plot the polynomial terms against one another (with vertical jitter).

&lt;img src="Day_2A_files/figure-html/poly-1.png" width="100%" /&gt;

---
class: inverse, center, middle
# Reducing predictors
---
class: onecol
## Zero and Near-Zero Variance Predictors
![:emphasize](Zero variance predictors) take on only a single value in the sample

- These predictors are **uninformative** and may lead to **modeling problems**

--

![:emphasize](Near-zero variance predictors) take on only a few unique values with low frequencies

- These predictors can easily become zero-variance predictors during resampling

--

&lt;p style="padding-top:25px;"&gt;For many algorithms, we want to &lt;b&gt;detect&lt;/b&gt; and &lt;b&gt;remove&lt;/b&gt; both types of predictors&lt;/p&gt;

(This may not be necessary for algorithms with built-in *predictor selection*)

---
## Zero and Nero-Zero Variance Predictors in R


```r
# Calculate predictors then detect and remove zero-variance predictors
zv_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
  step_mutate(species = "homo sapiens", under20 = age &lt; 20, over70 = age &gt; 70) %&gt;% 
* step_zv(all_predictors()) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_mutate (mutate_RKJVh): 
##  new (3): species, under20, over70
## 
## step_zv (zv_ygnwG): 
##  removed (1): species
```

---
## Zero and Nero-Zero Variance Predictors in R


```r
# Calculate predictors then detect and remove near-zero-variance predictors
nzv_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
  step_mutate(species = "homo sapiens", under20 = age &lt; 20, over70 = age &gt; 70) %&gt;% 
* step_nzv(all_predictors()) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_mutate (mutate_bLQdB): 
##  new (3): species, under20, over70
## 
## step_nzv (nzv_8gOiO): 
##  removed (2): species, over70
```

---
class: onecol
## Multicollinearity

![:emphasize](Highly correlated predictors) can lead to problems for some algorithms/procedures

- The model has to randomly choose between the predictors, leading to **instability**
- Model predictions may be fine, but model **interpretation** will often be obfuscated
- The cutoff for "problematically high" correlations varies (e.g., 0.5 to 0.9 or higher)

--

Predictors that are ![:emphasize](linear combinations) of other predictors are similarly problematic

- Occurs if a predictor variable can be predicted from the other predictor variables
- This is why dummy coding is preferred to one-hot encoding for some algorithms

--

&lt;p style="padding-top:25px;"&gt;For many algorithms, we want to &lt;b&gt;detect&lt;/b&gt; and &lt;b&gt;remove&lt;/b&gt; redundant predictors&lt;/p&gt;

(This may not be necessary for algorithms with *regularization* or *predictor selection*)

---
class: onecol
## Multicollinearity in R


```r
# Add some predictors with high correlations and linear dependency
mc_titanic &lt;- titanic %&gt;% mutate(
  wisdom = 100 + 0.25 * age + rnorm(nrow(.)), # highly correlated with age
  family = sibsp + parch                    # linear combo of sibsp and parch
)
mc_train &lt;- mc_titanic[index, ]
```

--


```r
glimpse(mc_train)
```

```
## Rows: 772
## Columns: 9
## $ survived &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TRU~
## $ pclass   &lt;chr&gt; "1st", "1st", "1st", "1st", "1st", "1st", "1st", "1st", "1st"~
## $ sex      &lt;chr&gt; "female", "male", "male", "female", "male", "female", "female~
## $ age      &lt;dbl&gt; 29.0000, 0.9167, 30.0000, 25.0000, 48.0000, 63.0000, 53.0000,~
## $ sibsp    &lt;int&gt; 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0~
## $ parch    &lt;int&gt; 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0~
## $ fare     &lt;dbl&gt; 211.3375, 151.5500, 151.5500, 151.5500, 26.5500, 77.9583, 51.~
## $ wisdom   &lt;dbl&gt; 108.8152, 100.3019, 107.9118, 106.1897, 111.5347, 114.7993, 1~
## $ family   &lt;int&gt; 0, 3, 3, 3, 0, 1, 2, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 2, 0, 0, 0~
```

---
class: onecol
## Multicollinearity in R


```r
# Estimate correlation matrix for the numeric predictors
mc_train %&gt;% select(where(is.numeric), -survived) %&gt;% cor() %&gt;% round(digits = 2)
```

```
##          age sibsp parch fare wisdom family
## age     1.00 -0.07  0.02 0.20   0.96  -0.02
## sibsp  -0.07  1.00  0.31 0.18  -0.06   0.80
## parch   0.02  0.31  1.00 0.24   0.03   0.82
## fare    0.20  0.18  0.24 1.00   0.21   0.26
## wisdom  0.96 -0.06  0.03 0.21   1.00  -0.02
## family -0.02  0.80  0.82 0.26  -0.02   1.00
```

---
class: onecol
## Multicollinearity in R


```r
# Detect and remove predictors that are highly correlated with another predictor
hc_recipe &lt;- 
  mc_titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_corr(all_numeric_predictors(), threshold = 0.9) %&gt;%
  prep(training = mc_train, log_changes = TRUE)
```

```
## step_corr (corr_mv45P): 
##  removed (1): age
```

--

.footnote[[1] If we want to consider correlations with categorical variables, we can add step_dummy() to the pipeline.&lt;br /&gt;[2] We could have also lowered the threshold to 0.8 in order to drop the `family` variable here.]
---
class: onecol
## Multicollinearity in R


```r
# Detect and remove predictors that are linear combinations of other predictors
lc_recipe &lt;- 
  mc_titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
  step_impute_knn(all_predictors()) %&gt;% 
* step_lincomb(all_numeric_predictors()) %&gt;%
  prep(training = mc_train, log_changes = TRUE)
```

```
## step_impute_knn (impute_knn_fNFcH): same number of columns
## 
## step_lincomb (lincomb_MtpG2): 
##  removed (1): family
```

---
class: onecol
## Dimensionality Reduction

Each feature/predictor included can be considered an additional "dimension"

![:emphasize](Dimensionality reduction) techniques try to find a smaller set of predictors to use

- If successful, little information from the original set of predictors will be lost
- Most techniques create new predictors as *functions of the original predictors*

--

**Principal Components Analysis** (PCA) is a commonly used technique

- The new predictors (PCs) are *linear combinations* of the original predictors
- The PCs are *uncorrelated* with one another, thus addressing multicollinearity
- PCs are extracted until a target amount of variability is explained (e.g., 75%)

.footnote[[1] Predictors should be normalized (i.e., centered and scaled) before PCA is used.&lt;br /&gt;[2] PCA is unsupervised (doesn't know the labels) but there are supervised  techniques.]

---
## Dimensionality Reduction in R


```r
# Normalize the numeric predictors and then do PCA
pca_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_normalize(all_numeric_predictors()) %&gt;%
* step_pca(all_numeric_predictors(), threshold = 0.75) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_normalize (normalize_JsWUT): same number of columns
## 
## step_pca (pca_coi59): 
##  new (3): PC1, PC2, PC3
##  removed (4): age, sibsp, parch, fare
```

.footnote[
[1] Note that PCA is most effective when there are many correlated predictors, so this is a weak use of it.&lt;br /&gt;
[2] Note that higher thresholds (e.g., 0.99) are often used with algorithms that can handle lots of predictors.&lt;br /&gt;
[3] `step_kpca_poly()` and `step_kpca_rbf()` are nonlinear, and `step_pls()` is supervised.]

---
class: inverse, center, middle
# Advanced Topics
---
## Feature Extraction

---
## Dealing with Missing Values

---
## Feature Selection


---
class: onecol
## Recommendations

Algorithm | Preprocessing Steps
:-------- | :------------------
Regularized Regression&amp;emsp; | `dummy`, `nzv`, `impute`, `corr`,  `lincomb`, (`transform`)&amp;emsp;
Decision Tree | (`decorrelate`)
Random Forest | (`nzv`), (`impute`), (`corr`)

For these algorithms, normalization is only really necessary for interpretation

However, other algorithms (e.g., MLP and SVM) may require normalization

To learn more about feature engineering, see [1]

.footnote[[1] Kuhn &amp; Johnson (2020). *Feature Engineering and Selection: A Practical Approach for Predictive Models.* CRC Press.]

---
class: inverse, center, middle
# Live Coding
---
class: inverse, center, middle
# Time for a Break!

<div class="countdown" id="timer_60eb0daa" style="right:33%;bottom:15%;left:33%;" data-warnwhen="60">
<code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current% / %total%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
