<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Feature Engineering</title>
    <meta charset="utf-8" />
    <meta name="author" content="Applied Machine Learning in R  Pittsburgh Summer Methodology Series" />
    <script src="Day_2A_files/header-attrs/header-attrs.js"></script>
    <link href="Day_2A_files/countdown/countdown.css" rel="stylesheet" />
    <script src="Day_2A_files/countdown/countdown.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <span style="font-size:48pt;">Feature Engineering</span>
## .big[üë∑ üóÑÔ∏è üõ†Ô∏è]
### Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series
### Lecture 2-A ‚ÄÉ ‚ÄÉ July 20, 2021

---








class: inverse, center, middle
# Overview

&lt;style type="text/css"&gt;
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
&lt;/style&gt;
---
class: onecol
## Feature Engineering
.left-column[
&lt;br /&gt;
&lt;img src="engineer.jpg" width="100%" /&gt;
]
.right-column[
**Prepare the predictors for analysis**
- *Extract* predictors
- *Transform* predictors
- *Re-encode* predictors
- *Combine* predictors
- *Reduce* predictor dimensionality
- *Impute* missing predictor values
- *Select* and drop predictors
]
---
class: onecol

## Motivation

**Features** are descriptions of the data points that help to predict the outcomes

- We may need to extract features from "raw" or "low-level" data (e.g., images)
- We may need to address issues with missing data and feature distributions

--

There are many potential ways to **encode** or "represent" the features/predictors

- e.g., adding, dropping, transforming, and combining predictors&lt;sup&gt;1&lt;/sup&gt;
- predictor encoding can have a big ![:emphasize](impact on predictive performance)&lt;sup&gt;2&lt;/sup&gt;
- The optimal encoding depends on both the **algorithm** and the **relationships**

.footnote[
[1] Some algorithms can learn their own, complex feature representations&lt;br /&gt;
[2] Some algorithms are more sensitive to feature encoding than others
]

---
class: onecol
## Examples of Feature Encodings
.left-column[
&lt;br /&gt;

&lt;img src="july.jpg" width="100%" /&gt;

]
.right-column[
**When an event or observation occurred**

- The numeric year *(2021)*
- The numeric month *(7)*
- The numeric day of the month *(20)*
- The numeric day of the year *(201)*
- Days since a reference *(diagnosis +2)*
- The day of the week *(Tuesday)*
- The season of the year *(Summer)*
- The type of day *(weekday)* 
- The presence of a holiday *(FALSE)*

]
---
class: onecol
## Issues to Navigate in Feature Engineering

.left-column[
&lt;br /&gt;
&lt;img src="cones.jpg" width="100%" /&gt;
]
.right-column[
- predictors with **non-normal** distributions
- predictors with vastly **different scales**
- predictors with extreme **outliers**
- predictors with **missing** or censored values
- predictors that are **correlated** with one another
- predictors that are **redundant** with one another
- predictors that have zero or **near-zero variance**
- predictors that are **uninformative** for a task
- predictors with **uncertainty** or unreliability
]

---

class: onecol
## Recipes for Feature Engineering

.left-column[
&lt;br /&gt;
&lt;img src="chef.jpg" width="100%" /&gt;

]

.right-column[
All of the predictor engineering steps can be done "by hand" in R

The `caret` package provides some basic convenience tools

We will be learning the &lt;b&gt;`recipes`&lt;/b&gt; package from `tidymodels`

1. Initiate a recipe by declaring data and roles using `recipe()`
1. Add one or more preprocessing steps using `step_*()`
1. Prepare/estimate the preprocessing steps using `prep()`
1. Apply these steps to the training and testing data with `bake()`
]

---
## Example Dataset: `titanic`


```r
# Import and preview the full titanic dataset
titanic &lt;- read.csv("titanic.csv")
glimpse(titanic)
```

```
## Rows: 1,309
## Columns: 8
## $ pclass   &lt;chr&gt; "1st", "1st", "1st", "1st", "1st", "1st", "1st", "1st", "1st"~
## $ survived &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FAL~
## $ sex      &lt;chr&gt; "female", "male", "female", "male", "female", "male", "female~
## $ age      &lt;dbl&gt; 29.0000, 0.9167, NA, 30.0000, 25.0000, 48.0000, 63.0000, 39.0~
## $ sibsp    &lt;int&gt; 0, 1, 1, 1, 1, 0, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1~
## $ parch    &lt;int&gt; 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1~
## $ fare     &lt;dbl&gt; 211.3375, 151.5500, 151.5500, 151.5500, 151.5500, 26.5500, 77~
## $ embarked &lt;chr&gt; "Southampton", "Southampton", "Southampton", "Southampton", "~
```

.pull-left[
- `pclass`: *Passenger class*
- `survived`: *Did passenger survive?*
- `sex`: *Passenger sex*
- `age`: *Passenger age (years)*
]

.pull-right[
- `sibsp`: *Siblings/spouses Aboard (\#)*
- `parch`: *Parents/children Aboard (\#)*
- `fare`: *Cost of passenger fare ($)*
- `embarked`: *Port of embarkation*
]

---
## Recipes for Feature Engineering


```r
# First, let's split the data into a training and testing set
index &lt;- caret::createDataPartition(y = titanic$survived, list = FALSE, p = 0.8)
titanic_train &lt;- titanic[index, ]
titanic_test &lt;- titanic[-index, ]
```

--


```r
# Initiate a recipe for predicting survived from all other variables
titanic_recipe &lt;- recipe(titanic, formula = survived ~ .)

# We could also be explicit with: formula = survived ~ pclass + sex + age + sibsp + parch + fare + embarked
```


```r
titanic_recipe %&gt;% print()
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          7
```


---
## Recipes for Feature Engineering


```r
# summary() provides a bit more information than print()
titanic_recipe %&gt;% summary()
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; variable &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; type &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; role &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; source &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; pclass &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nominal &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sex &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nominal &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; age &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sibsp &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; parch &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; fare &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; embarked &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nominal &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; survived &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; logical &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; outcome &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

--

&lt;p style="padding-top:25px;"&gt;Now we are ready to add some preprocessing (i.e., feature engineering) steps to the recipe!&lt;/p&gt;

---
class: onecol
## Common Steps

.pull-left[
- **Adding predictors**
  - Calculated predictors
  - Categorical predictors
  - Interaction Terms
- **Transforming predictors**
  - Centering and Scaling
  - Resolving Skewness
  - Addressing Outliers
]
.pull-right[

- **Reducing predictors**
  - Nero-Zero Variance
  - Multicollinearity
  - Dimensionality Reduction
- **Advanced Steps**
  - Feature Extraction
  - Dealing with Missing Values
  - Feature Selection
]

---
class: inverse, center, middle
# Adding predictors
---
class: onecol
## Calculated predictors

Some variables will need to be calculated from existing values and variables

- You may choose to score an instrument from item-level data
- You may choose to encode a predictor as the ratio of two values
- You may choose to calculate sums, means, counts, proportions, etc.

--

&lt;p style="padding-top:25px;"&gt;We will show you some basic steps for calculating variables within {recipes}&lt;/p&gt;

For more advanced/complex data wrangling, we recommend you read

- &lt;i&gt;[R for Data Science: Visualize, Model, Transform, Tidy, and Import Data](https://r4ds.had.co.nz/)&lt;/i&gt;&lt;br /&gt;by Wickham and Grolemund (book for purchase or online for free)

---
## Calculated predictors


```r
# Add a step to the recipe to calculate new predictors from existing predictors
cp_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_mutate(
*   numfamily = sibsp + parch,
*   over70 = age &gt; 70
* ) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_mutate (mutate_pEouV): 
##  new (2): numfamily, over70
```

---
## Calculated predictors


```r
# We can ask for a summary of cp_recipe to see what happened
cp_recipe %&gt;% summary()
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; variable &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; type &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; role &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; source &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; pclass &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nominal &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sex &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nominal &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; age &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sibsp &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; parch &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; fare &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; embarked &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nominal &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; survived &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; logical &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; outcome &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; original &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; numfamily &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; numeric &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; derived &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; over70 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; logical &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictor &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; derived &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
## Calculated predictors


```r
# If this is the only step in our recipe, we can bake() the recipe
# This will allow us to generate updated training and testing sets
cp_train &lt;- bake(cp_recipe, new_data = titanic_train)
cp_test &lt;- bake(cp_recipe, new_data = titanic_test)
```

--


```r
# Let's see if it worked
glimpse(cp_test)
```

```
## Rows: 261
## Columns: 10
## $ pclass    &lt;fct&gt; 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, ~
## $ sex       &lt;fct&gt; female, male, male, male, female, male, female, male, male, ~
## $ age       &lt;dbl&gt; 63, 39, 25, 42, 22, 36, 30, NA, 47, 39, 55, 64, 51, 31, 23, ~
## $ sibsp     &lt;int&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 3, 3, 0, ~
## $ parch     &lt;int&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 2, 0, ~
## $ fare      &lt;dbl&gt; 77.9583, NA, 91.0792, 26.5500, 55.0000, 512.3292, 86.5000, N~
## $ embarked  &lt;fct&gt; Southampton, Southampton, Cherbourg, Southampton, Southampto~
## $ survived  &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TR~
## $ numfamily &lt;int&gt; 1, 0, 1, 0, 1, 1, 0, 0, 0, 2, 2, 2, 0, 1, 1, 0, 0, 5, 5, 0, ~
## $ over70    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, NA, FALSE, ~
```

---
class: onecol
## Categorical predictors

Categorical predictors can be re-encoded into multiple binary (0 or 1) predictors

In `titanic`, the categorical variable &lt;b&gt;`sex`&lt;/b&gt; takes on the value *male* or *female*

--

.pull-left[
.center[![:emphasize](Dummy Coding)]

sex | sex_male
:---|:-------:
female | 0  
male   | 1 

&lt;p style="text-align:center;font-size:20px;"&gt;Efficient and avoids redundancy&lt;br /&gt;Good for GLM-based methods&lt;/p&gt;
]

--

.pull-right[
.center[![:emphasize](One-Hot Encoding)]

sex | sex_female | sex_male
:---|:----------:|:-------:
female | 1  | 0 
male   | 0  | 1

&lt;p style="text-align:center;font-size:20px;"&gt;Simple and easy to interpret&lt;br /&gt;Good for tree-based methods&lt;/p&gt;
]


---
## Categorical predictors in R


```r
# Add a step to the recipe to create dummy codes for pclass, sex, and embarked
dc_recipe &lt;- 
  titanic %&gt;%
  recipe(survived ~ .) %&gt;% 
* step_dummy(pclass, sex, embarked) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_dummy (dummy_2exuH): 
##  new (5): pclass_X2nd, pclass_X3rd, sex_male, ...
##  removed (3): pclass, sex, embarked
```

--

.footnote[[1] As a shortcut, we could also have used `step_dummy(all_nominal_predictors())`.]

---
## Categorical predictors in R


```r
# It is easy to modify this recipe for one-hot encoding
oh_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_dummy(pclass, sex, embarked, one_hot = TRUE) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_dummy (dummy_bQilX): 
##  new (8): pclass_X1st, pclass_X2nd, pclass_X3rd, sex_female, sex_male, ...
##  removed (3): pclass, sex, embarked
```

.footnote[[1] Note that step_dummy() is used for both dummy coding and one-hot encoding (with different arguments).]

---
class: onecol
## Interaction Terms

Interaction terms allow the meaning of one predictor to depend on other predictors

In this way, interaction terms allow predictor "effects" to be **contingent** or **conditional**

e.g., perhaps having parents or children on board the Titanic helps you predict survival... but the effects differs depending on whether the passenger is a man or a woman

--

&lt;p style="padding-top:25px;"&gt;Interaction terms are literally &lt;b&gt;products&lt;/b&gt; (i.e., multiplications) of two or more predictors&lt;/p&gt;

In order to include categorical variables in interaction terms, dummy code them first

`$$survived \sim parch + sex\_dummy + parch \times sex\_dummy$$`

---
## Interaction Terms in R


```r
# Add interaction term between parch and the sex_male dummy code
it_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
  step_dummy(pclass, sex) %&gt;% 
* step_interact(~ parch:sex_male) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_dummy (dummy_jCTgV): 
##  new (3): pclass_X2nd, pclass_X3rd, sex_male
##  removed (2): pclass, sex
## 
## step_interact (interact_WB5qk): 
##  new (1): parch_x_sex_male
```

.footnote[[1] step_interact() requires you to use tilde (`~`) and the colon operator (`:`) to specify interaction terms.&lt;br /&gt;[2] We also need to anticipate what the name of the dummy code variable will be and include that.&lt;br /&gt;[3] We could include higher-order interactions by adding more colons and predictors (e.g., `x1:x2:x3`).]

---
## Interaction Terms in R


```r
# Bake the recipe and preview the updated training set
bake(it_recipe, new_data = titanic_train) %&gt;% glimpse()
```

```
## Rows: 1,048
## Columns: 10
## $ age              &lt;dbl&gt; 29.0000, 0.9167, NA, 30.0000, 25.0000, 48.0000, 53.00~
## $ sibsp            &lt;int&gt; 0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ parch            &lt;int&gt; 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,~
## $ fare             &lt;dbl&gt; 211.3375, 151.5500, 151.5500, 151.5500, 151.5500, 26.~
## $ embarked         &lt;fct&gt; Southampton, Southampton, Southampton, Southampton, S~
## $ survived         &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F~
## $ pclass_X2nd      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ pclass_X3rd      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ sex_male         &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,~
## $ parch_x_sex_male &lt;dbl&gt; 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,~
```

---
class: inverse, center, middle
# Transforming predictors
---
class: onecol
## Normalizing

Predictors with vastly different means and SDs can cause problems for some algorithms

--

![:emphasize](Centering) a predictor involves changing its mean to `\(0.0\)`

- This is accomplished by subtracting the mean from every observation 

--

![:emphasize](Scaling) a predictor involves changing its standard deviation (and variance) to `\(1.0\)`

- This is accomplished by dividing each observation by the standard deviation

--

![:emphasize](Normalizing) a predictor involves centering it and then scaling it

- This is also sometimes called "standardizing" or `\(z\)`-scoring the predictor

---
## Centering Visualized

&lt;img src="Day_2A_files/figure-html/centering-1.png" width="100%" /&gt;

.footnote[The mean is now 0 but the shape and SD of the distribution are unchanged (i.e., it has been shifted left).]

---
## Scaling Visualized

&lt;img src="Day_2A_files/figure-html/scaling-1.png" width="100%" /&gt;

.footnote[The SD is now 1 and the mean is lower, but the shape of the distribution is unchanged.]

---
## Normalizing Visualized

&lt;img src="Day_2A_files/figure-html/normalizing-1.png" width="100%" /&gt;

.footnote[The mean is now 0 and the SD is now 1, but the shape of the distribution is unchanged.]

---
## Normalizing in R


```r
# Normalize the age variable using the training set mean and SD
norm_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_normalize(age) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_normalize (normalize_VmTuJ): same number of columns
```

--


```r
# Because of prep(), bake() uses the training set mean and SD to normalize the testing data¬≤
norm_test &lt;- bake(norm_recipe, new_data = titanic_test)
```


.footnote[
[1] We could also have used `step_center()` and/or `step_scale()` instead of `step_normalize()`.&lt;br /&gt;
[2] This is important to accurately estimating out-of-sample performance on truly novel data.
]

---
## Resolving Skewness

---
## Resolving Skewness in R

---
## Addressing Outliers

---
## Addressing Outliers in R

---
class: inverse, center, middle
# Reducing predictors
---
class: onecol
## Zero and Near-Zero Variance Predictors
![:emphasize](Zero variance predictors) take on only a single value in the sample

- These predictors are **uninformative** and may lead to **modeling problems**

--

![:emphasize](Near-zero variance predictors) take on only a few unique values with low frequencies

- These predictors can easily become zero-variance predictors during resampling

--

&lt;p style="padding-top:25px;"&gt;For many algorithms, we want to &lt;b&gt;detect&lt;/b&gt; and &lt;b&gt;remove&lt;/b&gt; both types of predictors&lt;/p&gt;

(This may not be necessary for algorithms with built-in *predictor selection*)

---
## Zero and Nero-Zero Variance Predictors in R


```r
# Calculate predictors then detect and remove zero-variance predictors
zv_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
  step_mutate(species = "homo sapiens", under20 = age &lt; 20, over70 = age &gt; 70) %&gt;% 
* step_zv(all_predictors()) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_mutate (mutate_JRyPZ): 
##  new (3): species, under20, over70
## 
## step_zv (zv_cPbmC): 
##  removed (1): species
```

---
## Zero and Nero-Zero Variance Predictors in R


```r
# Calculate predictors then detect and remove near-zero-variance predictors
nzv_recipe &lt;- 
  titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
  step_mutate(species = "homo sapiens", under20 = age &lt; 20, over70 = age &gt; 70) %&gt;% 
* step_nzv(all_predictors()) %&gt;%
  prep(training = titanic_train, log_changes = TRUE)
```

```
## step_mutate (mutate_f9Pow): 
##  new (3): species, under20, over70
## 
## step_nzv (nzv_BbHw8): 
##  removed (2): species, over70
```

---
class: onecol
## Multicollinearity

![:emphasize](Highly correlated predictors) can lead to problems for some algorithms/procedures

- The model has to randomly choose between the predictors, leading to **instability**
- Model predictions may be fine, but model **interpretation** will often be obfuscated
- The cutoff for "problematically high" correlations varies (e.g., 0.5 to 0.9 or higher)

--

Predictors that are ![:emphasize](linear combinations) of other predictors are similarly problematic

- Occurs if a predictor variable can be predicted from the other predictor variables
- This is why dummy coding is preferred to one-hot encoding for some algorithms

--

&lt;p style="padding-top:25px;"&gt;For many algorithms, we want to &lt;b&gt;detect&lt;/b&gt; and &lt;b&gt;remove&lt;/b&gt; redundant predictors&lt;/p&gt;

(This may not be necessary for algorithms with *regularization* or *predictor selection*)

---
class: onecol
## Multicollinearity in R


```r
# Add some predictors with high correlations and linear dependency
mc_titanic &lt;- titanic %&gt;% mutate(
  wisdom = 100 + 0.25 * age + rnorm(nrow(.)), # highly correlated with age
  family = sibsp + parch                    # linear combo of sibsp and parch
)
mc_train &lt;- mc_titanic[index, ]
```

--


```r
glimpse(mc_train)
```

```
## Rows: 1,048
## Columns: 10
## $ pclass   &lt;chr&gt; "1st", "1st", "1st", "1st", "1st", "1st", "1st", "1st", "1st"~
## $ survived &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TR~
## $ sex      &lt;chr&gt; "female", "male", "female", "male", "female", "male", "female~
## $ age      &lt;dbl&gt; 29.0000, 0.9167, NA, 30.0000, 25.0000, 48.0000, 53.0000, 71.0~
## $ sibsp    &lt;int&gt; 0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0~
## $ parch    &lt;int&gt; 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0~
## $ fare     &lt;dbl&gt; 211.3375, 151.5500, 151.5500, 151.5500, 151.5500, 26.5500, 51~
## $ embarked &lt;chr&gt; "Southampton", "Southampton", "Southampton", "Southampton", "~
## $ wisdom   &lt;dbl&gt; 104.1660, 99.6082, NA, 106.7358, 106.8009, 111.5884, 112.9914~
## $ family   &lt;int&gt; 0, 3, 3, 3, 3, 0, 2, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 2, 0~
```

---
class: onecol
## Multicollinearity in R


```r
# Estimate correlation matrix for the numeric predictors
mc_train %&gt;% select(where(is.numeric), -survived) %&gt;% cor() %&gt;% round(digits = 2)
```

```
##        age sibsp parch fare wisdom family
## age      1    NA    NA   NA     NA     NA
## sibsp   NA  1.00  0.39   NA     NA   0.87
## parch   NA  0.39  1.00   NA     NA   0.79
## fare    NA    NA    NA    1     NA     NA
## wisdom  NA    NA    NA   NA      1     NA
## family  NA  0.87  0.79   NA     NA   1.00
```

---
class: onecol
## Multicollinearity in R


```r
# Detect and remove predictors that are highly correlated with another predictor
hc_recipe &lt;- 
  mc_titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
* step_corr(all_numeric_predictors(), threshold = 0.9) %&gt;%
  prep(training = mc_train, log_changes = TRUE)
```

```
## step_corr (corr_exq9V): 
##  removed (1): wisdom
```

--

.footnote[[1] If we want to consider correlations with categorical variables, we can add step_dummy() to the pipeline.&lt;br /&gt;[2] We could have also lowered the threshold to 0.8 in order to drop the `family` variable here.]
---
class: onecol
## Multicollinearity in R


```r
# Detect and remove predictors that are linear combinations of other predictors
lc_recipe &lt;- 
  mc_titanic %&gt;% 
  recipe(survived ~ .) %&gt;% 
  step_impute_knn(all_predictors()) %&gt;% 
* step_lincomb(all_numeric_predictors()) %&gt;%
  prep(training = mc_train, log_changes = TRUE)
```

```
## step_impute_knn (impute_knn_O7b6U): same number of columns
## 
## step_lincomb (lincomb_1HFBD): 
##  removed (1): family
```

---
## Dimensionality Reduction

---
## Dimensionality Reduction in R

---
class: inverse, center, middle
# Advanced Topics
---
## Feature Extraction

---
## Dealing with Missing Values

---
## Feature Selection


---
class: onecol
## Recommendations

Algorithm | Preprocessing Steps
:-------- | :------------------
Regularized Regression&amp;emsp; | `dummy`, `nzv`, `impute`, `corr`,  `lincomb`, (`transform`)&amp;emsp;
Decision Tree | (`decorrelate`)
Random Forest | (`nzv`), (`impute`), (`corr`)

For these algorithms, normalization is only really necessary for interpretation

However, other algorithms (e.g., MLP and SVM) may require normalization

To learn more about feature engineering, see [1]

.footnote[[1] Kuhn &amp; Johnson (2020). *Feature Engineering and Selection: A Practical Approach for Predictive Models.* CRC Press.]

---
class: inverse, center, middle
# Live Coding
---
class: inverse, center, middle
# Time for a Break!

<div class="countdown" id="timer_60e9f5f8" style="right:33%;bottom:15%;left:33%;" data-warnwhen="60">
<code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current% / %total%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
