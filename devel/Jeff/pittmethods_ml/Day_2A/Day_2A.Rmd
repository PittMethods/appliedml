---
title: '<span style="font-size:48pt;">Feature Engineering</span>'
subtitle: '.big[üë∑ üóÑÔ∏è üõ†Ô∏è] ' 
author: 'Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series'
date: 'Lecture 2-A &emsp; &emsp; July 20, 2021'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  footnote_font_size = "20px",
  footnote_color = "gray",
  text_slide_number_font_size = "18px"
)
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(caret)
library(recipes)
```

class: inverse, center, middle
# Overview

<style type="text/css">
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
</style>
---
class: onecol
## Feature Engineering
.left-column[
<br />
```{r engineer, echo=FALSE}
include_graphics("engineer.jpg")
```
]
.right-column[
**Prepare the features for analysis**
- *Extract* features
- *Transform* features
- *Re-encode* features
- *Combine* features
- *Reduce* feature dimensionality
- *Impute* missing feature values
- *Select* and drop features
]
---
class: onecol

## Motivation

**Features** are descriptions of the data points that help to predict the labels

- We may need to extract features from "raw" or "low-level" data (e.g., images)
- We may need to address issues with missing data and feature distributions

--

There are many potential ways to **encode** or "represent" the features

- e.g., adding, dropping, transforming, and combining features<sup>1</sup>
- Feature encoding can have a big ![:emphasize](impact on predictive performance)<sup>2</sup>
- The optimal encoding depends on both the **algorithm** and the **relationships**

.footnote[
[1] Some algorithms can learn their own, complex feature representations<br />
[2] Some algorithms are more sensitive to feature encoding than others
]

---
class: onecol
## Examples of Feature Encodings
.left-column[
<br />

```{r, echo=FALSE}
include_graphics("july.jpg")
```

]
.right-column[
**When an event or observation occurred**

- The numeric year *(2021)*
- The numeric month *(7)*
- The numeric day of the month *(20)*
- The numeric day of the year *(201)*
- Days since a reference *(diagnosis +2)*
- The day of the week *(Tuesday)*
- The season of the year *(Summer)*
- The type of day *(weekday)* 
- The presence of a holiday *(FALSE)*

]
---
class: onecol
## Issues to Navigate in Feature Engineering
.left-column[
<br />
```{r, echo=FALSE}
include_graphics("cones.jpg")
```
]
.right-column[
- Features with **non-normal** distributions
- Features with vastly **different scales**
- Features with extreme **outliers**
- Features with **missing** or censored values
- Features that are **correlated** with one another
- Features that are **redundant** with one another
- Features that have zero or **near-zero variance**
- Features that are **uninformative** for a task
- Features with **uncertainty** or unreliability
]

---
class: onecol
## Lecture Topics

.pull-left[
- **Adding Features**
  - Calculated Features
  - Categorical Features
  - Interaction Terms
- **Transforming Features**
  - Centering and Scaling
  - Resolving Skewness
  - Addressing Outliers
]
.pull-right[

- **Reducing Features**
  - Nero-Zero Variance
  - Multicollinearity
  - Dimensionality Reduction
- **Advanced Topics**
  - Feature Extraction
  - Dealing with Missing Values
  - Feature Selection
]

---

## Example Dataset

```{r}
# Load and preview the etitanic dataset from the earth package
data(etitanic, package = "earth")
glimpse(etitanic)
```

---
class: inverse, center, middle
# Adding Features
---
class: onecol
## Calculated Features

Some variables will need to be calculated from existing values and variables

- You may choose to score an instrument from item-level data
- You may choose to encode a feature as the ratio of two values
- You may choose to calculate sums, means, counts, proportions, etc.

We will provide some example code in `base` R and `tidyverse` formats

But to learn more about data wrangling in R, we recommend the following

- <i>[R for Data Science: Visualize, Model, Transform, Tidy, and Import Data](https://r4ds.had.co.nz/)</i><br />by Wickham and Grolemund (book for purchase or online for free)

---
## Calculated Features in R
```{r}
# Read in and partition the data
set.seed(2021)
trainIndex <- caret::createDataPartition(iris$Species, p = 0.8, list = FALSE, times = 1)
irisTrain <- iris[trainIndex, ]

# Add calculated features using base R
irisTrain$Avg.Length <- (irisTrain$Sepal.Length + irisTrain$Petal.Length) / 2
irisTrain$Width.Ratio <- irisTrain$Sepal.Width / irisTrain$Petal.Width
```

```{r, echo=FALSE}
knitr::kable(irisTrain) %>% 
  kableExtra::scroll_box(height = "250px")
```

---

## Calculated Features in R

```{r}
# Add calculated features using tidyverse
library(tidyverse)
irisTrain <- irisTrain %>% 
  mutate(
    Avg.Length = (Sepal.Length + Petal.Length) / 2,
    Width.Ratio = Sepal.Width / Petal.Width
  )
```

```{r, echo=FALSE}
knitr::kable(irisTrain) %>% 
  kableExtra::scroll_box(height = "250px")
```

---
class: onecol
## Recipes for Feature Engineering

.left-column[
<br />
```{r, echo=FALSE}
include_graphics("chef.jpg")
```

]

.right-column[
All of the feature engineering steps can be done "by hand" in R

The `caret` package provides some basic convenience tools

We will be learning the <b>`recipes`</b> package from `tidymodels`

1. Initiate a recipe by declaring data and roles using `recipe()`
1. Add one or more preprocessing steps using `step_*()`
1. Prepare/estimate the preprocessing steps using `prep()`
1. Apply these steps to the training and testing data with `bake()`
]

---
## Recipes for Feature Engineering

```{r}
# First, let's split the data into a training and testing set
index <- caret::createDataPartition(y = etitanic$survived, list = FALSE, p = 0.8)
etitanic_train <- etitanic[index, ]
etitanic_test <- etitanic[-index, ]
```

```{r}
# Initiate a recipe for predicting survived from all other variables
titanic_recipe <- recipe(etitanic, formula = survived ~ .)

# We could also be explicit with: formula = survived ~ pclass + sex + age + sibsp + parch
```

```{r}
titanic_recipe %>% print()
```


---
## Recipes for Feature Engineering

```{r, eval=FALSE}
# summary() provides a bit more information than print()
titanic_recipe %>% summary()
```

```{r,echo=FALSE}
titanic_recipe %>% summary() %>% kable()
```

--

<p style="padding-top:25px;">Now we are ready to add some preprocessing (i.e., feature engineering) steps to the recipe!</p>

---
class: onecol
## Categorical Features

Categorical features can be re-encoded into multiple binary (0 or 1) features

In `etitanic`, the categorical variable <b>`sex`</b> takes on the value *male* or *female*

--

.pull-left[
.center[![:emphasize](Dummy Coding)]

sex | sex_male
:-----|:----:
female | 0  
male | 1 

<p style="text-align:center;font-size:20px;">Efficient and avoids redundancy<br />Good for GLM-based methods</p>
]

--

.pull-right[
.center[![:emphasize](One-Hot Encoding)]

sex | sex_female | sex_male
:-----|:----:|:----:
female | 1  | 0 
male | 0 | 1

<p style="text-align:center;font-size:20px;">Simple and easy to interpret<br />Good for tree-based methods</p>
]


---
## Categorical Features in R

```{r}
# Add a step to the recipe to create dummy codes for pclass and sex
dc_recipe <- 
  etitanic %>%
  recipe(survived ~ .) %>% 
  step_dummy(pclass, sex) %>% #<<
  prep(training = etitanic_train, log_changes = TRUE)
```

--

.footnote[[1] As a shortcut, we could also have used `step_dummy(all_nominal_predictors())`.]

---
## Categorical Features in R

```{r}
# If this were the only step we wanted to do, we could prep and bake the recipe
dc_train <- bake(dc_recipe, new_data = etitanic_train)
dc_test <- bake(dc_recipe, new_data = etitanic_test)
```

--

```{r}
# Let's see if it worked
glimpse(dc_test)
```

---
## Categorical Features in R

```{r}
# It is easy to modify this recipe for one-hot encoding
oh_recipe <- 
  etitanic %>% 
  recipe(survived ~ .) %>% 
  step_dummy(pclass, sex, one_hot = TRUE) %>% #<<
  prep(training = etitanic_train, log_changes = TRUE)
```

.footnote[[1] Note that step_dummy() is used for both dummy coding and one-hot encoding (with different arguments).]

---
## Interaction Terms

---
## Interaction Terms in R

---
class: inverse, center, middle
# Transforming Features
---
class: onecol
## Centering and Scaling

Definition of centering

Definition of scaling
---
## Centering and Scaling in R

---
## Resolving Skewness

---
## Resolving Skewness in R

---
## Addressing Outliers

---
## Addressing Outliers in R

---
class: inverse, center, middle
# Reducing Features
---
class: onecol
## Zero and Near-Zero Variance Predictors
![:emphasize](Zero variance predictors) take on only a single value in the sample

- These predictors are **uninformative** and may lead to **modeling problems**

--

![:emphasize](Near-zero variance predictors) take on only a few unique values with low frequencies

- These predictors can easily become zero-variance predictors during resampling

--

<p style="padding-top:25px;">For many algorithms, we want to <b>detect</b> and <b>remove</b> both types of predictors</p>

(This may not be necessary for algorithms with built-in *feature selection*)

---
## Zero and Nero-Zero Variance Predictors in R

```{r}
# Add some predictors with zero and near-zero variance
nzv_etitanic <- etitanic %>% mutate(species = "homo sapiens", over70 = age > 70)
nzv_train <- nzv_etitanic[index, ]
mean(nzv_train$over70)
```

--

```{r}
# Preview the simulated predictor variables
glimpse(nzv_etitanic)
```

---
class: onecol
## Zero and Nero-Zero Variance Predictors in R

```{r}
# Detect and remove zero-variance predictors
zv_recipe <- 
  nzv_etitanic %>% 
  recipe(survived ~ .) %>% 
  step_zv(all_predictors()) %>% #<<
  prep(training = nzv_train, log_changes = TRUE)
```

--

```{r}
# Detect and remove zero-variance AND near-zero-variance predictors
nzv_recipe <- 
  nzv_etitanic %>% 
  recipe(survived ~ .) %>% 
  step_nzv(all_predictors()) %>% #<<
  prep(training = nzv_train, log_changes = TRUE)
```

---
class: onecol
## Multicollinearity

![:emphasize](Highly correlated predictors) can lead to problems for some algorithms/procedures

- The model has to randomly choose between the predictors, leading to **instability**
- Model predictions may be fine, but model **interpretation** will often be obfuscated
- The cutoff for "problematically high" correlations varies (e.g., 0.5 to 0.9 or higher)

--

Predictors that are ![:emphasize](linear combinations) of other predictors are similarly problematic

- Occurs if a predictor variable can be predicted from the other predictor variables
- This is why dummy coding is preferred to one-hot encoding for some algorithms

--

<p style="padding-top:25px;">For many algorithms, we want to <b>detect</b> and <b>remove</b> redundant predictors</p>

(This may not be necessary for algorithms with *regularization* or *feature selection*)

---
class: onecol
## Multicollinearity in R

```{r}
# Add some predictors with high correlations and linear dependency
mc_etitanic <- etitanic %>% mutate(
  wisdom = 100 + 0.25 * age + rnorm(nrow(.)), # highly correlated with age
  family = sibsp + parch                    # linear combo of sibsp and parch
)
mc_train <- mc_etitanic[index, ]
```

--

```{r}
glimpse(mc_train)
```

---
class: onecol
## Multicollinearity in R

```{r}
# Estimate correlation matrix for the numeric predictors
mc_train %>% select(where(is.numeric), -survived) %>% cor() %>% round(digits = 2)
```

---
class: onecol
## Multicollinearity in R

```{r}
# Detect and remove predictors that are highly correlated with another predictor
hc_recipe <- 
  mc_etitanic %>% 
  recipe(survived ~ .) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9) %>% #<<
  prep(training = mc_train, log_changes = TRUE)
```

--

.footnote[[1] If we want to consider correlations with categorical variables, we can add step_dummy() to the pipeline.<br />[2] We could have also lowered the threshold to 0.8 in order to drop the `family` variable here.]
---
class: onecol
## Multicollinearity in R

```{r}
# Detect and remove predictors that are linear combinations of other predictors
lc_recipe <- 
  mc_etitanic %>% 
  recipe(survived ~ .) %>% 
  step_lincomb(all_numeric_predictors()) %>%  #<<
  prep(training = mc_train, log_changes = TRUE)
```

---
## Dimensionality Reduction

---
## Dimensionality Reduction in R

---
class: inverse, center, middle
# Advanced Topics
---
## Feature Extraction

---
## Dealing with Missing Values

---
## Feature Selection


---
class: onecol
## Recommendations

Algorithm | Preprocessing Steps
:-------- | :------------------
Regularized Regression | `dummy`, `nzv`, `impute`, `decorrelate`, (`transform`)
Decision Tree | (`decorrelate`)
Random Forest | (`nzv`), (`impute`), (`decorrelate`)

For these algorithms, normalization is only really necessary for interpretation

However, other algorithms (e.g., MLP and SVM) may require normalization

To learn more about feature engineering, see [1] and the [recipes](https://recipes.tidymodels.org/) package

.footnote[[1] Kuhn & Johnson (2020). *Feature Engineering and Selection: A Practical Approach for Predictive Models.* CRC Press.]

---
class: inverse, center, middle
# Live Coding
---
class: inverse, center, middle
# Time for a Break!

```{r countdown, echo=FALSE}
countdown(
  minutes = 10, 
  seconds = 0, 
  right = "33%", 
  left = "33%",
  bottom = "15%",
  color_background = "white",
  color_text = "black",
  color_running_background = "white",
  color_running_text = "black",
  warn_when = 60
)
```
