---
title: '<span style="font-size:48pt;">Feature Engineering</span>'
subtitle: '.big[üë∑ üóÑÔ∏è üõ†Ô∏è] ' 
author: 'Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series'
date: 'Lecture 2-A &emsp; &emsp; July 20, 2021'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  footnote_font_size = "20px",
  footnote_color = "gray",
  text_slide_number_font_size = "18px"
)
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(caret)
library(recipes)
```

class: inverse, center, middle
# Overview

<style type="text/css">
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
</style>
---
class: onecol
## Feature Engineering
.left-column[
<br />
```{r engineer, echo=FALSE}
include_graphics("engineer.jpg")
```
]
.right-column[
**Prepare the predictors for analysis**
- *Extract* predictors
- *Transform* predictors
- *Re-encode* predictors
- *Combine* predictors
- *Reduce* predictor dimensionality
- *Impute* missing predictor values
- *Select* and drop predictors
]
---
class: onecol

## Motivation

**Features** are descriptions of the data points that help to predict the outcomes

- We may need to extract features from "raw" or "low-level" data (e.g., images)
- We may need to address issues with missing data and feature distributions

--

There are many potential ways to **encode** or "represent" the features/predictors

- e.g., adding, dropping, transforming, and combining predictors<sup>1</sup>
- predictor encoding can have a big ![:emphasize](impact on predictive performance)<sup>2</sup>
- The optimal encoding depends on both the **algorithm** and the **relationships**

.footnote[
[1] Some algorithms can learn their own, complex feature representations<br />
[2] Some algorithms are more sensitive to feature encoding than others
]

---
class: onecol
## Examples of Feature Encodings
.left-column[
<br />

```{r, echo=FALSE}
include_graphics("july.jpg")
```

]
.right-column[
**When an event or observation occurred**

- The numeric year *(2021)*
- The numeric month *(7)*
- The numeric day of the month *(20)*
- The numeric day of the year *(201)*
- Days since a reference *(diagnosis +2)*
- The day of the week *(Tuesday)*
- The season of the year *(Summer)*
- The type of day *(weekday)* 
- The presence of a holiday *(FALSE)*

]
---
class: onecol
## Issues to Navigate in Feature Engineering

.left-column[
<br />
```{r, echo=FALSE}
include_graphics("cones.jpg")
```
]
.right-column[
- predictors with **non-normal** distributions
- predictors with vastly **different scales**
- predictors with extreme **outliers**
- predictors with **missing** or censored values
- predictors that are **correlated** with one another
- predictors that are **redundant** with one another
- predictors that have zero or **near-zero variance**
- predictors that are **uninformative** for a task
- predictors with **uncertainty** or unreliability
]

---

class: onecol
## Recipes for Feature Engineering

.left-column[
<br />
```{r, echo=FALSE}
include_graphics("chef.jpg")
```

]

.right-column[
All of the predictor engineering steps can be done "by hand" in R

The `caret` package provides some basic convenience tools

We will be learning the <b>`recipes`</b> package from `tidymodels`

1. Initiate a recipe by declaring data and roles using `recipe()`
1. Add one or more preprocessing steps using `step_*()`
1. Prepare/estimate the preprocessing steps using `prep()`
1. Apply these steps to the training and testing data with `bake()`
]

---
## Example Dataset: `titanic`

```{r}
# Import and preview the full titanic dataset
titanic <- read.csv("titanic.csv")
glimpse(titanic)
```

.pull-left[
- `survived`: *Did passenger survive? {FALSE, TRUE}*
- `pclass`: *Passenger class {1st, 2nd, 3rd}*
- `sex`: *Passenger sex {female, male}*
- `age`: *Passenger age (years)*
]

.pull-right[
- `sibsp`: *Siblings/spouses Aboard (\#)*
- `parch`: *Parents/children Aboard (\#)*
- `fare`: *Cost of passenger fare ($)*
]

---
## Recipes for Feature Engineering

```{r}
# First, let's split the data into a training and testing set
index <- caret::createDataPartition(y = titanic$survived, list = FALSE, p = 0.8)
titanic_train <- titanic[index, ]
titanic_test <- titanic[-index, ]
```

--

```{r}
# Initiate a recipe for predicting survived from all other variables
titanic_recipe <- recipe(titanic, formula = survived ~ .)

# We could also be explicit with: formula = survived ~ pclass + sex + age + sibsp + parch + fare
```

```{r}
titanic_recipe %>% print()
```


---
## Recipes for Feature Engineering

```{r, eval=FALSE}
# summary() provides a bit more information than print()
titanic_recipe %>% summary()
```

```{r,echo=FALSE}
titanic_recipe %>% summary() %>% kable()
```

--

<p style="padding-top:25px;">Now we are ready to add some preprocessing (i.e., feature engineering) steps to the recipe!</p>

---
class: onecol
## Common Steps

.pull-left[
- **Adding predictors**
  - Calculated predictors
  - Categorical predictors
  - Interaction Terms
- **Transforming predictors**
  - Centering and Scaling
  - Resolving Skewness
  - Addressing Outliers
]
.pull-right[

- **Reducing predictors**
  - Nero-Zero Variance
  - Multicollinearity
  - Dimensionality Reduction
- **Advanced Steps**
  - Feature Extraction
  - Dealing with Missing Values
  - Feature Selection
]

---
class: inverse, center, middle
# Adding predictors
---
class: onecol
## Calculated predictors

Some variables will need to be calculated from existing values and variables

- You may choose to score an instrument from item-level data
- You may choose to encode a predictor as the ratio of two values
- You may choose to calculate sums, means, counts, proportions, etc.

--

<p style="padding-top:25px;">We will show you some basic steps for calculating variables within {recipes}</p>

For more advanced/complex data wrangling, we recommend you read

- <i>[R for Data Science: Visualize, Model, Transform, Tidy, and Import Data](https://r4ds.had.co.nz/)</i><br />by Wickham and Grolemund (book for purchase or online for free)

---
## Calculated predictors

```{r}
# Add a step to the recipe to calculate new predictors from existing predictors
cp_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_mutate( #<<
    numfamily = sibsp + parch, #<<
    over70 = age > 70 #<<
  ) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

---
## Calculated predictors

```{r, eval=FALSE}
# We can ask for a summary of cp_recipe to see what happened
cp_recipe %>% summary()
```

```{r, echo=FALSE}
cp_recipe %>% summary() %>% kable()
```

---
## Calculated predictors

```{r}
# If this is the only step in our recipe, we can bake() the recipe
# This will allow us to generate updated training and testing sets
cp_train <- bake(cp_recipe, new_data = titanic_train)
cp_test <- bake(cp_recipe, new_data = titanic_test)
```

--

```{r}
# Let's see if it worked
glimpse(cp_test)
```

---
class: onecol
## Categorical predictors

Categorical predictors can be re-encoded into multiple binary (0 or 1) predictors

In `titanic`, the categorical variable <b>`sex`</b> takes on the value *male* or *female*

--

.pull-left[
.center[![:emphasize](Dummy Coding)]

sex | sex_male
:---|:-------:
female | 0  
male   | 1 

<p style="text-align:center;font-size:20px;">Efficient and avoids redundancy<br />Good for GLM-based methods</p>
]

--

.pull-right[
.center[![:emphasize](One-Hot Encoding)]

sex | sex_female | sex_male
:---|:----------:|:-------:
female | 1  | 0 
male   | 0  | 1

<p style="text-align:center;font-size:20px;">Simple and easy to interpret<br />Good for tree-based methods</p>
]


---
## Categorical predictors in R

```{r}
# Add a step to the recipe to create dummy codes for pclass and sex
dc_recipe <- 
  titanic %>%
  recipe(survived ~ .) %>% 
  step_dummy(pclass, sex) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

--

.footnote[[1] As a shortcut, we could also have used `step_dummy(all_nominal_predictors())`.]

---
## Categorical predictors in R

```{r}
# It is easy to modify this recipe for one-hot encoding
oh_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_dummy(pclass, sex, one_hot = TRUE) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

.footnote[[1] Note that step_dummy() is used for both dummy coding and one-hot encoding (with different arguments).]

---
class: onecol
## Interaction Terms

Interaction terms allow the meaning of one predictor to depend on other predictors

In this way, interaction terms allow predictor "effects" to be **contingent** or **conditional**

e.g., perhaps having parents or children on board the Titanic helps you predict survival... but the effects differs depending on whether the passenger is a man or a woman

--

<p style="padding-top:25px;">Interaction terms are literally <b>products</b> (i.e., multiplications) of two or more predictors</p>

In order to include categorical variables in interaction terms, dummy code them first

$$survived \sim parch + sex\_dummy + parch \times sex\_dummy$$

---
## Interaction Terms in R

```{r}
# Add interaction term between parch and the sex_male dummy code
it_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_dummy(pclass, sex) %>% 
  step_interact(~ parch:sex_male) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

.footnote[[1] step_interact() requires you to use tilde (`~`) and the colon operator (`:`) to specify interaction terms.<br />[2] We also need to anticipate what the name of the dummy code variable will be and include that.<br />[3] We could include higher-order interactions by adding more colons and predictors (e.g., `x1:x2:x3`).]

---
## Interaction Terms in R

```{r}
# Bake the recipe and preview the updated training set
bake(it_recipe, new_data = titanic_train) %>% glimpse()
```

---
class: inverse, center, middle
# Transforming predictors
---
class: onecol
## Normalizing

Predictors with vastly different means and SDs can cause problems for some algorithms

--

![:emphasize](Centering) a predictor involves changing its mean to $0.0$

- This is accomplished by subtracting the mean from every observation 

--

![:emphasize](Scaling) a predictor involves changing its standard deviation (and variance) to $1.0$

- This is accomplished by dividing each observation by the standard deviation

--

![:emphasize](Normalizing) a predictor involves centering it and then scaling it

- This is also sometimes called "standardizing" or $z$-scoring the predictor

---
## Centering Visualized

```{r centering, echo=FALSE}
x <- titanic$age
p1 <- ggplot(tibble(x), aes(x)) + geom_density(na.rm = TRUE) +
  annotate(
    "label", 
    x = mean(x, na.rm = TRUE), 
    y = 0.005, 
    label = glue::glue("M={round(mean(x, na.rm = TRUE), 1)}, SD={round(sd(x, na.rm = TRUE), 1)}"),
    size = 5
    ) +
  labs(x = "age", y = "density") +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
x_c <- x - mean(x, na.rm = TRUE)
p2 <- ggplot(tibble(x_c), aes(x_c)) + geom_density(na.rm = TRUE) + 
  annotate(
    "label", 
    x = mean(x_c, na.rm = TRUE), 
    y = 0.005, 
    label = glue::glue("M={round(mean(x_c, na.rm = TRUE), 1)}, SD={round(sd(x_c, na.rm = TRUE), 1)}"),
    size = 5
    ) +
  labs(x = "age_centered", y = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p1 | p2
```

.footnote[The mean is now 0 but the shape and SD of the distribution are unchanged (i.e., it has been shifted left).]

---
## Scaling Visualized

```{r scaling, echo=FALSE}
x_s <- x / sd(x, na.rm = TRUE)
p3 <- ggplot(tibble(x_s), aes(x_s)) + geom_density(na.rm = TRUE) + 
  annotate(
    "label", 
    x = mean(x_s, na.rm = TRUE), 
    y = 0.075, 
    label = glue::glue("M={round(mean(x_s, na.rm = TRUE), 1)}, SD={round(sd(x_s, na.rm = TRUE), 1)}"),
    size = 5
    ) +
  labs(x = "age_scaled", y = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p1 | p3
```

.footnote[The SD is now 1 and the mean is lower, but the shape of the distribution is unchanged.]

---
## Normalizing Visualized

```{r normalizing, echo=FALSE}
x_n <- (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
p4 <- ggplot(tibble(x_n), aes(x_n)) + geom_density(na.rm = TRUE) + 
  annotate(
    "label", 
    x = mean(x_n, na.rm = TRUE), 
    y = 0.075, 
    label = glue::glue("M={round(mean(x_n, na.rm = TRUE), 1)}, SD={round(sd(x_n, na.rm = TRUE), 1)}"),
    size = 5
    ) +
  labs(x = "age_normalized", y = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18) +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p1 | p4
```

.footnote[The mean is now 0 and the SD is now 1, but the shape of the distribution is unchanged.]

---
## Normalizing in R

```{r}
# Normalize the age variable using the training set mean and SD
norm_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_normalize(age) %>% #<<  
  prep(training = titanic_train, log_changes = TRUE)
```

--

```{r}
# Because of prep(), bake() uses the training set mean and SD to normalize the testing data¬≤
norm_test <- bake(norm_recipe, new_data = titanic_test)
```


.footnote[
[1] We could also have used `step_center()` and/or `step_scale()` instead of `step_normalize()`.<br />
[2] This is important to accurately estimating out-of-sample performance on truly novel data.
]

---
## Resolving Skewness

---
## Resolving Skewness in R

---
## Addressing Outliers

---
## Addressing Outliers in R

---
class: inverse, center, middle
# Reducing predictors
---
class: onecol
## Zero and Near-Zero Variance Predictors
![:emphasize](Zero variance predictors) take on only a single value in the sample

- These predictors are **uninformative** and may lead to **modeling problems**

--

![:emphasize](Near-zero variance predictors) take on only a few unique values with low frequencies

- These predictors can easily become zero-variance predictors during resampling

--

<p style="padding-top:25px;">For many algorithms, we want to <b>detect</b> and <b>remove</b> both types of predictors</p>

(This may not be necessary for algorithms with built-in *predictor selection*)

---
## Zero and Nero-Zero Variance Predictors in R

```{r}
# Calculate predictors then detect and remove zero-variance predictors
zv_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_mutate(species = "homo sapiens", under20 = age < 20, over70 = age > 70) %>% 
  step_zv(all_predictors()) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

---
## Zero and Nero-Zero Variance Predictors in R

```{r}
# Calculate predictors then detect and remove near-zero-variance predictors
nzv_recipe <- 
  titanic %>% 
  recipe(survived ~ .) %>% 
  step_mutate(species = "homo sapiens", under20 = age < 20, over70 = age > 70) %>% 
  step_nzv(all_predictors()) %>% #<<
  prep(training = titanic_train, log_changes = TRUE)
```

---
class: onecol
## Multicollinearity

![:emphasize](Highly correlated predictors) can lead to problems for some algorithms/procedures

- The model has to randomly choose between the predictors, leading to **instability**
- Model predictions may be fine, but model **interpretation** will often be obfuscated
- The cutoff for "problematically high" correlations varies (e.g., 0.5 to 0.9 or higher)

--

Predictors that are ![:emphasize](linear combinations) of other predictors are similarly problematic

- Occurs if a predictor variable can be predicted from the other predictor variables
- This is why dummy coding is preferred to one-hot encoding for some algorithms

--

<p style="padding-top:25px;">For many algorithms, we want to <b>detect</b> and <b>remove</b> redundant predictors</p>

(This may not be necessary for algorithms with *regularization* or *predictor selection*)

---
class: onecol
## Multicollinearity in R

```{r}
# Add some predictors with high correlations and linear dependency
mc_titanic <- titanic %>% mutate(
  wisdom = 100 + 0.25 * age + rnorm(nrow(.)), # highly correlated with age
  family = sibsp + parch                    # linear combo of sibsp and parch
)
mc_train <- mc_titanic[index, ]
```

--

```{r}
glimpse(mc_train)
```

---
class: onecol
## Multicollinearity in R

```{r}
# Estimate correlation matrix for the numeric predictors
mc_train %>% select(where(is.numeric), -survived) %>% cor() %>% round(digits = 2)
```

---
class: onecol
## Multicollinearity in R

```{r}
# Detect and remove predictors that are highly correlated with another predictor
hc_recipe <- 
  mc_titanic %>% 
  recipe(survived ~ .) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9) %>% #<<
  prep(training = mc_train, log_changes = TRUE)
```

--

.footnote[[1] If we want to consider correlations with categorical variables, we can add step_dummy() to the pipeline.<br />[2] We could have also lowered the threshold to 0.8 in order to drop the `family` variable here.]
---
class: onecol
## Multicollinearity in R

```{r}
# Detect and remove predictors that are linear combinations of other predictors
lc_recipe <- 
  mc_titanic %>% 
  recipe(survived ~ .) %>% 
  step_impute_knn(all_predictors()) %>% 
  step_lincomb(all_numeric_predictors()) %>%  #<<
  prep(training = mc_train, log_changes = TRUE)
```

---
## Dimensionality Reduction

---
## Dimensionality Reduction in R

---
class: inverse, center, middle
# Advanced Topics
---
## Feature Extraction

---
## Dealing with Missing Values

---
## Feature Selection


---
class: onecol
## Recommendations

Algorithm | Preprocessing Steps
:-------- | :------------------
Regularized Regression&emsp; | `dummy`, `nzv`, `impute`, `corr`,  `lincomb`, (`transform`)&emsp;
Decision Tree | (`decorrelate`)
Random Forest | (`nzv`), (`impute`), (`corr`)

For these algorithms, normalization is only really necessary for interpretation

However, other algorithms (e.g., MLP and SVM) may require normalization

To learn more about feature engineering, see [1]

.footnote[[1] Kuhn & Johnson (2020). *Feature Engineering and Selection: A Practical Approach for Predictive Models.* CRC Press.]

---
class: inverse, center, middle
# Live Coding
---
class: inverse, center, middle
# Time for a Break!

```{r countdown, echo=FALSE}
countdown(
  minutes = 10, 
  seconds = 0, 
  right = "33%", 
  left = "33%",
  bottom = "15%",
  color_background = "white",
  color_text = "black",
  color_running_background = "white",
  color_running_text = "black",
  warn_when = 60
)
```
