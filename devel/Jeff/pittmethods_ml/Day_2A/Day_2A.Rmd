---
title: '<span style="font-size:48pt;">Feature Engineering</span>'
subtitle: '.big[üë∑ üóÑÔ∏è üõ†Ô∏è] ' 
author: 'Applied Machine Learning in R <br />Pittsburgh Summer Methodology Series'
date: 'Lecture 2-A &emsp; &emsp; July 20, 2021'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      beforeInit: "macros.js"
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  footnote_font_size = "20px",
  footnote_color = "gray",
  text_slide_number_font_size = "18px"
)
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(caret)
```

class: inverse, center, middle
# Overview

<style type="text/css">
.onecol {
    font-size: 26px;
}
.twocol {
  font-size: 24px;
}
</style>
---
class: onecol
## Feature Engineering
.left-column[
<br />
```{r engineer, echo=FALSE}
include_graphics("engineer.jpg")
```
]
.right-column[
**Prepare the features for analysis**
- *Extract* features
- *Transform* features
- *Re-encode* features
- *Combine* features
- *Reduce* feature dimensionality
- *Impute* missing feature values
- *Select* and drop features
]
---
class: onecol

## Motivation

**Features** are descriptions of the data points that help to predict the labels

- We may need to extract features from "raw" or "low-level" data (e.g., images)
- We may need to address issues with missing data and feature distributions

--

There are many potential ways to **encode** or "represent" the features

- e.g., adding, dropping, transforming, and combining features<sup>1</sup>
- Feature encoding can have a big ![:emphasize](impact on predictive performance)<sup>2</sup>
- The optimal encoding depends on both the **algorithm** and the **relationships**

.footnote[
[1] Some algorithms can learn their own, complex feature representations<br />
[2] Some algorithms are more sensitive to feature encoding than others
]

---
class: onecol
## Examples of Feature Encodings
.left-column[
<br />

```{r, echo=FALSE}
include_graphics("july.jpg")
```

]
.right-column[
**When an event or observation occurred**

- The numeric year *(2021)*
- The numeric month *(7)*
- The numeric day of the month *(20)*
- The numeric day of the year *(201)*
- Days since a reference *(diagnosis +2)*
- The day of the week *(Tuesday)*
- The season of the year *(Summer)*
- The type of day *(weekday)* 
- The presence of a holiday *(FALSE)*

]
---
class: onecol
## Issues to Navigate in Feature Engineering
.left-column[
<br />
```{r, echo=FALSE}
include_graphics("cones.jpg")
```
]
.right-column[
- Features with **non-normal** distributions
- Features with vastly **different scales**
- Features with extreme **outliers**
- Features with **missing** or censored values
- Features that are **correlated** with one another
- Features that are **redundant** with one another
- Features that have zero or **near-zero variance**
- Features that are **uninformative** for a task
- Features with **uncertainty** or unreliability
]
---

---
class: onecol
## Lecture Topics

.pull-left[
- **Adding Features**
  - Calculated Features
  - Categorical Features
  - Interaction Terms
- **Transforming Features**
  - Centering and Scaling
  - Resolving Skewness
  - Addressing Outliers
]
.pull-right[

- **Reducing Features**
  - Nero-Zero Variance
  - Multicollinearity
  - Dimensionality Reduction
- **Advanced Topics**
  - Feature Extraction
  - Dealing with Missing Values
  - Feature Selection
]

---
class: inverse, center, middle
# Adding Features
---
class: onecol
## Calculated Features

Some variables will need to be calculated from existing values and variables

- You may choose to score an instrument from item-level data
- You may choose to encode a feature as the ratio of two values
- You may choose to calculate sums, means, counts, proportions, etc.

We will provide some example code in `base` R and `tidyverse` formats

But to learn more about data wrangling in R, we recommend the following

- <i>[R for Data Science: Visualize, Model, Transform, Tidy, and Import Data](https://r4ds.had.co.nz/)</i><br />by Wickham and Grolemund (book for purchase or online for free)

---
## Calculated Features in R
```{r}
# Read in and partition the data
set.seed(2021)
trainIndex <- caret::createDataPartition(iris$Species, p = 0.8, list = FALSE, times = 1)
irisTrain <- iris[trainIndex, ]

# Add calculated features using base R
irisTrain$Avg.Length <- (irisTrain$Sepal.Length + irisTrain$Petal.Length) / 2
irisTrain$Width.Ratio <- irisTrain$Sepal.Width / irisTrain$Petal.Width
```

```{r, echo=FALSE}
knitr::kable(irisTrain) %>% 
  kableExtra::scroll_box(height = "250px")
```

---

## Calculated Features in R

```{r}
# Add calculated features using tidyverse
library(tidyverse)
irisTrain <- irisTrain %>% 
  mutate(
    Avg.Length = (Sepal.Length + Petal.Length) / 2,
    Width.Ratio = Sepal.Width / Petal.Width
  )
```

```{r, echo=FALSE}
knitr::kable(irisTrain) %>% 
  kableExtra::scroll_box(height = "250px")
```

---
class: onecol
## Categorical Features

Categorical features can be re-encoded into multiple binary (0 or 1) features

Imagine a categorical variable ** `age` ** that is either *Child* or *Adolescent* or *Adult*

--

.pull-left[
.center[![:emphasize](Dummy Coding)]

Group | age1 | age2
:-----|:----:|:----:
Child | 0  | 0
Adolescent | 1 | 0
Adult | 0 | 1

<p style="text-align:center;font-size:20px;">Efficient and avoids redundancy<br />Good for GLM-based methods</p>
]

.pull-right[
.center[![:emphasize](One-Hot Encoding)]

Group | age1 | age2 | age3
:-----|:----:|:----:|:----:
Child | 1  | 0  | 0
Adolescent | 0 | 1 | 0
Adult | 0 | 0 | 1

<p style="text-align:center;font-size:20px;">Simple and easy to interpret<br />Good for tree-based methods</p>
]


---
class: onecol
## Categorical Features in R
If you save a categorical variable as a factor in R, it will dummy code it by default

But we can also explicitly dummy code it using `caret::dummyVars()`

```{r}
data(etitanic, package = "earth")
```

```{r,echo=FALSE}
knitr::kable(etitanic) %>% kableExtra::scroll_box(height="275px")
```

---
class: onecol
## Categorical Features in R

```{r, eval=FALSE}
# Dummy Coding
dummy <- caret::dummyVars(survived ~ ., data = etitanic, fullRank = TRUE) #<<
predict(dummy, newdata = etitanic)
```

```{r,echo=FALSE}
dummy <- caret::dummyVars(survived ~ ., data = etitanic, fullRank = TRUE)
predict(dummy, newdata = etitanic) %>% 
  knitr::kable() %>% 
  kableExtra::scroll_box(height="300px")
```

---
class: onecol
## Categorical Features in R
```{r, eval=FALSE}
# One-Hot Encoding
onehot <- caret::dummyVars(survived ~ ., data = etitanic, fullRank = FALSE) #<<
predict(onehot, newdata = etitanic)
```

```{r,echo=FALSE}
onehot <- caret::dummyVars(survived ~ ., data = etitanic, fullRank = FALSE)
predict(onehot, newdata = etitanic) %>% 
  knitr::kable() %>% 
  kableExtra::scroll_box(height="300px")
```

---
## Interaction Terms

---
## Interaction Terms in R

---
class: inverse, center, middle
# Transforming Features
---
class: onecol
## Centering and Scaling

Definition of centering

Definition of scaling
---
## Centering and Scaling in R

---
## Resolving Skewness

---
## Resolving Skewness in R

---
## Addressing Outliers

---
## Addressing Outliers in R

---
class: inverse, center, middle
# Reducing Features
---
class: onecol
## Zero and Near-Zero Variance Predictors
![:emphasize](Zero variance predictors) take on only a single value in the sample

- These predictors are **uninformative** and may lead to **modeling problems**

--

![:emphasize](Near-zero variance predictors) take on only a few unique values with low frequencies

- These predictors can easily become zero-variance predictors during resampling

--

<p style="padding-top:25px;">For many algorithms, we want to <b>detect</b> and <b>remove</b> both types of predictors</p>

(This may not be necessary for algorithms with built-in *feature selection*)

---
## Zero and Nero-Zero Variance Predictors in R

```{r}
# Simulate some predictors with acceptable, near-zero, and zero variance
example <- data.frame(
  x1 = rbinom(100, 1, prob = 0.50), # mix of 0s and 1s
  x2 = rbinom(100, 1, prob = 0.99), # nearly all 1s
  w1 = rbinom(100, 5, prob = 0.20), # mix of 0s, 1s, 2s, and 3s
  w2 = rbinom(100, 5, prob = 0.00)  # all 0s
)
```

--

```{r}
# Preview the simulated predictor variables
glimpse(example)
```

---
class: onecol
## Zero and Nero-Zero Variance Predictors in R

```{r}
# Detect the column numbers of variables with near-zero variance
nzv <- caret::nearZeroVar(example) %>% print()
```

--

```{r}
# Show the metrics used for detection
caret::nearZeroVar(example, saveMetrics = TRUE)
```

--

```{r}
# Drop the zero and near-zero variance predictors
example2 <- example[, -nzv]
```

---
## Zero and Nero-Zero Variance Predictors in R

```{r}
# Drop zero variance predictors using preProcess
preProcess(example, method = "zv")
```

--

```{r}
# Drop near-zero variance predictors using preProcess
preProcess(example, method = "nzv")
```

.footnote[[1] Note that preProcess() will ignore non-numeric variables, so dummy code categorical variables first.]

---
class: onecol
## Multicollinearity

![:emphasize](Highly correlated predictors) can lead to problems for some algorithms/procedures

- The model has to randomly choose between the predictors, leading to **instability**
- Model predictions may be fine, but model **interpretation** will often be obfuscated
- The cutoff for "problematically high" correlations varies (e.g., 0.5 to 0.9 or higher)

--

Predictors that are ![:emphasize](linear combinations) of other predictors are similarly problematic

- Occurs if a predictor variable can be predicted from the other predictor variables
- This is why dummy coding is preferred to one-hot encoding for some algorithms

--

<p style="padding-top:25px;">For many algorithms, we want to <b>detect</b> and <b>remove</b> redundant predictors</p>

(This may not be necessary for algorithms with *regularization* or *feature selection*)

---
class: onecol
## Multicollinearity in R
```{r}
# Simulate some predictors with high correlations and linear dependency
example <- tibble::tibble(
  x1 = rnorm(n = 100, mean = 0, sd = 1),
  x2 = rnorm(n = 100, mean = 0, sd = 1),
  x3 = runif(n = 100, min = -3, max = 3),
  hicorr = x1 + rnorm(n = 100, mean = 10, sd = 0.3), # high correlation with x1
  lincom = (x1 + x2) / 2                             # linear combo of x1 and x2
)
```

--

```{r}
# Preview the simulated predictor variables
glimpse(example)
```

---
class: onecol
## Multicollinearity in R
```{r}
# Estimate correlation matrix
cor(example) %>% round(2)
```

--

```{r}
# Detect the column numbers of variables with high correlations to drop
fc <- caret::findCorrelation(cor(example), cutoff = 0.9) %>% print()
```

--

```{r}
# Drop predictor variables with high correlations
example2 <- example[, -fc]
```

---
class: onecol
## Multicollinearity in R

```{r}
caret::preProcess(example, method = "corr")
```

---
class: onecol
## Multicollinearity in R

```{r}
# Detect linear combinations of predictors
flc <- caret::findLinearCombos(example) %>% print()
```

```{r}
# Drop the problematic predictor variables
example2 <- example[, -flc$remove]
```

.footnote[Sadly, there is no preProcess() method for findLinearCombos(), but the <a href="https://recipes.tidymodels.org/">recipes</a> package does have an equivalent.]
---
## Dimensionality Reduction

---
## Dimensionality Reduction in R

---
class: inverse, center, middle
# Advanced Topics
---
## Feature Extraction

---
## Dealing with Missing Values

---
## Feature Selection


---
class: onecol
## Recommendations

Algorithm | Preprocessing Steps
:-------- | :------------------
Regularized Regression | `dummy`, `nzv`, `impute`, `decorrelate`, (`transform`)
Decision Tree | (`decorrelate`)
Random Forest | (`nzv`), (`impute`), (`decorrelate`)

For these algorithms, normalization is only really necessary for interpretation

However, other algorithms (e.g., MLP and SVM) may require normalization

To learn more about feature engineering, see [1] and the [recipes](https://recipes.tidymodels.org/) package

.footnote[[1] Kuhn & Johnson (2020). *Feature Engineering and Selection: A Practical Approach for Predictive Models.* CRC Press.]

---
class: inverse, center, middle
# Live Coding
---
class: inverse, center, middle
# Time for a Break!

```{r countdown, echo=FALSE}
countdown(
  minutes = 10, 
  seconds = 0, 
  right = "33%", 
  left = "33%",
  bottom = "15%",
  color_background = "white",
  color_text = "black",
  color_running_background = "white",
  color_running_text = "black",
  warn_when = 60
)
```
