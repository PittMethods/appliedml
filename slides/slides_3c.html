<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Decision Trees and Random Forests</title>
    <meta charset="utf-8" />
    <meta name="author" content="Pittsburgh Summer Methodology Series" />
    <script src="slides_3c_files/header-attrs/header-attrs.js"></script>
    <link href="slides_3c_files/tachyons/tachyons.min.css" rel="stylesheet" />
    <script src="slides_3c_files/clipboard/clipboard.min.js"></script>
    <link href="slides_3c_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="slides_3c_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link href="slides_3c_files/countdown/countdown.css" rel="stylesheet" />
    <script src="slides_3c_files/countdown/countdown.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="styles.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# <span style="font-size:48pt;">Decision Trees and Random Forests</span>
]
.subtitle[
## .big[ü§î üå¥ üçÇÔ∏è]
]
.author[
### Pittsburgh Summer Methodology Series
]
.date[
### Day 3C ‚ÄÉ ‚ÄÉ August 10, 2022
]

---










class: inverse, center, middle
# Overview

---
class: twocol
## Lecture Topics

.pull-left[
**Simple Decision Trees**
- Motivation (modeling nonlinearity)
- Classification trees
- Regression trees
- Recursive partitioning
- Pruning
- Stopping criteria 

**Ensemble Methods**
- Random forests 
- Aggregating predictions
- Accuracy vs interpretability  
]

.pull-right[
&lt;img src="../figs/flowchart2.jpg" width="400" height="450" /&gt;
]

---
class: onecol
## Geometry of Data

So far, we've modeled linear relationships with linear boundaries between classes, e.g.: 

&lt;img src="slides_3c_files/figure-html/unnamed-chunk-2-1.png" width="100%" /&gt;

---
class: onecol
## Geometry of Data

But what about other types of relationships? 

&lt;img src="slides_3c_files/figure-html/unnamed-chunk-3-1.png" width="100%" /&gt;

---
class: onecol
## Geometry of Data

But what about other types of relationships? 

&lt;img src="slides_3c_files/figure-html/unnamed-chunk-4-1.png" width="100%" /&gt;

---
class: onecol
## Geometry of Data

These classes are very clearly separated.

However, we can't use a **single equation** to describe the boundaries between them. 

.pull-left[
&lt;img src="slides_3c_files/figure-html/unnamed-chunk-5-1.png" width="100%" /&gt;
]

.pull-right[
&lt;img src="slides_3c_files/figure-html/unnamed-chunk-6-1.png" width="100%" /&gt;
]

--

.bg-light-green.b--dark-green.ba.bw1.br3.pl4[
Decision trees capture complex decision boundaries and maintain interpretability.
]

---
class: inverse, center, middle
# Decision Trees

---
class: onecol
## Decision Trees

Decision trees use `if-then` logic to partition data into .imp[homogeneous groups], e.g.,:

--

`if has legs` &lt;/br&gt;
`| if barks then animal = dog` &lt;/br&gt;
`| else animal = cat` &lt;/br&gt;
`else animal = fish`

--

&lt;p style="padding-top:30px;"&gt;This is a simple **classification tree**, but decision trees are also used for regression.

Benefits and drawbacks of decision trees include: 

- Pros: **highly interpretable** and able to model nonlinear relationships 

- Cons: typically having **poorer accuracy** than other methods (e.g., random forests).

---
class: twocol
## Decision Trees

.pull-left[
Trees are often visualized graphically.

If a statement is .imp[true], you go to the left. 

If a statement is .imp[false], you go to the right.

The top node is called the **root node**.

Subsequent splitting nodes are **branches**.

Output labels are **terminal nodes** or **leaves**.
]

.pull-right[
&lt;img src="../figs/simpletree.png" width="100%" /&gt;
]

---
class: inverse, center, middle
# Classification Trees

---
class: onecol
## Building a Classification Tree

The goal of classification trees are to partition data into homogeneous groups.

This is defined by .imp[purity] (including more of one class than another class per node).

We use .imp[recursive partioning] to find data splits that maximize purity of each node.

--

&lt;p style="padding-top:30px;"&gt;The .imp[Gini index]&lt;sup&gt;[1]&lt;/sup&gt; is the most commonly-used metric for quantifying purity.

`$$Gini = 1 - \sum\limits_{i = 1}^C(p_i)^2$$`

where `\(p_i\)` = the probability of being in the `\(i\)`th class and `\(C\)` = total number of classes

.footnote[
The Gini index ranges from 0 - 1, with smaller values indicating greater purity.
]

---
class: onecol
## Building a Classification Tree

Let's walk through an example classification tree, with a toy depression risk dataset. 

Stressful Event&amp;emsp; | Family History&amp;emsp; | Age&amp;emsp;&amp;emsp;&amp;emsp; | Depression Risk&amp;emsp;
:------- | :-------- | :------- |:------- |
No | Yes | 10 | Low 
No | No | 12 | Low
Yes | Yes | 16 | High
Yes | Yes | 22 | High
No | Yes | 30 | High 
No | No | 38 | Low
Yes | No | 46 | Low

--

The first thing to do is choose the .imp[root node] (feature that best predicts depression risk). 

---
class: twocol
## Choosing the Root Node

.pull-left[
Start: find **Gini index** of stressful life events.

.imp[Stressful Event]&amp;emsp; | Family History&amp;emsp; | Age&amp;emsp;&amp;emsp;&amp;emsp; | .imp[Depression Risk]&amp;emsp;
:------- | :-------- | :------- |:------- |
.imp[No] | Yes | 10 | .imp[Low] 
.imp[No] | No | 12 | .imp[Low]
.imp[Yes] | Yes | 16 | .imp[High]
.imp[Yes] | Yes | 22 | .imp[High]
.imp[No] | Yes | 30 | .imp[High] 
.imp[No] | No | 38 | .imp[Low]
.imp[Yes] | No | 46 | .imp[Low]
]

--

.pull-right[
&lt;img src="../figs/stresstree.png" width="90%" /&gt;
]

---
class: twocol
## Choosing the Root Node

.pull-left[
Both terminal nodes are **impure**, with people having high and low depression risk. 
]

.pull-right[
&lt;img src="../figs/stresstree.png" width="90%" /&gt;
]

---
count: false
class: twocol
## Choosing the Root Node
.pull-left[
Both terminal nodes are **impure**, with people having high and low depression risk. 

Let's calculate the **Gini impurity** of each leaf: 

`\(Gini_{leaf} = 1 - P(High)^2 - P(Low)^2\)`
]

.pull-right[
&lt;img src="../figs/stresstree.png" width="90%" /&gt;
]


---
count: false
class: twocol
## Choosing the Root Node
.pull-left[
Both terminal nodes are **impure**, with people having high and low depression risk. 

Let's calculate the **Gini impurity** of each leaf: 

`\(Gini_{leaf} = 1 - P(High)^2 - P(Low)^2\)`

`\(Gini_{left} = 1 - (0.666)^2  - (0.333)^2 = 0.444\)`

`\(Gini_{right} = 1 - (0.25)^2 - (0.75)^2 = 0.375\)`
]

.pull-right[
&lt;img src="../figs/stresstree.png" width="90%" /&gt;
]

---
count: false
class: twocol
## Choosing the Root Node
.pull-left[
Both terminal nodes are **impure**, with people having high and low depression risk. 

Let's calculate the **Gini impurity** of each leaf: 

`\(Gini_{leaf} = 1 - P(High)^2 - P(Low)^2\)`

`\(Gini_{left} = 1 - (0.666)^2  - (0.333)^2 = 0.444\)`

`\(Gini_{right} = 1 - (0.25)^2 - (0.75)^2 = 0.375\)`

The total Gini index is the weighted average:

`\(Gini_{stress} = (\frac{3}{7})*0.444 + (\frac{4}{7})*0.375 = 0.405\)`
]

.pull-right[
&lt;img src="../figs/stresstree.png" width="90%" /&gt;
]


---
class: twocol
## Choosing the Root Node

We can compare this to the Gini index for family history, which comes to:

.pull-left[
`\(Gini_{family} = 0.214\)`

Stressful Event&amp;emsp; | .imp[Family History]&amp;emsp; | Age&amp;emsp;&amp;emsp;&amp;emsp; | .imp[Depression Risk]&amp;emsp;
:------- | :-------- | :------- |:------- |
No | .imp[Yes] | 10 | .imp[Low] 
No | .imp[No] | 12 | .imp[Low]
Yes | .imp[Yes] | 16 | .imp[High]
Yes | .imp[Yes] | 22 | .imp[High]
No | .imp[Yes] | 30 | .imp[High] 
No | .imp[No] | 38 | .imp[Low]
Yes | .imp[No] | 46 | .imp[Low]
]

--

.pull-right[
&lt;img src="../figs/familytree.png" width="90%" /&gt;
]

---
class: twocol
## Choosing the Root Node

Calculating the Gini index for **numerical features** is slightly more complicated. 

We sort values from lowest to highest, calculate the midpoint of adjacent rows, and use these cutoffs to find the lowest Gini index. The lowest Gini index is for `age &lt; 14.5` (Gini = 0.343): 

.pull-left[
Stressful Event&amp;emsp; | Family History&amp;emsp; | .imp[Age]&amp;emsp;&amp;emsp;&amp;emsp; | .imp[Depression Risk]&amp;emsp;
:------- | :-------- | :------- |:------- |
No | Yes | .imp[10] | .imp[Low] 
No | No | .imp[13] | .imp[Low]
Yes | Yes | .imp[16] | .imp[High]
Yes | Yes | .imp[22] | .imp[High]
No | Yes | .imp[30] | .imp[High] 
No | No | .imp[38] | .imp[Low]
Yes | No | .imp[46] | .imp[Low]
]

.pull-right[
&lt;img src="../figs/agetree.png" width="90%" /&gt;
]

---
class: twocol
## Recursive Partioning

.pull-left[
Family history has the lowest Gini index of all features (i.e., highest purity).

Thus, we set family history as the root node. 

We then .imp[continue partioning the data] to find the next split from an impure node.

We can continue this process until we are left with .imp[only pure leaves]. 

If we are left with an impure leaf, the label is set to the mode of all observations within the leaf.
]

.pull-right[
&lt;img src="../figs/familytree2.png" width="90%" /&gt;
]

---
class: inverse, center, middle
# Regression Trees

---
class: onecol
## Building a Regression Tree

Let's say we have data that look like this. How should we model these data? 

&lt;img src="slides_3c_files/figure-html/unnamed-chunk-16-1.png" width="100%" /&gt;

---
## Building a Regression Tree

&lt;img src="slides_3c_files/figure-html/unnamed-chunk-17-1.png" width="100%" /&gt;

---
class: onecol
## Simple Regression Tree

A regression tree can solve this problem by partioning data into homogeneous groups.

.pull-left[
&lt;img src="slides_3c_files/figure-html/unnamed-chunk-18-1.png" width="100%" /&gt;
]

--

.pull-right[
&lt;img src="../figs/regtree.png" width="80%" /&gt;
]

---
class: onecol
## Building a Regression Tree

We start with the entire data set `\(S\)` to find the .imp[optimal feature and splitting value that partitions the data] into two groups `\(S_1\)` and `\(S_2\)` to minimize the sum of squared errors: 

`$$SSE = \sum\limits_{i \in S_1}(y_i-\bar{y}_1)^2 + \sum\limits_{i \in S_2}(y_i-\bar{y}_2)^2$$`
where
- `\(\bar{y}_1\)` = mean of training set outcomes in group `\(S_1\)`
- `\(\bar{y}_2\)` = mean of training set outcomes in group `\(S_2\)`

--

Within each `\(S_1\)` and `\(S_2\)` group, we repeat recursive partitioning until the number of samples in each terminal node falls below some threshold (typically, `\(n=20\)`).

The predicted value of a terminal node is then the mean of all observations in that node. 

---
class: onecol
## Comprehension Check 

**Let's say we continued this recursive partioning process until we were left only with pure nodes (classification tree) or minimal SSE (regression tree). &lt;/br&gt; &lt;/br&gt; What are some problems that could arise?**

--

Some answers: 

- Overfitting training data

- Poor prediction on test data/future/new data
- Instability of model (if data are slightly altered, you may find entirely different splits)

- Small number of participants in leaves

- Selection bias: features with higher number of distinct values are favored

- Poorer interpretation

---
class: inverse, center, middle
# Preventing Overfitting

---
class: twocol
## Preventing Overfitting

.pull-left[
Trees will continue to grow until each terminal node is entirely homogeneous.

This inevitably leads to **poor prediction** on testing data and any future datasets.

To prevent this, we need to **stop the algorithm at some point**, before it overfits. 

There are two main methods that can prevent overfitting in trees: stopping criteria and pruning.

]
.pull-right[
&lt;img src="slides_3c_files/figure-html/unnamed-chunk-20-1.png" width="100%" /&gt;
]

---
class: twocol
## Stopping Criteria 

.left-column.pv3[
&lt;img src="../figs/stop.png" width="100%" /&gt;
]

.right-column[
Stopping criteria prevent trees from growing in certain conditions:

1) Standard: if the leaf is homogenous (standard)

2) Standard: if the leaf will have too few observations, typically `\(n=20\)`

3) If the total number of leaves in the tree is above some threshold

We can determine the number of leaves to stop at by using **hyperperamter tuning**! 

Thus, stopping criteria prevent us from overfitting a full tree. 
]

---
class: twocol
## Pruning

.left-column.pv3[
&lt;img src="../figs/pruning.png" width="100%" /&gt;
]
.right-column[

An alternative to stopping criterion is pruning a fully-grown tree.

Rather than preventing a complex tree from growing, we can **impose a penalty** on complex trees with cost complexity pruning. 

This is a similar to concept to `\(\lambda\)` in regularized regression. 

The pruning penalty allows us to sacrifice some training accuracy to improve test accuracy. 

We also **tune the penalty hyperparameter** in {tune}.
]

---
class: onecol
## Decision Trees Summary

A single tree has **excellent interpretability** and is easy to explain to people. 

Decision trees closely mirror the **human decision-making processes**! 

However, a single tree is typically **not flexible enough** to accurately predict new data. 

**Single trees are unstable**; they change dramatically with a small shift in training data.

--

&lt;p style="padding-top:30px;"&gt;One solution is to .imp[aggregate predictions from many decision trees together]. 

This may help us **improve predictive accuracy and model stability**. 

---
class: inverse, center, middle
# Random Forests

---
class: twocol
## Random Forests

.left-column.pv3[
&lt;img src="../figs/randomforest.png" width="100%" /&gt;
]

.right-column[
Random forests aim to combine the simplicity of a single tree with greater model **flexibility**. 

We do this by building **multiple decision trees**.

We then **aggregate their predictions** together for one final prediction. 

This is called an **ensemble method**. 

By using multiple trees (rather than just one), we can achieve greater **predictive accuracy** in new datasets.

However, this comes at the cost of **lower interpretability**.
]

---
class: onecol
## Random Forests

Random forests use **bootstrapped aggregation** (bagging) to combine predictions from many  trees. Each tree is fit with a bootstrapped dataset and predictions are aggregated. 

&lt;img src="../figs/bagging.png" width="70%" /&gt;

---
class: onecol
## Random Forests

Random forests also  **decorrelate** trees by only using a subset of features at each split.

This increases **model stability** for more reliable and less variable predictions. 

.pull-left[
&lt;img src="../figs/decorrelate_1.png" width="100%" /&gt;
]

---
class: onecol
## Random Forests

Random forests also  **decorrelate** trees by only using a subset of features at each split.

This increases **model stability** for more reliable and less variable predictions. 

.pull-left[
&lt;img src="../figs/decorrelate_2.png" width="100%" /&gt;
]

--

.pull-right[
&lt;p style="padding-top:190px;"&gt;The number of *m* features chosen at each split is a .imp[hyperparameter]. 

We can tune this during cross-validation to find the optimal value. 
]

---
class: onecol
## Building a Random Forest

Let's return to this toy dataset to walk through building a random forest model.

.pull-left[
Stressful Event&amp;emsp; | Family History&amp;emsp; | Age&amp;emsp;&amp;emsp;&amp;emsp; | Depression Risk&amp;emsp;
:------- | :-------- | :------- |:------- |
No | Yes | 10 | Low 
No | No | 12 | Low
Yes | Yes | 16 | High
Yes | Yes | 22 | High
No | Yes | 30 | High 
No | No | 38 | Low
Yes | No | 46 | Low
]

--

.pull-right[
Step 1 is making **bootstrapped dataset**. 

We randomly draw (with replacement) samples, to create a bootstrapped dataset of the same size. 

This bootstrapped dataset will include some observations more than once. 

Other observations will be left out (note: this is called the **out-of-bag dataset**).
]

---
class: twocol
## Building a Random Forest

**Step 2**: Using bootstrapped data, build decision tree with a random subset of features per split. 

--

.pull-left[

.imp[Stressful Event]&amp;emsp; | Family History&amp;emsp; | .imp[Age]&amp;emsp;&amp;emsp;&amp;emsp; | Depression Risk&amp;emsp;
:------- | :-------- | :------- |:------- |
Yes | Yes | 22 | High
No | No | 38 | Low
Yes | Yes | 16 | High
No | No | 12 | Low
No | Yes | 30 | High 
No | No | 38 | Low
Yes | Yes | 22 | High
]

--

.pull-right[
&lt;img src="../figs/forest_split1.png" width="80%" /&gt;
]

---
class: twocol
## Building a Random Forest

**Step 2**: Using bootstrapped data, build decision tree with a random subset of features per split. 

.pull-left[

Stressful Event&amp;emsp; | .imp[Family History]&amp;emsp; | .imp[Age]&amp;emsp;&amp;emsp;&amp;emsp; | Depression Risk&amp;emsp;
:------- | :-------- | :------- |:------- |
Yes | Yes | 22 | High
No | No | 38 | Low
Yes | Yes | 16 | High
No | No | 12 | Low
No | Yes | 30 | High 
No | No | 38 | Low
Yes | Yes | 22 | High
]

.pull-right[
&lt;img src="../figs/forest_split2.png" width="80%" /&gt;
]

---
class: onecol
## Building a Random Forest

**Step 3**: Repeat steps 1 and 2! 

Generate another bootstrapped dataset and build another decision tree, using only a random subset of `\(m\)` features at each split. Repeat for many (e.g., 1000) trees. &lt;pstyle="padding-bottom:20px;"&gt;

&lt;img src="../figs/manytrees.png" width="100%" /&gt;

---
class: onecol
## Building a Random Forest

**Step 4**: Evaluate accuracy via out-of-bag (OOB) samples that are correctly classified. 

--

These are the two observations **not selected** in the 1st bootstrapped dataset. We can run these through all the trees built without them, and see if they get correctly classified.

.pull-left[
Stressful Event&amp;emsp; | Family History&amp;emsp; | Age&amp;emsp;&amp;emsp;&amp;emsp; | Depression Risk&amp;emsp;
:------- | :-------- | :------- |:------- |
No | Yes | 10 | Low 
Yes | No | 46 | Low
]

--

.pull-right[
&lt;img src="../figs/manytrees.png" width="100%" /&gt;
]

--

We do this for all the other OOB observations for all the trees in our random forest and examine the proportion of correctly classified OOB observations.

---
class: onecol

## Random Forests Summary

Random forests are a .imp[powerful ML algorithm] that can model nonlinearity in data. 

By only using a random subset of features at each tree split, **RF trees are decorrelated**.

Aggregating many bootstrapped, uncorrelated trees is effective at **reducing variance**.

Random forests tend to have .imp[higher accuracy] than single decision trees.

--

&lt;p style="padding-top:30px;"&gt;When building random forests, tuning parameters include: 

- The number of features to consider at each split

- The number of decision trees in the forest


.bg-light-green.b--dark-green.ba.bw1.br3.pl4[
The next section will walk through building random forest models in R.
]

---
class: inverse, center, middle
# Time for a Break!
<div class="countdown" id="timer_62f07215" style="right:33%;bottom:15%;left:33%;" data-warnwhen="60">
<code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current% / %total%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false,
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
